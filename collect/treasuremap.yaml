---
schema: deckhand/LayeringPolicy/v1
metadata:
  schema: metadata/Control/v1
  name: layering-policy
data:
  layerOrder:
    - global
    - type
    - site
    - cicd  # overrides for pipeline automation
...
---
schema: 'drydock/BootAction/v1'
metadata:
  schema: 'metadata/Document/v1'
  name: airship-target
  storagePolicy: 'cleartext'
  layeringDefinition:
    abstract: false
    layer: global
data:
  signaling: false
  assets:
    - path: /etc/systemd/system/airship.target
      type: unit
      permissions: '444'
      data: |
        [Unit]
        Description=Airshipt bootaction target
        After=multi-user.target cloud-init.target

        [Install]
        WantedBy=graphical.target

      data_pipeline:
        - utf8_decode
...
---
schema: 'drydock/BootAction/v1'
metadata:
  schema: 'metadata/Document/v1'
  name: nested-virt
  labels:
    name: nested-virt-global
    application: 'drydock'
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: 'cleartext'
data:
  signaling: false
  assets:
    - path: /etc/modprobe.d/nested-virt.conf
      type: file
      permissions: '644'
      data_pipeline:
        - utf8_decode
      data: |
        options kvm-intel nested=y
...
---
schema: 'drydock/BootAction/v1'
metadata:
  schema: 'metadata/Document/v1'
  name: promjoin-systemd-unit
  storagePolicy: 'cleartext'
  layeringDefinition:
    abstract: false
    layer: global
  labels:
    name: promjoin-systemd-unit
    application: 'drydock'
data:
  signaling: false
  assets:
    - path: /etc/systemd/system/promjoin.service
      type: unit
      permissions: '444'
      data: |
        [Unit]
        Description=Promenade Initialization Service
        After=network-online.target local-fs.target cloud-init.target
        ConditionPathExists=!/var/lib/prom.done

        [Service]
        Type=oneshot
        ExecStart=/opt/promjoin.sh

        [Install]
        WantedBy=airship.target

      data_pipeline:
        - utf8_decode
...
---
schema: 'drydock/BootAction/v1'
metadata:
  schema: 'metadata/Document/v1'
  name: seccomp-profiles
  storagePolicy: 'cleartext'
  layeringDefinition:
    abstract: false
    layer: global
  substitutions:
    - src:
        schema: pegleg/SeccompProfile/v1
        name: seccomp-default
        path: .savePath
      dest:
        path: .assets[0].path
    - src:
        schema: pegleg/SeccompProfile/v1
        name: seccomp-default
        path: .content
      dest:
        path: .assets[0].data

data:
  signaling: false
  assets:
    - type: file
      permissions: '600'
      data_pipeline:
        - utf8_decode
...
---
schema: 'drydock/BootAction/v1'
metadata:
  schema: 'metadata/Document/v1'
  name: apparmor-profiles
  storagePolicy: 'cleartext'
  layeringDefinition:
    abstract: false
    layer: global
  substitutions:
    - src:
        schema: pegleg/AppArmorProfile/v1
        name: airship-default
        path: .savePath
      dest:
        path: .assets[0].path
    - src:
        schema: pegleg/AppArmorProfile/v1
        name: airship-default
        path: .content
      dest:
        path: .assets[0].data
    - src:
        schema: pegleg/AppArmorProfile/v1
        name: airship-apparmor-loader
        path: .savePath
      dest:
        path: .assets[1].path
    - src:
        schema: pegleg/AppArmorProfile/v1
        name: airship-apparmor-loader
        path: .content
      dest:
        path: .assets[1].data

data:
  signaling: false
  assets:
    - type: file
      permissions: '600'
      data_pipeline:
        - utf8_decode
    - type: file
      permissions: '600'
      data_pipeline:
        - utf8_decode
...
---
schema: armada/Manifest/v1
metadata:
  schema: metadata/Document/v1
  name: full-site
  layeringDefinition:
    abstract: false
    layer: global
  labels:
    name: full-site-global
  storagePolicy: cleartext
data:
  release_prefix: airship
  chart_groups:
    - podsecuritypolicy
    - kubernetes-proxy
    - kubernetes-container-networking
    - kubernetes-dns
    - kubernetes-etcd
    - kubernetes-haproxy
    - kubernetes-core
    - ingress-kube-system
    - ucp-ceph-update
    - ucp-ceph-config
    - ucp-core
    - ucp-keystone
    - ucp-divingbell
    - ucp-armada
    - ucp-deckhand
    - ucp-drydock-scaled
    - ucp-promenade
    - ucp-shipyard
    - ucp-prometheus-openstack-exporter
    - osh-infra-ingress-controller
    - osh-infra-ceph-config
    - osh-infra-radosgw
    - osh-infra-logging
    - osh-infra-monitoring
    - osh-infra-mariadb
    - osh-infra-dashboards
    - openstack-ingress-controller
    - openstack-ceph-config
    - openstack-tenant-ceph
    - openstack-mariadb
    - openstack-rabbitmq
    - openstack-memcached
    - openstack-keystone
    - openstack-radosgw
    - openstack-glance
    - openstack-cinder
    - openstack-compute-kit
    - openstack-heat
    - osh-infra-prometheus-openstack-exporter
    - openstack-horizon
...
---
schema: armada/Manifest/v1
metadata:
  schema: metadata/Document/v1
  name: cluster-bootstrap
  layeringDefinition:
    abstract: false
    layer: global
  labels:
    name: cluster-bootstrap-global
  storagePolicy: cleartext
data:
  release_prefix: airship
  chart_groups:
    - podsecuritypolicy
    - kubernetes-proxy
    - kubernetes-container-networking
    - kubernetes-dns
    - kubernetes-etcd
    - kubernetes-haproxy
    - kubernetes-core
    - ingress-kube-system
    - ucp-ceph
    - ucp-ceph-config
    - ucp-core
    - ucp-keystone
    - ucp-divingbell
    - ucp-armada
    - ucp-deckhand
    - ucp-drydock
    - ucp-promenade
    - ucp-shipyard
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: kubernetes-proxy
  labels:
    name: kubernetes-proxy-global
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    # Chart source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.kubernetes.proxy
      dest:
        path: .source

    # Images
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.kubernetes.proxy
      dest:
        path: .values.images.tags

    # IP Addresses
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .kubernetes.pod_cidr
      dest:
        path: .values.command_prefix[1]
        pattern: POD_CIDR

    # Secrets
    - src:
        schema: deckhand/CertificateAuthority/v1
        name: kubernetes
        path: .
      dest:
        path: .values.secrets.tls.ca
data:
  chart_name: proxy
  release: kubernetes-proxy
  namespace: kube-system
  wait:
    timeout: 600
    labels:
      release_group: airship-kubernetes-proxy
  upgrade:
    no_hooks: false
    pre:
      delete:
        - type: job
          labels:
            release_group: airship-kubernetes-proxy
  values:
    command_prefix:
      - /proxy
      - --cluster-cidr=POD_CIDR
      - --proxy-mode=iptables
    kube_service:
      host: 127.0.0.1
      port: 6553
    livenessProbe:
      whitelist:
        - tiller-deploy
  dependencies:
    - kubernetes-proxy-htk
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: kubernetes-proxy-htk
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.kubernetes.proxy-htk
      dest:
        path: .source
data:
  chart_name: kubernetes-proxy-htk
  release: kubernetes-proxy-htk
  namespace: kubernetes-proxy-htk
  dependencies: []
...
---
schema: armada/ChartGroup/v1
metadata:
  schema: metadata/Document/v1
  name: kubernetes-proxy
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
data:
  description: Kubernetes proxy
  sequenced: true
  chart_group:
    - kubernetes-proxy
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: haproxy
  labels:
    name: haproxy-global
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:

    # Chart source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.kubernetes.haproxy
      dest:
        path: .source

    # Images
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.kubernetes.haproxy
      dest:
        path: .values.images.tags

    # Kubernetes configuration
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .kubernetes.api_service_ip
      dest:
        path: .values.conf.anchor.kubernetes_url
        pattern: KUBERNETES_IP

data:
  chart_name: haproxy
  release: haproxy
  namespace: kube-system
  protected:
    continue_processing: true
  wait:
    timeout: 600
    labels:
      release_group: airship-haproxy
  upgrade:
    no_hooks: false
    pre:
      delete:
        - type: job
          labels:
            release_group: airship-haproxy
  values:
    conf:
      anchor:
        kubernetes_url: https://KUBERNETES_IP:443
        services:
          default:
            kubernetes:
              server_opts: "check port 6443"
              conf_parts:
                frontend:
                  - mode tcp
                  - option tcpka
                  - bind *:6553
                backend:
                  - mode tcp
                  - option tcpka
                  - option tcp-check
                  - option redispatch
          kube-system:
            kubernetes-etcd:
              server_opts: "check port 2379"
              conf_parts:
                frontend:
                  - mode tcp
                  - option tcpka
                  - bind *:2378
                backend:
                  - mode tcp
                  - option tcpka
                  - option tcp-check
                  - option redispatch
  dependencies:
    - haproxy-htk
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: haproxy-htk
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.kubernetes.haproxy-htk
      dest:
        path: .source
data:
  chart_name: haproxy-htk
  release: haproxy-htk
  namespace: haproxy-htk
  values: {}
  dependencies: []
...
---
schema: armada/ChartGroup/v1
metadata:
  schema: metadata/Document/v1
  name: kubernetes-haproxy
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
data:
  description: HAProxy for Kubernetes
  chart_group:
    - haproxy
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: kubernetes-etcd-global
  layeringDefinition:
    abstract: true
    layer: global
  labels:
    name: kubernetes-etcd-global
  storagePolicy: cleartext
  substitutions:

    # Chart source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.kubernetes.etcd
      dest:
        path: .source

    # Images
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.kubernetes.etcd
      dest:
        path: .values.images.tags

    # IP addresses
    -
      src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .kubernetes.etcd_service_ip
      dest:
        path: .values.service.ip
    -
      src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .kubernetes.etcd_service_ip
      dest:
        path: .values.anchor.etcdctl_endpoint

    # CAs
    -
      src:
        schema: deckhand/CertificateAuthority/v1
        name: kubernetes-etcd
        path: .
      dest:
        path: .values.secrets.tls.client.ca
    -
      src:
        schema: deckhand/CertificateAuthority/v1
        name: kubernetes-etcd-peer
        path: .
      dest:
        path: .values.secrets.tls.peer.ca

    -
      src:
        schema: deckhand/Certificate/v1
        name: kubernetes-etcd-anchor
        path: .
      dest:
        path: .values.secrets.anchor.tls.cert
    -
      src:
        schema: deckhand/CertificateKey/v1
        name: kubernetes-etcd-anchor
        path: .
      dest:
        path: .values.secrets.anchor.tls.key

data:
  chart_name: etcd
  release: kubernetes-etcd
  namespace: kube-system
  protected:
    continue_processing: true
  wait:
    timeout: 600
    labels:
      release_group: airship-kubernetes-etcd
  upgrade:
    no_hooks: false
    pre:
      delete:
        - type: job
          labels:
            release_group: airship-kubernetes-etcd
  values:
    labels:
      anchor:
        node_selector_key: kubernetes-etcd
        node_selector_value: enabled
    etcd:
      host_data_path: /var/lib/etcd/kubernetes
      host_etc_path: /etc/etcd/kubernetes
    service:
      name: kubernetes-etcd
    network:
      service_client:
        name: service_client
        port: 2379
        target_port: 2379
      service_peer:
        name: service_peer
        port: 2380
        target_port: 2380
  dependencies:
    - kubernetes-etcd-htk
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: kubernetes-etcd-htk
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.kubernetes.etcd-htk
      dest:
        path: .source
data:
  chart_name: kubernetes-etcd-htk
  release: kubernetes-etcd-htk
  namespace: kubernetes-etcd-htk
  values: {}
  dependencies: []
...
---
schema: armada/ChartGroup/v1
metadata:
  schema: metadata/Document/v1
  name: kubernetes-etcd
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
data:
  description: Kubernetes etcd
  chart_group:
    - kubernetes-etcd
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: global-ingress-kube-system
  labels:
    ingress: kube-system
    name: ingress-kube-system-global
  layeringDefinition:
    abstract: true
    layer: global
  storagePolicy: cleartext
  substitutions:
    # Chart source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.kubernetes.ingress
      dest:
        path: .source
    # Images
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.kubernetes.ingress
      dest:
        path: .values.images.tags
data:
  chart_name: ingress-kube-system
  release: ingress-kube-system
  namespace: kube-system
  wait:
    timeout: 300
    labels:
      release_group: airship-ingress-kube-system
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
    pre:
      delete:
        - type: job
          labels:
            release_group: airship-ingress-kube-system
  values:
    labels:
      server:
        node_selector_key: kube-ingress
        node_selector_value: enabled
      error_server:
        node_selector_key: kube-ingress
        node_selector_value: enabled
    deployment:
      mode: cluster
      type: Deployment
    network:
      host_namespace: true
      ingress:
        annotations:
          nginx.ingress.kubernetes.io/proxy-read-timeout: "603"
    pod:
      replicas:
        ingress: 1
        error_page: 1
  dependencies:
    - ingress-kube-system-htk
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: ingress-kube-system-htk
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.kubernetes.ingress-htk
      dest:
        path: .source
data:
  chart_name: ingress-kube-system-htk
  release: ingress-kube-system-htk
  namespace: ingress-kube-system-htk
  values: {}
  dependencies: []
...
---
schema: armada/ChartGroup/v1
metadata:
  schema: metadata/Document/v1
  name: ingress-kube-system
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
data:
  description: Ingress for the site
  chart_group:
    - ingress-kube-system
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: kubernetes-calico-etcd-global
  layeringDefinition:
    abstract: true
    layer: global
  labels:
    name: kubernetes-calico-etcd-global
  storagePolicy: cleartext
  substitutions:

    # Chart source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.kubernetes.calico.etcd
      dest:
        path: .source

    # Image versions
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.calico.etcd
      dest:
        path: .values.images.tags

    # IP addresses
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .calico.etcd.service_ip
      dest:
        path: .values.service.ip
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .calico.etcd.service_ip
      dest:
        path: .values.anchor.etcdctl_endpoint

    # CAs
    - src:
        schema: deckhand/CertificateAuthority/v1
        name: calico-etcd
        path: .
      dest:
        path: .values.secrets.tls.client.ca
    - src:
        schema: deckhand/CertificateAuthority/v1
        name: calico-etcd-peer
        path: .
      dest:
        path: .values.secrets.tls.peer.ca

    # Anchor client cert
    - src:
        schema: deckhand/Certificate/v1
        name: calico-etcd-anchor
        path: .
      dest:
        path: .values.secrets.anchor.tls.cert
    - src:
        schema: deckhand/CertificateKey/v1
        name: calico-etcd-anchor
        path: .
      dest:
        path: .values.secrets.anchor.tls.key

data:
  chart_name: etcd
  release: kubernetes-calico-etcd
  namespace: kube-system
  protected:
    continue_processing: true
  wait:
    timeout: 600
    labels:
      release_group: airship-kubernetes-calico-etcd
  upgrade:
    no_hooks: false
    pre:
      delete:
        - type: job
          labels:
            release_group: airship-kubernetes-calico-etcd
  values:
    labels:
      anchor:
        node_selector_key: calico-etcd
        node_selector_value: enabled
    etcd:
      host_data_path: /var/lib/etcd/calico
      host_etc_path: /etc/etcd/calico
    bootstrapping:
      enabled: true
      host_directory: /var/lib/anchor
      filename: calico-etcd-bootstrap
    service:
      name: calico-etcd
    network:
      service_client:
        name: service_client
        port: 6666
        target_port: 6666
      service_peer:
        name: service_peer
        port: 6667
        target_port: 6667
  dependencies:
    - kubernetes-calico-etcd-htk
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: kubernetes-calico-etcd-htk
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.kubernetes.calico.etcd-htk
      dest:
        path: .source
data:
  chart_name: kubernetes-calico-etcd-htk
  release: kubernetes-calico-etcd-htk
  namespace: kubernetes-calico-etcd-htk
  values: {}
  dependencies: []
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: kubernetes-calico
  layeringDefinition:
    abstract: false
    layer: global
  labels:
    name: kubernetes-calico-global
  storagePolicy: cleartext
  substitutions:
    # Chart source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.kubernetes.calico.calico
      dest:
        path: .source
    # Image versions
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.calico.calico
      dest:
        path: .values.images.tags
    # IP addresses
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .calico.etcd.service_ip
      dest:
        path: .values.endpoints.etcd.host_fqdn_override.default
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .kubernetes.pod_cidr
      dest:
        path: .values.networking.podSubnet
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .kubernetes.api_service_ip
      dest:
        path: .values.conf.controllers.K8S_API
        pattern: SUB_KUBERNETES_IP

    # Other site-specific configuration
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .calico.ip_autodetection_method
      dest:
        path: .values.conf.node.IP_AUTODETECTION_METHOD

    # Certificates
    - src:
        schema: deckhand/CertificateAuthority/v1
        name: calico-etcd
        path: .
      dest:
        path: .values.endpoints.etcd.auth.client.tls.ca
    - src:
        schema: deckhand/Certificate/v1
        name: calico-node
        path: .
      dest:
        path: .values.endpoints.etcd.auth.client.tls.crt
    - src:
        schema: deckhand/CertificateKey/v1
        name: calico-node
        path: .
      dest:
        path: .values.endpoints.etcd.auth.client.tls.key

data:
  chart_name: calico
  release: kubernetes-calico
  namespace: kube-system
  protected:
    continue_processing: true
  wait:
    timeout: 600
    labels:
      release_group: airship-kubernetes-calico
  upgrade:
    no_hooks: false
    pre:
      delete:
        - type: job
          labels:
            release_group: airship-kubernetes-calico
  values:
    conf:
      cni_network_config:
        name: k8s-pod-network
        cniVersion: 0.3.0
        plugins:
          - type: calico
            etcd_endpoints: __ETCD_ENDPOINTS__
            etcd_ca_cert_file: /etc/calico/pki/ca
            etcd_cert_file: /etc/calico/pki/crt
            etcd_key_file: /etc/calico/pki/key
            log_level: info
            ipam:
              type: calico-ipam
            policy:
              type: k8s
            kubernetes:
              kubeconfig: __KUBECONFIG_FILEPATH__
          - type: portmap
            snat: true
            capabilities:
              portMappings: true

      controllers:
        K8S_API: "https://SUB_KUBERNETES_IP:443"

      node:
        CALICO_STARTUP_LOGLEVEL: INFO
        CLUSTER_TYPE: "k8s,bgp"
        ETCD_CA_CERT_FILE: /etc/calico/pki/ca
        ETCD_CERT_FILE: /etc/calico/pki/crt
        ETCD_KEY_FILE: /etc/calico/pki/key
        WAIT_FOR_STORAGE: "true"

    endpoints:
      etcd:
        hosts:
          default: calico-etcd
        scheme:
          default: https

    networking:
      mtu: 1500
      settings:
        mesh: "on"
        ippool:
          ipip:
            enabled: "true"
            mode: "Always"
          nat_outgoing: "true"
          disabled: "false"

    manifests:
      daemonset_calico_etcd: false
      job_image_repo_sync: false
      service_calico_etcd: false
  dependencies:
    - calico-htk
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: calico-htk
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.kubernetes.calico.calico-htk
      dest:
        path: .source
data:
  chart_name: calico-htk
  release: calico-htk
  namespace: calico-htk
  values: {}
  dependencies: []
...
---
schema: armada/ChartGroup/v1
metadata:
  schema: metadata/Document/v1
  name: kubernetes-container-networking
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
data:
  description: Container networking via Calico
  sequenced: true
  chart_group:
    - kubernetes-calico-etcd
    - kubernetes-calico
...
---
schema: armada/ChartGroup/v1
metadata:
  schema: metadata/Document/v1
  name: kubernetes-core
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
data:
  description: Kubernetes components
  chart_group:
    - kubernetes-apiserver
    - kubernetes-controller-manager
    - kubernetes-scheduler
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: kubernetes-controller-manager
  labels:
    name: kubernetes-controller-manager-global
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    # Chart source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.kubernetes.controller-manager
      dest:
        path: .source

    # Images
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.kubernetes.controller-manager
      dest:
        path: .values.images.tags

    # IP addresses
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .kubernetes.pod_cidr
      dest:
        path: .values.network.pod_cidr
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .kubernetes.service_cidr
      dest:
        path: .values.network.service_cidr
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .kubernetes.pod_cidr
      dest:
        path: .values.command_prefix[1]
        pattern: SUB_POD_CIDR
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .kubernetes.service_cidr
      dest:
        path: .values.command_prefix[2]
        pattern: SUB_SERVICE_CIDR

    # CA
    - src:
        schema: deckhand/CertificateAuthority/v1
        name: kubernetes
        path: .
      dest:
        path: .values.secrets.tls.ca

    # Certificates
    - src:
        schema: deckhand/Certificate/v1
        name: controller-manager
        path: .
      dest:
        path: .values.secrets.tls.cert
    - src:
        schema: deckhand/CertificateKey/v1
        name: controller-manager
        path: .
      dest:
        path: .values.secrets.tls.key

    # Private key for Kubernetes service account token signing
    - src:
        schema: deckhand/PrivateKey/v1
        name: service-account
        path: .
      dest:
        path: .values.secrets.service_account.private_key

data:
  chart_name: controller-manager
  release: kubernetes-controller-manager
  namespace: kube-system
  protected:
    continue_processing: true
  wait:
    timeout: 600
    labels:
      release_group: airship-kubernetes-controller-manager
  upgrade:
    no_hooks: false
    pre:
      delete:
        - type: job
          labels:
            release_group: airship-kubernetes-controller-manager
  values:
    command_prefix:
      - /controller-manager
      - --cluster-cidr=SUB_POD_CIDR
      - --service-cluster-ip-range=SUB_SERVICE_CIDR
      - --node-monitor-period=5s
      - --node-monitor-grace-period=20s
      - --pod-eviction-timeout=60s
    network:
      kubernetes_netloc: 127.0.0.1:6553
  dependencies:
    - kubernetes-controller-manager-htk
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: kubernetes-controller-manager-htk
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.kubernetes.controller-manager-htk
      dest:
        path: .source
data:
  chart_name: kubernetes-controller-manager-htk
  release: kubernetes-controller-manager-htk
  namespace: kubernetes-controller-manager-htk
  values: {}
  dependencies: []
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: kubernetes-apiserver
  labels:
    name: kubernetes-apiserver-global
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    # Chart source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.kubernetes.apiserver
      dest:
        path: .source

    # Images
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.kubernetes.apiserver
      dest:
        path: .values.images.tags

    # IP addresses
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .kubernetes.api_service_ip
      dest:
        path: .values.network.kubernetes_service_ip
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .kubernetes.pod_cidr
      dest:
        path: .values.network.pod_cidr
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .kubernetes.service_cidr
      dest:
        path: .values.apiserver.arguments[1]
        pattern: SERVICE_CIDR

    # Kubernetes Port Range
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .kubernetes.service_node_port_range
      dest:
        path: .values.apiserver.arguments[2]
        pattern: SERVICE_NODE_PORT_RANGE

    # CA
    - src:
        schema: deckhand/CertificateAuthority/v1
        name: kubernetes
        path: .
      dest:
        path: .values.secrets.tls.ca

    # Certificates
    - src:
        schema: deckhand/Certificate/v1
        name: apiserver
        path: .
      dest:
        path: .values.secrets.tls.cert
    - src:
        schema: deckhand/CertificateKey/v1
        name: apiserver
        path: .
      dest:
        path: .values.secrets.tls.key
    - src:
        schema: deckhand/CertificateAuthority/v1
        name: kubernetes-etcd
        path: .
      dest:
        path: .values.secrets.etcd.tls.ca
    - src:
        schema: deckhand/Certificate/v1
        name: apiserver-etcd
        path: .
      dest:
        path: .values.secrets.etcd.tls.cert
    - src:
        schema: deckhand/CertificateKey/v1
        name: apiserver-etcd
        path: .
      dest:
        path: .values.secrets.etcd.tls.key
    - src:
        schema: deckhand/PublicKey/v1
        name: service-account
        path: .
      dest:
        path: .values.secrets.service_account.public_key

    # Encryption policy
    - src:
        schema: promenade/EncryptionPolicy/v1
        name: encryption-policy
        path: .etcd
      dest:
        path: .values.conf.encryption_provider.content.resources

data:
  chart_name: apiserver
  release: kubernetes-apiserver
  namespace: kube-system
  protected:
    continue_processing: true
  wait:
    timeout: 600
    labels:
      release_group: airship-kubernetes-apiserver
  upgrade:
    no_hooks: false
    pre:
      delete:
        - type: job
          labels:
            release_group: airship-kubernetes-apiserver
  values:
    apiserver:
      etcd:
        endpoints: https://127.0.0.1:2378
      tls:
        tls-cipher-suites: "TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA,TLS_RSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_CBC_SHA,TLS_RSA_WITH_AES_256_CBC_SHA"
        # https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/
        # Possible values: VersionTLS10, VersionTLS11, VersionTLS12
        tls-min-version: 'VersionTLS12'
      arguments:
        - --authorization-mode=Node,RBAC
        - --service-cluster-ip-range=SERVICE_CIDR
        - --service-node-port-range=SERVICE_NODE_PORT_RANGE
        - --endpoint-reconciler-type=lease
        - --feature-gates=PodShareProcessNamespace=true
        - --v=3
    conf:
      encryption_provider:
        file: encryption_provider.yaml
        command_options:
          - '--experimental-encryption-provider-config=/etc/kubernetes/apiserver/encryption_provider.yaml'
        content:
          kind: EncryptionConfig
          apiVersion: v1
      eventconfig:
        file: eventconfig.yaml
        content:
          kind: Configuration
          apiVersion: eventratelimit.admission.k8s.io/v1alpha1
          limits:
          - type: Server
            qps: 100
            burst: 1000
      acconfig:
        file: acconfig.yaml
        command_options:
          - '--enable-admission-plugins=PodSecurityPolicy,NamespaceLifecycle,LimitRanger,ServiceAccount,PersistentVolumeLabel,DefaultStorageClass,ResourceQuota,DefaultTolerationSeconds,NodeRestriction,EventRateLimit'
          - '--admission-control-config-file=/etc/kubernetes/apiserver/acconfig.yaml'
        content:
          kind: AdmissionConfiguration
          apiVersion: apiserver.k8s.io/v1alpha1
          plugins:
          - name: EventRateLimit
            path: eventconfig.yaml
  dependencies:
    - kubernetes-apiserver-htk
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: kubernetes-apiserver-htk
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.kubernetes.apiserver-htk
      dest:
        path: .source
data:
  chart_name: kubernetes-apiserver-htk
  release: kubernetes-apiserver-htk
  namespace: kubernetes-apiserver-htk
  values: {}
  dependencies: []
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: kubernetes-scheduler
  labels:
    name: kubernetes-scheduler-global
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    # Chart source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.kubernetes.scheduler
      dest:
        path: .source

    # Images
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.kubernetes.scheduler
      dest:
        path: .values.images.tags

    # CA
    - src:
        schema: deckhand/CertificateAuthority/v1
        name: kubernetes
        path: .
      dest:
        path: .values.secrets.tls.ca

    # Certificates
    - src:
        schema: deckhand/Certificate/v1
        name: scheduler
        path: .
      dest:
        path: .values.secrets.tls.cert
    - src:
        schema: deckhand/CertificateKey/v1
        name: scheduler
        path: .
      dest:
        path: .values.secrets.tls.key

data:
  chart_name: scheduler
  release: kubernetes-scheduler
  namespace: kube-system
  protected:
    continue_processing: true
  wait:
    timeout: 600
    labels:
      release_group: airship-kubernetes-scheduler
  upgrade:
    no_hooks: false
    pre:
      delete:
        - type: job
          labels:
            release_group: airship-kubernetes-scheduler
  values:
    network:
      kubernetes_netloc: 127.0.0.1:6553
  dependencies:
    - kubernetes-scheduler-htk
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: kubernetes-scheduler-htk
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.kubernetes.scheduler-htk
      dest:
        path: .source
data:
  chart_name: kubernetes-scheduler-htk
  release: kubernetes-scheduler-htk
  namespace: kubernetes-scheduler-htk
  values: {}
  dependencies: []
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: coredns
  layeringDefinition:
    abstract: false
    layer: global
  labels:
    name: coredns-global
  storagePolicy: cleartext
  substitutions:
    # Chart source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.kubernetes.coredns
      dest:
        path: .source

    # Images
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.kubernetes.coredns
      dest:
        path: .values.images.tags

    # IP Addresses
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .dns.service_ip
      dest:
        path: .values.service.ip

    # Zones
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .dns.cluster_domain
      dest:
        path: .values.conf.coredns.corefile
        pattern: '(CLUSTER_DOMAIN)'
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .kubernetes.service_cidr
      dest:
        path: .values.conf.coredns.corefile
        pattern: '(SERVICE_CIDR)'
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .kubernetes.pod_cidr
      dest:
        path:  .values.conf.coredns.corefile
        pattern: '(POD_CIDR)'

    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .dns.upstream_servers[0]
      dest:
        path: .values.conf.coredns.corefile
        pattern: '(UPSTREAM1)'
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .dns.upstream_servers[1]
      dest:
        path: .values.conf.coredns.corefile
        pattern: '(UPSTREAM2)'
data:
  chart_name: coredns
  release: coredns
  namespace: kube-system
  wait:
    timeout: 600
    labels:
      release_group: airship-coredns
  upgrade:
    no_hooks: false
    pre:
      delete:
        - type: job
          labels:
            release_group: airship-coredns
  values:
    conf:
      coredns:
        corefile: |
          .:53 {
              errors
              health
              autopath @kubernetes
              kubernetes CLUSTER_DOMAIN SERVICE_CIDR POD_CIDR {
                pods insecure
                fallthrough in-addr.arpa ip6.arpa
                upstream UPSTREAM1
                upstream UPSTREAM2
              }
              prometheus :9153
              forward . UPSTREAM1 UPSTREAM2
              cache 30
          }

    labels:
      coredns:
        node_selector_key: kube-dns
        node_selector_value: enabled

  dependencies:
    - coredns-htk
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: coredns-htk
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.kubernetes.coredns-htk
      dest:
        path: .source
data:
  chart_name: coredns-htk
  release: coredns-htk
  namespace: coredns-htk
  values:
    pod:
      # TODO: replicas can be removed once we switch coredns to
      # DaemonSet-only.  It will be deployed with both DaemonSet
      # and Deployment-managed pods as we transition to DaemonSet.
      replicas:
        coredns: 2
    manifests:
      daemonset: true
      # TODO: `deployment` can be set to false once we switch coredns to
      # DaemonSet-only.  It will be deployed with both DaemonSet
      # and Deployment-managed pods as we transition to DaemonSet.
      deployment: true
  dependencies: []
...
---
schema: armada/ChartGroup/v1
metadata:
  schema: metadata/Document/v1
  name: kubernetes-dns
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
data:
  description: Cluster DNS
  chart_group:
    - coredns
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: osh-infra-helm-toolkit
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.osh_infra.helm_toolkit
      dest:
        path: .source
data:
  chart_name: helm-toolkit
  release: osh-infra-helm-toolkit
  namespace: osh-infra-helm-toolkit
  wait:
    timeout: 600
    labels:
      release_group: airship-osh-infra-helm-toolkit
  upgrade:
    no_hooks: true
  values: {}
  dependencies: []
...
---
schema: armada/ChartGroup/v1
metadata:
  schema: metadata/Document/v1
  name: osh-infra-mariadb
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
data:
  description: OpenStack-Infra MariaDB
  chart_group:
    - osh-infra-mariadb
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: osh-infra-mariadb
  labels:
    name: osh-infra-mariadb-global
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    # Chart source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.osh.mariadb
      dest:
        path: .source

    # Images
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.osh.mariadb
      dest:
        path: .values.images.tags

    # Endpoints
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_infra_endpoints
        path: .osh_infra.oslo_db
      dest:
        path: .values.endpoints.oslo_db
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_infra_endpoints
        path: .osh_infra.prometheus_mysql_exporter
      dest:
        path: .values.endpoints.prometheus_mysql_exporter

    # Accounts
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_infra_service_accounts
        path: .osh_infra.oslo_db.admin
      dest:
        path: .values.endpoints.oslo_db.auth.admin
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_infra_service_accounts
        path: .osh_infra.prometheus_mysql_exporter.user
      dest:
        path: .values.endpoints.prometheus_mysql_exporter.auth.user

    # Secrets
    - dest:
        path: .values.endpoints.oslo_db.auth.admin.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_infra_oslo_db_admin_password
        path: .
    - dest:
        path: .values.endpoints.oslo_db.auth.exporter.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_infra_oslo_db_exporter_password
        path: .
data:
  chart_name: osh-infra-mariadb
  release: osh-infra-mariadb
  namespace: osh-infra
  wait:
    timeout: 900
    labels:
      release_group: airship-osh-infra-mariadb
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
    pre:
      delete:
        - type: job
          labels:
            release_group: airship-osh-infra-mariadb
  values:
    labels:
      server:
        node_selector_key: openstack-control-plane
        node_selector_value: enabled
      prometheus_mysql_exporter:
        node_selector_key: openstack-control-plane
        node_selector_value: enabled
    monitoring:
      prometheus:
        enabled: true
  dependencies:
    - osh-helm-toolkit
...
---
schema: armada/ChartGroup/v1
metadata:
  schema: metadata/Document/v1
  name: osh-infra-ceph-config
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
data:
  description: Ceph config for OpenStack-Infra namespace(s)
  chart_group:
    - osh-infra-ceph-config
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: osh-infra-ceph-config
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    # Chart source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.ucp.ceph-provisioners
      dest:
        path: .source

    # Images
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.ceph.ceph-provisioners
      dest:
        path: .values.images.tags

    # IP addresses
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .storage.ceph.public_cidr
      dest:
        path: .values.network.public
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .storage.ceph.cluster_cidr
      dest:
        path: .values.network.cluster

    # Endpoints
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: ucp_endpoints
        path: .ceph.ceph_mon
      dest:
        path: .values.endpoints.ceph_mon


data:
  chart_name: osh-infra-ceph-config
  release: osh-infra-ceph-config
  namespace: osh-infra
  wait:
    timeout: 900
    labels:
      release_group: airship-osh-infra-ceph-config
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
    pre:
      delete:
        - type: job
          labels:
            release_group: airship-osh-infra-ceph-config
  values:
    labels:
      job:
        node_selector_key: openstack-control-plane
        node_selector_value: enabled
      provisioner:
        node_selector_key: openstack-control-plane
        node_selector_value: enabled
    deployment:
      ceph: false
      client_secrets: true
      rbd_provisioner: false
      cephfs_provisioner: false
      rgw_keystone_user_and_endpoints: false
    bootstrap:
      enabled: false
    storageclass:
      rbd:
        ceph_configmap_name: ceph-etc
        parameters:
          userSecretName: pvc-ceph-client-key
      cephfs:
        provision_storage_class: false
  dependencies:
    - ceph-htk
...
---
schema: armada/ChartGroup/v1
metadata:
  schema: metadata/Document/v1
  name: osh-infra-radosgw
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
data:
  description: Deploy Radosgw for OSH-Infra
  chart_group:
    - osh-infra-radosgw
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: osh-infra-radosgw
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    # Chart source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.ucp.ceph-rgw
      dest:
        path: .source

    # Images
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.ceph.ceph-rgw
      dest:
        path: .values.images.tags

    # IP addresses
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .storage.ceph.public_cidr
      dest:
        path: .values.network.public
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .storage.ceph.cluster_cidr
      dest:
        path: .values.network.cluster

    # Endpoints
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_infra_endpoints
        path: .osh_infra.ceph_object_store
      dest:
        path: .values.endpoints.ceph_object_store
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: ucp_endpoints
        path: .ceph.ceph_mon
      dest:
        path: .values.endpoints.ceph_mon

    # Credentials
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_infra_service_accounts
        path: .osh_infra.ceph_object_store.admin
      dest:
        path: .values.endpoints.ceph_object_store.auth.admin

    # Secrets
    - dest:
        path: .values.endpoints.ceph_object_store.auth.admin.access_key
      src:
        schema: deckhand/Passphrase/v1
        name: osh_infra_rgw_s3_admin_access_key
        path: .
    - dest:
        path: .values.endpoints.ceph_object_store.auth.admin.secret_key
      src:
        schema: deckhand/Passphrase/v1
        name: osh_infra_rgw_s3_admin_secret_key
        path: .

data:
  chart_name: osh-infra-radosgw
  release: osh-infra-radosgw
  namespace: osh-infra
  wait:
    timeout: 900
    labels:
      release_group: airship-osh-infra-radosgw
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
    pre:
      delete:
        - type: job
          labels:
            release_group: airship-osh-infra-radosgw
  values:
    labels:
      job:
        node_selector_key: openstack-control-plane
        node_selector_value: enabled
      rgw:
        node_selector_key: ceph-rgw
        node_selector_value: enabled
    deployment:
      storage_secrets: false
      ceph: true
      rbd_provisioner: false
      cephfs_provisioner: false
      client_secrets: false
      rgw_keystone_user_and_endpoints: false
    bootstrap:
      enabled: false
    conf:
      rgw_s3:
        enabled: true
    ceph_client:
      configmap: ceph-etc
  dependencies:
    - osh-infra-helm-toolkit
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: prometheus-kube-state-metrics
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    # Chart source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.osh_infra.prometheus_kube_state_metrics
      dest:
        path: .source

    # Images
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.osh_infra.prometheus_kube_state_metrics
      dest:
        path: .values.images.tags

    # Endpoints
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_infra_endpoints
        path: .osh_infra.kube_state_metrics
      dest:
        path: .values.endpoints.kube_state_metrics
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_infra_endpoints
        path: .osh_infra.kube_scheduler
      dest:
        path: .values.endpoints.kube_scheduler
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_infra_endpoints
        path: .osh_infra.kube_controller_manager
      dest:
        path: .values.endpoints.kube_controller_manager

data:
  chart_name: prometheus-kube-state-metrics
  release: prometheus-kube-state-metrics
  namespace: kube-system
  wait:
    timeout: 900
    labels:
      release_group: airship-prometheus-kube-state-metrics
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
    pre:
      delete:
        - type: job
          labels:
            release_group: airship-prometheus-kube-state-metrics
      create: []
    post:
      create: []
  values:
    labels:
      kube_state_metrics:
        node_selector_key: openstack-control-plane
        node_selector_value: enabled
      job:
        node_selector_key: openstack-control-plane
        node_selector_value: enabled
  dependencies:
    - osh-infra-helm-toolkit
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: prometheus
  labels:
    name: prometheus-global
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    # Chart source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.osh_infra.prometheus
      dest:
        path: .source

    # Images
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.osh_infra.prometheus
      dest:
        path: .values.images.tags

    # Endpoints
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_infra_endpoints
        path: .osh_infra.monitoring
      dest:
        path: .values.endpoints.monitoring
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_infra_endpoints
        path: .osh_infra.alerts
      dest:
        path: .values.endpoints.alerts
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_infra_endpoints
        path: .osh_infra.ldap
      dest:
        path: .values.endpoints.ldap

    # Accounts
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_infra_service_accounts
        path: .osh_infra.prometheus.admin
      dest:
        path: .values.endpoints.monitoring.auth.admin

    # Secrets
    - dest:
        path: .values.endpoints.monitoring.auth.admin.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_infra_prometheus_admin_password
        path: .

    # LDAP Details
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_infra_service_accounts
        path: .osh_infra.ldap.admin
      dest:
        path: .values.endpoints.ldap.auth.admin
    - dest:
        path: .values.endpoints.ldap.auth.admin.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_keystone_ldap_password
        path: .

data:
  chart_name: prometheus
  release: prometheus
  namespace: osh-infra
  wait:
    timeout: 900
    labels:
      release_group: airship-prometheus
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
    pre:
      delete:
        - type: job
          labels:
            release_group: airship-prometheus
      create: []
    post:
      create: []
  values:
    manifests:
      ingress: false
      service_ingress: false
    labels:
      prometheus:
        node_selector_key: openstack-control-plane
        node_selector_value: enabled
      job:
        node_selector_key: openstack-control-plane
        node_selector_value: enabled
    pod:
      replicas:
        prometheus: 3
      resources:
        enabled: true
        prometheus:
          limits:
            memory: "64Gi"
            cpu: "4000m"
          requests:
            memory: "16Gi"
            cpu: "2000m"
    storage:
      requests:
        storage: 500Gi
    conf:
      prometheus:
        command_line_flags:
          storage.tsdb.max_block_duration: 17h
        scrape_configs:
          global:
            scrape_interval: 60s
            evaluation_interval: 60s
          scrape_configs:
            # NOTE(srwilkers): The job definition for Prometheus should always be
            # listed first, so we can inject the basic auth username and password
            # via the endpoints section
            - job_name: 'prometheus-metrics'
              kubernetes_sd_configs:
              - role: endpoints
              scrape_interval: 60s
              relabel_configs:
              - source_labels:
                  - __meta_kubernetes_service_name
                action: keep
                regex: "prom-metrics"
              - source_labels:
                  - __meta_kubernetes_service_annotation_prometheus_io_scrape
                action: keep
                regex: true
              - source_labels:
                  - __meta_kubernetes_service_annotation_prometheus_io_scheme
                action: replace
                target_label: __scheme__
                regex: (https?)
              - source_labels:
                  - __meta_kubernetes_service_annotation_prometheus_io_path
                action: replace
                target_label: __metrics_path__
                regex: (.+)
              - source_labels:
                  - __address__
                  - __meta_kubernetes_service_annotation_prometheus_io_port
                action: replace
                target_label: __address__
                regex: ([^:]+)(?::\d+)?;(\d+)
                replacement: $1:$2
              - action: labelmap
                regex: __meta_kubernetes_service_label_(.+)
              - source_labels:
                  - __meta_kubernetes_namespace
                action: replace
                target_label: kubernetes_namespace
              - source_labels:
                  - __meta_kubernetes_service_name
                action: replace
                target_label: instance
              - source_labels:
                  - __meta_kubernetes_service_name
                action: replace
                target_label: kubernetes_name
              - source_labels:
                  - __meta_kubernetes_service_name
                target_label: job
                replacement: ${1}
            - job_name: kubelet
              scheme: https
              # This TLS & bearer token file config is used to connect to the actual scrape
              # endpoints for cluster components. This is separate to discovery auth
              # configuration because discovery & scraping are two separate concerns in
              # Prometheus. The discovery auth config is automatic if Prometheus runs inside
              # the cluster. Otherwise, more config options have to be provided within the
              # <kubernetes_sd_config>.
              tls_config:
                ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
              bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
              kubernetes_sd_configs:
              - role: node
              scrape_interval: 45s
              relabel_configs:
              - action: labelmap
                regex: __meta_kubernetes_node_label_(.+)
              - target_label: __address__
                replacement: kubernetes.default.svc:443
              - source_labels:
                  - __meta_kubernetes_node_name
                regex: (.+)
                target_label: __metrics_path__
                replacement: /api/v1/nodes/${1}/proxy/metrics
              - source_labels:
                  - __meta_kubernetes_node_name
                action: replace
                target_label: kubernetes_io_hostname
              # Scrape config for Kubelet cAdvisor.
              #
              # This is required for Kubernetes 1.7.3 and later, where cAdvisor metrics
              # (those whose names begin with 'container_') have been removed from the
              # Kubelet metrics endpoint.  This job scrapes the cAdvisor endpoint to
              # retrieve those metrics.
              #
              # In Kubernetes 1.7.0-1.7.2, these metrics are only exposed on the cAdvisor
              # HTTP endpoint; use "replacement: /api/v1/nodes/${1}:4194/proxy/metrics"
              # in that case (and ensure cAdvisor's HTTP server hasn't been disabled with
              # the --cadvisor-port=0 Kubelet flag).
              #
              # This job is not necessary and should be removed in Kubernetes 1.6 and
              # earlier versions, or it will cause the metrics to be scraped twice.
            - job_name: 'kubernetes-cadvisor'

              # Default to scraping over https. If required, just disable this or change to
              # `http`.
              scheme: https

              # This TLS & bearer token file config is used to connect to the actual scrape
              # endpoints for cluster components. This is separate to discovery auth
              # configuration because discovery & scraping are two separate concerns in
              # Prometheus. The discovery auth config is automatic if Prometheus runs inside
              # the cluster. Otherwise, more config options have to be provided within the
              # <kubernetes_sd_config>.
              tls_config:
                ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
              bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

              kubernetes_sd_configs:
              - role: node

              relabel_configs:
              - action: labelmap
                regex: __meta_kubernetes_node_label_(.+)
              - target_label: __address__
                replacement: kubernetes.default.svc:443
              - source_labels:
                  - __meta_kubernetes_node_name
                regex: (.+)
                target_label: __metrics_path__
                replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor
              metric_relabel_configs:
              - source_labels:
                  - __name__
                regex: 'container_network_tcp_usage_total'
                action: drop
              - source_labels:
                  - __name__
                regex: 'container_tasks_state'
                action: drop
              - source_labels:
                  - __name__
                regex: 'container_network_udp_usage_total'
                action: drop
              - source_labels:
                  - __name__
                regex: 'container_memory_failures_total'
                action: drop
              - source_labels:
                  - __name__
                regex: 'container_cpu_load_average_10s'
                action: drop
              - source_labels:
                  - __name__
                regex: 'container_cpu_system_seconds_total'
                action: drop
              - source_labels:
                  - __name__
                regex: 'container_cpu_user_seconds_total'
                action: drop
              - source_labels:
                  - __name__
                regex: 'container_fs_inodes_free'
                action: drop
              - source_labels:
                  - __name__
                regex: 'container_fs_inodes_total'
                action: drop
              - source_labels:
                  - __name__
                regex: 'container_fs_io_current'
                action: drop
              - source_labels:
                  - __name__
                regex: 'container_fs_io_time_seconds_total'
                action: drop
              - source_labels:
                  - __name__
                regex: 'container_fs_io_time_weighted_seconds_total'
                action: drop
              - source_labels:
                  - __name__
                regex: 'container_fs_read_seconds_total'
                action: drop
              - source_labels:
                  - __name__
                regex: 'container_fs_reads_merged_total'
                action: drop
              - source_labels:
                  - __name__
                regex: 'container_fs_reads_merged_total'
                action: drop
              - source_labels:
                  - __name__
                regex: 'container_fs_reads_total'
                action: drop
              - source_labels:
                  - __name__
                regex: 'container_fs_sector_reads_total'
                action: drop
              - source_labels:
                  - __name__
                regex: 'container_fs_sector_writes_total'
                action: drop
              - source_labels:
                  - __name__
                regex: 'container_fs_write_seconds_total'
                action: drop
              - source_labels:
                  - __name__
                regex: 'container_fs_writes_bytes_total'
                action: drop
              - source_labels:
                  - __name__
                regex: 'container_fs_writes_merged_total'
                action: drop
              - source_labels:
                  - __name__
                regex: 'container_fs_writes_total'
                action: drop
              - source_labels:
                  - __name__
                regex: 'container_last_seen'
                action: drop
              - source_labels:
                  - __name__
                regex: 'container_memory_cache'
                action: drop
              - source_labels:
                  - __name__
                regex: 'container_memory_failcnt'
                action: drop
              - source_labels:
                  - __name__
                regex: 'container_memory_max_usage_bytes'
                action: drop
              - source_labels:
                  - __name__
                regex: 'container_memory_rss'
                action: drop
              - source_labels:
                  - __name__
                regex: 'container_memory_swap'
                action: drop
              - source_labels:
                  - __name__
                regex: 'container_memory_usage_bytes'
                action: drop
              - source_labels:
                  - __name__
                regex: 'container_network_receive_errors_total'
                action: drop
              - source_labels:
                  - __name__
                regex: 'container_network_receive_packets_dropped_total'
                action: drop
              - source_labels:
                  - __name__
                regex: 'container_network_receive_packets_total'
                action: drop
              - source_labels:
                  - __name__
                regex: 'container_network_transmit_errors_total'
                action: drop
              - source_labels:
                  - __name__
                regex: 'container_network_transmit_packets_dropped_total'
                action: drop
              - source_labels:
                  - __name__
                regex: 'container_network_transmit_packets_total'
                action: drop
              - source_labels:
                  - __name__
                regex: 'container_spec_cpu_period'
                action: drop
              - source_labels:
                  - __name__
                regex: 'container_spec_cpu_shares'
                action: drop
              - source_labels:
                  - __name__
                regex: 'container_spec_memory_limit_bytes'
                action: drop
              - source_labels:
                  - __name__
                regex: 'container_spec_memory_reservation_limit_bytes'
                action: drop
              - source_labels:
                  - __name__
                regex: 'container_spec_memory_swap_limit_bytes'
                action: drop
              - source_labels:
                  - __name__
                regex: 'container_start_time_seconds'
                action: drop
              # Scrape config for API servers.
              #
              # Kubernetes exposes API servers as endpoints to the default/kubernetes
              # service so this uses `endpoints` role and uses relabelling to only keep
              # the endpoints associated with the default/kubernetes service using the
              # default named port `https`. This works for single API server deployments as
              # well as HA API server deployments.
            - job_name: 'apiserver'
              kubernetes_sd_configs:
              - role: endpoints
              scrape_interval: 45s
              # Default to scraping over https. If required, just disable this or change to
              # `http`.
              scheme: https
              # This TLS & bearer token file config is used to connect to the actual scrape
              # endpoints for cluster components. This is separate to discovery auth
              # configuration because discovery & scraping are two separate concerns in
              # Prometheus. The discovery auth config is automatic if Prometheus runs inside
              # the cluster. Otherwise, more config options have to be provided within the
              # <kubernetes_sd_config>.
              tls_config:
                ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
                # If your node certificates are self-signed or use a different CA to the
                # master CA, then disable certificate verification below. Note that
                # certificate verification is an integral part of a secure infrastructure
                # so this should only be disabled in a controlled environment. You can
                # disable certificate verification by uncommenting the line below.
                #
                # insecure_skip_verify: true
              bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
              # Keep only the default/kubernetes service endpoints for the https port. This
              # will add targets for each API server which Kubernetes adds an endpoint to
              # the default/kubernetes service.
              relabel_configs:
              - source_labels:
                  - __meta_kubernetes_namespace
                  - __meta_kubernetes_service_name
                  - __meta_kubernetes_endpoint_port_name
                action: keep
                regex: default;kubernetes;https
              metric_relabel_configs:
              - source_labels:
                  - __name__
                regex: 'apiserver_admission_controller_admission_latencies_seconds_bucket'
                action: drop
              - source_labels:
                  - __name__
                regex: 'rest_client_request_latency_seconds_bucket'
                action: drop
              - source_labels:
                  - __name__
                regex: 'apiserver_response_sizes_bucket'
                action: drop
              - source_labels:
                  - __name__
                regex: 'apiserver_admission_step_admission_latencies_seconds_bucket'
                action: drop
              - source_labels:
                  - __name__
                regex: 'apiserver_admission_controller_admission_latencies_seconds_count'
                action: drop
              - source_labels:
                  - __name__
                regex: 'apiserver_admission_controller_admission_latencies_seconds_sum'
                action: drop
              - source_labels:
                  - __name__
                regex: 'apiserver_request_latencies_summary'
                action: drop
            # Scrape config for service endpoints.
            #
            # The relabeling allows the actual service scrape endpoint to be configured
            # via the following annotations:
            #
            # * `prometheus.io/scrape`: Only scrape services that have a value of `true`
            # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need
            # to set this to `https` & most likely set the `tls_config` of the scrape config.
            # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
            # * `prometheus.io/port`: If the metrics are exposed on a different port to the
            # service then set this appropriately.
            - job_name: 'openstack-exporter'
              kubernetes_sd_configs:
              - role: endpoints
              scrape_interval: 60s
              relabel_configs:
              - source_labels:
                  - __meta_kubernetes_service_name
                action: keep
                regex: "openstack-metrics"
              - source_labels:
                  - __meta_kubernetes_service_annotation_prometheus_io_scrape
                action: keep
                regex: true
              - source_labels:
                  - __meta_kubernetes_service_annotation_prometheus_io_scheme
                action: replace
                target_label: __scheme__
                regex: (https?)
              - source_labels:
                  - __meta_kubernetes_service_annotation_prometheus_io_path
                action: replace
                target_label: __metrics_path__
                regex: (.+)
              - source_labels:
                  - __address__
                  - __meta_kubernetes_service_annotation_prometheus_io_port
                action: replace
                target_label: __address__
                regex: ([^:]+)(?::\d+)?;(\d+)
                replacement: $1:$2
              - action: labelmap
                regex: __meta_kubernetes_service_label_(.+)
              - source_labels:
                  - __meta_kubernetes_namespace
                action: replace
                target_label: kubernetes_namespace
              - source_labels:
                  - __meta_kubernetes_service_name
                action: replace
                target_label: instance
              - source_labels:
                  - __meta_kubernetes_service_name
                action: replace
                target_label: kubernetes_name
              - source_labels:
                  - __meta_kubernetes_service_name
                target_label: job
                replacement: ${1}
            - job_name: 'kubernetes-service-endpoints'
              kubernetes_sd_configs:
              - role: endpoints
              scrape_interval: 60s
              relabel_configs:
              - source_labels:
                  - __meta_kubernetes_service_name
                action: drop
                regex: '(openstack-metrics|prom-metrics)'
              - source_labels:
                  - __meta_kubernetes_service_annotation_prometheus_io_scrape
                action: keep
                regex: true
              - source_labels:
                  - __meta_kubernetes_service_annotation_prometheus_io_scheme
                action: replace
                target_label: __scheme__
                regex: (https?)
              - source_labels:
                  - __meta_kubernetes_service_annotation_prometheus_io_path
                action: replace
                target_label: __metrics_path__
                regex: (.+)
              - source_labels:
                  - __address__
                  - __meta_kubernetes_service_annotation_prometheus_io_port
                action: replace
                target_label: __address__
                regex: ([^:]+)(?::\d+)?;(\d+)
                replacement: $1:$2
              - action: labelmap
                regex: __meta_kubernetes_service_label_(.+)
              - source_labels:
                  - __meta_kubernetes_namespace
                action: replace
                target_label: kubernetes_namespace
              - source_labels:
                  - __meta_kubernetes_service_name
                action: replace
                target_label: kubernetes_name
              - source_labels:
                  - __meta_kubernetes_service_name
                target_label: job
                replacement: ${1}
            # Example scrape config for pods
            #
            # The relabeling allows the actual pod scrape endpoint to be configured via the
            # following annotations:
            #
            # * `prometheus.io/scrape`: Only scrape pods that have a value of `true`
            # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
            # * `prometheus.io/port`: Scrape the pod on the indicated port instead of the
            # pod's declared ports (default is a port-free target if none are declared).
            - job_name: 'kubernetes-pods'
              kubernetes_sd_configs:
              - role: pod
              relabel_configs:
              - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
                action: keep
                regex: true
              - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
                action: replace
                target_label: __metrics_path__
                regex: (.+)
              - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
                action: replace
                regex: ([^:]+)(?::\d+)?;(\d+)
                replacement: $1:$2
                target_label: __address__
              - action: labelmap
                regex: __meta_kubernetes_pod_label_(.+)
              - source_labels: [__meta_kubernetes_namespace]
                action: replace
                target_label: kubernetes_namespace
              - source_labels: [__meta_kubernetes_pod_name]
                action: replace
                target_label: kubernetes_pod_name
            - job_name: calico-etcd
              kubernetes_sd_configs:
              - role: service
              scrape_interval: 20s
              relabel_configs:
              - action: labelmap
                regex: __meta_kubernetes_service_label_(.+)
              - action: keep
                source_labels:
                  - __meta_kubernetes_service_name
                regex: "calico-etcd"
              - action: keep
                source_labels:
                  - __meta_kubernetes_namespace
                regex: kube-system
                target_label: namespace
              - source_labels:
                  - __meta_kubernetes_pod_name
                target_label: pod
              - source_labels:
                  - __meta_kubernetes_service_name
                target_label: service
              - source_labels:
                  - __meta_kubernetes_service_name
                target_label: job
                replacement: ${1}
              - source_labels:
                  - __meta_kubernetes_service_label
                target_label: job
                regex: calico-etcd
                replacement: ${1}
              - target_label: endpoint
                replacement: "calico-etcd"
          alerting:
            alertmanagers:
            - kubernetes_sd_configs:
                - role: pod
              tls_config:
                ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
              bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
              relabel_configs:
              - source_labels: [__meta_kubernetes_pod_label_application]
                regex: alertmanager
                action: keep
              - source_labels: [__meta_kubernetes_pod_container_port_name]
                regex: alerts-api
                action: keep
              - source_labels: [__meta_kubernetes_pod_container_port_name]
                regex: peer-mesh
                action: drop
        rules:
          alertmanager:
            groups:
            - name: alertmanager.rules
              rules:
              - alert: AlertmanagerConfigInconsistent
                expr: count_values("config_hash", alertmanager_config_hash) BY (service) / ON(service) GROUP_LEFT() label_replace(prometheus_operator_alertmanager_spec_replicas, "service", "alertmanager-$1", "alertmanager", "(.*)") != 1
                for: 5m
                labels:
                  severity: critical
                annotations:
                  description: The configuration of the instances of the Alertmanager cluster `{{$labels.service}}` are out of sync.
                  summary: Alertmanager configurations are inconsistent
              - alert: AlertmanagerDownOrMissing
                expr: label_replace(prometheus_operator_alertmanager_spec_replicas, "job", "alertmanager-$1", "alertmanager", "(.*)") / ON(job) GROUP_RIGHT() sum(up) BY (job) != 1
                for: 5m
                labels:
                  severity: warning
                annotations:
                  description: An unexpected number of Alertmanagers are scraped or Alertmanagers disappeared from discovery.
                  summary: Alertmanager down or not discovered
              - alert: FailedReload
                expr: alertmanager_config_last_reload_successful == 0
                for: 10m
                labels:
                  severity: warning
                annotations:
                  description: Reloading Alertmanager's configuration has failed for {{ $labels.namespace }}/{{ $labels.pod}}.
                  summary: Alertmanager configuration reload has failed
          etcd3:
            groups:
            - name: etcd3.rules
              rules:
              - alert: etcd_InsufficientMembers
                expr: count(up{job="etcd"} == 0) > (count(up{job="etcd"}) / 2 - 1)
                for: 3m
                labels:
                  severity: critical
                annotations:
                  description: If one more etcd member goes down the cluster will be unavailable
                  summary: etcd cluster insufficient members
              - alert: etcd_NoLeader
                expr: etcd_server_has_leader{job="etcd"} == 0
                for: 1m
                labels:
                  severity: critical
                annotations:
                  description: etcd member {{ $labels.instance }} has no leader
                  summary: etcd member has no leader
              - alert: etcd_HighNumberOfLeaderChanges
                expr: increase(etcd_server_leader_changes_seen_total{job="etcd"}[1h]) > 3
                labels:
                  severity: warning
                annotations:
                  description: etcd instance {{ $labels.instance }} has seen {{ $value }} leader changes within the last hour
                  summary: a high number of leader changes within the etcd cluster are happening
              - alert: etcd_HighNumberOfFailedGRPCRequests
                expr: sum(rate(etcd_grpc_requests_failed_total{job="etcd"}[5m])) BY (grpc_method) / sum(rate(etcd_grpc_total{job="etcd"}[5m])) BY (grpc_method) > 0.01
                for: 10m
                labels:
                  severity: warning
                annotations:
                  description: '{{ $value }}% of requests for {{ $labels.grpc_method }} failed on etcd instance {{ $labels.instance }}'
                  summary: a high number of gRPC requests are failing
              - alert: etcd_HighNumberOfFailedGRPCRequests
                expr: sum(rate(etcd_grpc_requests_failed_total{job="etcd"}[5m])) BY (grpc_method) / sum(rate(etcd_grpc_total{job="etcd"}[5m])) BY (grpc_method) > 0.05
                for: 5m
                labels:
                  severity: critical
                annotations:
                  description: '{{ $value }}% of requests for {{ $labels.grpc_method }} failed on etcd instance {{ $labels.instance }}'
                  summary: a high number of gRPC requests are failing
              - alert: etcd_GRPCRequestsSlow
                expr: histogram_quantile(0.99, rate(etcd_grpc_unary_requests_duration_seconds_bucket[5m])) > 0.15
                for: 10m
                labels:
                  severity: critical
                annotations:
                  description: on etcd instance {{ $labels.instance }} gRPC requests to {{ $labels.grpc_method }} are slow
                  summary: slow gRPC requests
              - alert: etcd_HighNumberOfFailedHTTPRequests
                expr: sum(rate(etcd_http_failed_total{job="etcd"}[5m])) BY (method) / sum(rate(etcd_http_received_total{job="etcd"}[5m])) BY (method) > 0.01
                for: 10m
                labels:
                  severity: warning
                annotations:
                  description: '{{ $value }}% of requests for {{ $labels.method }} failed on etcd instance {{ $labels.instance }}'
                  summary: a high number of HTTP requests are failing
              - alert: etcd_HighNumberOfFailedHTTPRequests
                expr: sum(rate(etcd_http_failed_total{job="etcd"}[5m])) BY (method) / sum(rate(etcd_http_received_total{job="etcd"}[5m])) BY (method) > 0.05
                for: 5m
                labels:
                  severity: critical
                annotations:
                  description: '{{ $value }}% of requests for {{ $labels.method }} failed on etcd instance {{ $labels.instance }}'
                  summary: a high number of HTTP requests are failing
              - alert: etcd_HTTPRequestsSlow
                expr: histogram_quantile(0.99, rate(etcd_http_successful_duration_seconds_bucket[5m])) > 0.15
                for: 10m
                labels:
                  severity: warning
                annotations:
                  description: on etcd instance {{ $labels.instance }} HTTP requests to {{ $labels.method }} are slow
                  summary: slow HTTP requests
              - alert: etcd_EtcdMemberCommunicationSlow
                expr: histogram_quantile(0.99, rate(etcd_network_member_round_trip_time_seconds_bucket[5m])) > 0.15
                for: 10m
                labels:
                  severity: warning
                annotations:
                  description: etcd instance {{ $labels.instance }} member communication with {{ $labels.To }} is slow
                  summary: etcd member communication is slow
              - alert: etcd_HighNumberOfFailedProposals
                expr: increase(etcd_server_proposals_failed_total{job="etcd"}[1h]) > 5
                labels:
                  severity: warning
                annotations:
                  description: etcd instance {{ $labels.instance }} has seen {{ $value }} proposal failures within the last hour
                  summary: a high number of proposals within the etcd cluster are failing
              - alert: etcd_HighFsyncDurations
                expr: histogram_quantile(0.99, rate(etcd_disk_wal_fsync_duration_seconds_bucket[5m])) > 0.5
                for: 10m
                labels:
                  severity: warning
                annotations:
                  description: etcd instance {{ $labels.instance }} fync durations are high
                  summary: high fsync durations
              - alert: etcd_HighCommitDurations
                expr: histogram_quantile(0.99, rate(etcd_disk_backend_commit_duration_seconds_bucket[5m])) > 0.25
                for: 10m
                labels:
                  severity: warning
                annotations:
                  description: etcd instance {{ $labels.instance }} commit durations are high
                  summary: high commit durations
          kube_apiserver:
            groups:
            - name: kube-apiserver.rules
              rules:
              - alert: K8SApiserverDown
                expr: absent(up{job="apiserver"} == 1)
                for: 5m
                labels:
                  severity: critical
                annotations:
                  description: Prometheus failed to scrape API server(s), or all API servers have disappeared from service discovery.
                  summary: API server unreachable
              - alert: K8SApiServerLatency
                expr: histogram_quantile(0.99, sum(apiserver_request_latencies_bucket{verb!~"CONNECT|WATCHLIST|WATCH|PROXY"}) WITHOUT (instance, resource)) / 1e+06 > 1
                for: 10m
                labels:
                  severity: warning
                annotations:
                  description: 99th percentile Latency for {{ $labels.verb }} requests to the kube-apiserver is higher than 1s.
                  summary: Kubernetes apiserver latency is high
          kube_controller_manager:
            groups:
            - name: kube-controller-manager.rules
              rules:
              - alert: K8SControllerManagerDown
                expr: absent(up{job="kube-controller-manager-discovery"} == 1)
                for: 5m
                labels:
                  severity: critical
                annotations:
                  description: There is no running K8S controller manager. Deployments and replication controllers are not making progress.
                  runbook: https://coreos.com/tectonic/docs/latest/troubleshooting/controller-recovery.html#recovering-a-controller-manager
                  summary: Controller manager is down
          kubelet:
            groups:
            - name: kubelet.rules
              rules:
              - alert: K8SNodeNotReady
                expr: kube_node_status_ready{condition="true"} == 0
                for: 1h
                labels:
                  severity: warning
                annotations:
                  description: The Kubelet on {{ $labels.node }} has not checked in with the API, or has set itself to NotReady, for more than an hour
                  summary: Node status is NotReady
              - alert: K8SManyNodesNotReady
                expr: count(kube_node_status_ready{condition="true"} == 0) > 1 and (count(kube_node_status_ready{condition="true"} == 0) / count(kube_node_status_ready{condition="true"})) > 0.2
                for: 1m
                labels:
                  severity: critical
                annotations:
                  description: '{{ $value }} Kubernetes nodes (more than 10% are in the NotReady state).'
                  summary: Many Kubernetes nodes are Not Ready
              - alert: K8SKubeletDown
                expr: count(up{job="kubelet"} == 0) / count(up{job="kubelet"}) > 0.03
                for: 1h
                labels:
                  severity: warning
                annotations:
                  description: Prometheus failed to scrape {{ $value }}% of kubelets.
                  summary: Many Kubelets cannot be scraped
              - alert: K8SKubeletDown
                expr: absent(up{job="kubelet"} == 1) or count(up{job="kubelet"} == 0) / count(up{job="kubelet"}) > 0.1
                for: 1h
                labels:
                  severity: critical
                annotations:
                  description: Prometheus failed to scrape {{ $value }}% of kubelets, or all Kubelets have disappeared from service discovery.
                  summary: Many Kubelets cannot be scraped
              - alert: K8SKubeletTooManyPods
                expr: kubelet_running_pod_count > 100
                labels:
                  severity: warning
                annotations:
                  description: Kubelet {{$labels.instance}} is running {{$value}} pods, close to the limit of 110
                  summary: Kubelet is close to pod limit
          kubernetes:
            groups:
            - name: kubernetes.rules
              rules:
              - record: cluster_namespace_controller_pod_container:spec_memory_limit_bytes
                expr: sum(label_replace(container_spec_memory_limit_bytes{container_name!=""}, "controller", "$1", "pod_name", "^(.*)-[a-z0-9]+")) BY (cluster, namespace, controller, pod_name, container_name)
              - record: cluster_namespace_controller_pod_container:spec_cpu_shares
                expr: sum(label_replace(container_spec_cpu_shares{container_name!=""}, "controller", "$1", "pod_name", "^(.*)-[a-z0-9]+")) BY (cluster, namespace, controller, pod_name, container_name)
              - record: cluster_namespace_controller_pod_container:cpu_usage:rate
                expr: sum(label_replace(irate(container_cpu_usage_seconds_total{container_name!=""}[5m]), "controller", "$1", "pod_name", "^(.*)-[a-z0-9]+")) BY (cluster, namespace, controller, pod_name, container_name)
              - record: cluster_namespace_controller_pod_container:memory_usage:bytes
                expr: sum(label_replace(container_memory_usage_bytes{container_name!=""}, "controller", "$1", "pod_name", "^(.*)-[a-z0-9]+")) BY (cluster, namespace, controller, pod_name, container_name)
              - record: cluster_namespace_controller_pod_container:memory_working_set:bytes
                expr: sum(label_replace(container_memory_working_set_bytes{container_name!=""}, "controller", "$1", "pod_name", "^(.*)-[a-z0-9]+")) BY (cluster, namespace, controller, pod_name, container_name)
              - record: cluster_namespace_controller_pod_container:memory_rss:bytes
                expr: sum(label_replace(container_memory_rss{container_name!=""}, "controller", "$1", "pod_name", "^(.*)-[a-z0-9]+")) BY (cluster, namespace, controller, pod_name, container_name)
              - record: cluster_namespace_controller_pod_container:memory_cache:bytes
                expr: sum(label_replace(container_memory_cache{container_name!=""}, "controller", "$1", "pod_name", "^(.*)-[a-z0-9]+")) BY (cluster, namespace, controller, pod_name, container_name)
              - record: cluster_namespace_controller_pod_container:disk_usage:bytes
                expr: sum(label_replace(container_disk_usage_bytes{container_name!=""}, "controller", "$1", "pod_name", "^(.*)-[a-z0-9]+")) BY (cluster, namespace, controller, pod_name, container_name)
              - record: cluster_namespace_controller_pod_container:memory_pagefaults:rate
                expr: sum(label_replace(irate(container_memory_failures_total{container_name!=""}[5m]), "controller", "$1", "pod_name", "^(.*)-[a-z0-9]+")) BY (cluster, namespace, controller, pod_name, container_name, scope, type)
              - record: cluster_namespace_controller_pod_container:memory_oom:rate
                expr: sum(label_replace(irate(container_memory_failcnt{container_name!=""}[5m]), "controller", "$1", "pod_name", "^(.*)-[a-z0-9]+")) BY (cluster, namespace, controller, pod_name, container_name, scope, type)
              - record: cluster:memory_allocation:percent
                expr: 100 * sum(container_spec_memory_limit_bytes{pod_name!=""}) BY (cluster) / sum(machine_memory_bytes) BY (cluster)
              - record: cluster:memory_used:percent
                expr: 100 * sum(container_memory_usage_bytes{pod_name!=""}) BY (cluster) / sum(machine_memory_bytes) BY (cluster)
              - record: cluster:cpu_allocation:percent
                expr: 100 * sum(container_spec_cpu_shares{pod_name!=""}) BY (cluster) / sum(container_spec_cpu_shares{id="/"} * ON(cluster, instance) machine_cpu_cores) BY (cluster)
              - record: cluster:node_cpu_use:percent
                expr: 100 * sum(rate(node_cpu{mode!="idle"}[5m])) BY (cluster) / sum(machine_cpu_cores) BY (cluster)
              - record: cluster_resource_verb:apiserver_latency:quantile_seconds
                expr: histogram_quantile(0.99, sum(apiserver_request_latencies_bucket) BY (le, cluster, job, resource, verb)) / 1e+06
                labels:
                  quantile: "0.99"
              - record: cluster_resource_verb:apiserver_latency:quantile_seconds
                expr: histogram_quantile(0.9, sum(apiserver_request_latencies_bucket) BY (le, cluster, job, resource, verb)) / 1e+06
                labels:
                  quantile: "0.9"
              - record: cluster_resource_verb:apiserver_latency:quantile_seconds
                expr: histogram_quantile(0.5, sum(apiserver_request_latencies_bucket) BY (le, cluster, job, resource, verb)) / 1e+06
                labels:
                  quantile: "0.5"
              - record: cluster:scheduler_e2e_scheduling_latency:quantile_seconds
                expr: histogram_quantile(0.99, sum(scheduler_e2e_scheduling_latency_microseconds_bucket) BY (le, cluster)) / 1e+06
                labels:
                  quantile: "0.99"
              - record: cluster:scheduler_e2e_scheduling_latency:quantile_seconds
                expr: histogram_quantile(0.9, sum(scheduler_e2e_scheduling_latency_microseconds_bucket) BY (le, cluster)) / 1e+06
                labels:
                  quantile: "0.9"
              - record: cluster:scheduler_e2e_scheduling_latency:quantile_seconds
                expr: histogram_quantile(0.5, sum(scheduler_e2e_scheduling_latency_microseconds_bucket) BY (le, cluster)) / 1e+06
                labels:
                  quantile: "0.5"
              - record: cluster:scheduler_scheduling_algorithm_latency:quantile_seconds
                expr: histogram_quantile(0.99, sum(scheduler_scheduling_algorithm_latency_microseconds_bucket) BY (le, cluster)) / 1e+06
                labels:
                  quantile: "0.99"
              - record: cluster:scheduler_scheduling_algorithm_latency:quantile_seconds
                expr: histogram_quantile(0.9, sum(scheduler_scheduling_algorithm_latency_microseconds_bucket) BY (le, cluster)) / 1e+06
                labels:
                  quantile: "0.9"
              - record: cluster:scheduler_scheduling_algorithm_latency:quantile_seconds
                expr: histogram_quantile(0.5, sum(scheduler_scheduling_algorithm_latency_microseconds_bucket) BY (le, cluster)) / 1e+06
                labels:
                  quantile: "0.5"
              - record: cluster:scheduler_binding_latency:quantile_seconds
                expr: histogram_quantile(0.99, sum(scheduler_binding_latency_microseconds_bucket) BY (le, cluster)) / 1e+06
                labels:
                  quantile: "0.99"
              - record: cluster:scheduler_binding_latency:quantile_seconds
                expr: histogram_quantile(0.9, sum(scheduler_binding_latency_microseconds_bucket) BY (le, cluster)) / 1e+06
                labels:
                  quantile: "0.9"
              - record: cluster:scheduler_binding_latency:quantile_seconds
                expr: histogram_quantile(0.5, sum(scheduler_binding_latency_microseconds_bucket) BY (le, cluster)) / 1e+06
                labels:
                  quantile: "0.5"
              - alert: kube_statefulset_replicas_unavailable
                expr: kube_statefulset_status_replicas < kube_statefulset_replicas
                for: 5m
                labels:
                  severity: page
                annotations:
                  description: 'statefulset {{$labels.statefulset}} has {{$value}} replicas, which is less than desired'
                  summary: '{{$labels.statefulset}}: has inssuficient replicas.'
              - alert: kube_daemonsets_misscheduled
                expr: kube_daemonset_status_number_misscheduled > 0
                for: 10m
                labels:
                  severity: warning
                annotations:
                  description: 'Daemonset {{$labels.daemonset}} is running where it is not supposed to run'
                  summary: 'Daemonsets not scheduled correctly'
              - alert: kube_daemonsets_not_scheduled
                expr: kube_daemonset_status_desired_number_scheduled - kube_daemonset_status_current_number_scheduled > 0
                for: 10m
                labels:
                  severity: warning
                annotations:
                  description: '{{ $value }} of Daemonset {{$labels.daemonset}} scheduled which is less than desired number'
                  summary: 'Less than desired number of daemonsets scheduled'
              - alert: kube_deployment_replicas_unavailable
                expr: kube_deployment_status_replicas_unavailable > 0
                for: 10m
                labels:
                  severity: page
                annotations:
                  description: 'deployment {{$labels.deployment}} has {{$value}} replicas unavailable'
                  summary: '{{$labels.deployment}}: has inssuficient replicas.'
              - alert: kube_rollingupdate_deployment_replica_less_than_spec_max_unavailable
                expr: kube_deployment_status_replicas_available - kube_deployment_spec_strategy_rollingupdate_max_unavailable < 0
                for: 10m
                labels:
                  severity: page
                annotations:
                  description: 'deployment {{$labels.deployment}} has {{$value}} replicas available which is less than specified as max unavailable during a rolling update'
                  summary: '{{$labels.deployment}}: has inssuficient replicas during a rolling update.'
              - alert: kube_job_status_failed
                expr: kube_job_status_failed > 0
                for: 10m
                labels:
                  severity: page
                annotations:
                  description: 'Job {{$labels.exported_job}} is in failed status'
                  summary: '{{$labels.exported_job}} has failed status'
              - alert: kube_pod_status_pending
                expr: kube_pod_status_phase{phase="Pending"} == 1
                for: 10m
                labels:
                  severity: page
                annotations:
                  description: 'Pod {{$labels.pod}} in namespace {{$labels.namespace}} has been in pending status for more than 10 minutes'
                  summary: 'Pod {{$labels.pod}} in namespace {{$labels.namespace}} in pending status'
              - alert: kube_pod_error_image_pull
                expr: kube_pod_container_status_waiting_reason {reason="ErrImagePull"} == 1
                for: 10m
                labels:
                  severity: page
                annotations:
                  description: 'Pod {{$labels.pod}} in namespace {{$labels.namespace}} has an Image pull error for more than 10 minutes'
                  summary: 'Pod {{$labels.pod}} in namespace {{$labels.namespace}} in error status'
              - alert: kube_pod_status_error_image_pull
                expr: kube_pod_container_status_waiting_reason {reason="ErrImagePull"} == 1
                for: 10m
                labels:
                  severity: page
                annotations:
                  description: 'Pod {{$labels.pod}} in namespace {{$labels.namespace}} has an Image pull error for more than 10 minutes'
                  summary: 'Pod {{$labels.pod}} in namespace {{$labels.namespace}} in error status'
              - alert: kube_replicaset_missing_replicas
                expr:  kube_replicaset_spec_replicas -  kube_replicaset_status_ready_replicas > 0
                for: 10m
                labels:
                  severity: page
                annotations:
                  description: 'Replicaset {{$labels.replicaset}} is missing desired number of replicas for more than 10 minutes'
                  summary: 'Replicaset {{$labels.replicaset}} is missing replicas'
              - alert: kube_pod_container_terminated
                expr: kube_pod_container_status_terminated_reason{reason=~"OOMKilled|Error|ContainerCannotRun"} > 0
                for: 10m
                labels:
                  severity: page
                annotations:
                  description: 'Pod {{$labels.pod}} in namespace {{$labels.namespace}} has a container terminated for more than 10 minutes'
                  summary: 'Pod {{$labels.pod}} in namespace {{$labels.namespace}} in error status'
              - alert: volume_claim_capacity_high_utilization
                expr: (kubelet_volume_stats_available_bytes / kubelet_volume_stats_capacity_bytes) > 0.80
                for: 5m
                labels:
                  severity: page
                annotations:
                  description: 'volume claim {{$labels.persistentvolumeclaim}} usage has exceeded 80% of total capacity'
                  summary: '{{$labels.persistentvolumeclaim}} usage has exceeded 80% of total capacity.'
          basic_linux:
            groups:
            - name: basic_linux.rules
              rules:
              - alert: node_filesystem_full_80percent
                expr: sort(node_filesystem_free{device!="ramfs"} < node_filesystem_size{device!="ramfs"}
                  * 0.2) / 1024 ^ 3
                for: 5m
                labels:
                  severity: page
                annotations:
                  description: '{{$labels.alias}} device {{$labels.device}} on {{$labels.mountpoint}}
                    got less than 10% space left on its filesystem.'
                  summary: '{{$labels.alias}}: Filesystem is running out of space soon.'
              - alert: node_filesystem_full_in_4h
                expr: predict_linear(node_filesystem_free{device!="ramfs"}[1h], 4 * 3600) <= 0
                for: 5m
                labels:
                  severity: page
                annotations:
                  description: '{{$labels.alias}} device {{$labels.device}} on {{$labels.mountpoint}}
                    is running out of space of in approx. 4 hours'
                  summary: '{{$labels.alias}}: Filesystem is running out of space in 4 hours.'
              - alert: node_filedescriptors_full_in_3h
                expr: predict_linear(node_filefd_allocated[1h], 3 * 3600) >= node_filefd_maximum
                for: 20m
                labels:
                  severity: page
                annotations:
                  description: '{{$labels.alias}} is running out of available file descriptors
                    in approx. 3 hours'
                  summary: '{{$labels.alias}} is running out of available file descriptors in
                    3 hours.'
              - alert: node_load1_90percent
                expr: node_load1 / ON(alias) count(node_cpu{mode="system"}) BY (alias) >= 0.9
                for: 1h
                labels:
                  severity: page
                annotations:
                  description: '{{$labels.alias}} is running with > 90% total load for at least
                    1h.'
                  summary: '{{$labels.alias}}: Running on high load.'
              - alert: node_cpu_util_90percent
                expr: 100 - (avg(irate(node_cpu{mode="idle"}[5m])) BY (alias) * 100) >= 90
                for: 1h
                labels:
                  severity: page
                annotations:
                  description: '{{$labels.alias}} has total CPU utilization over 90% for at least
                    1h.'
                  summary: '{{$labels.alias}}: High CPU utilization.'
              - alert: node_ram_using_90percent
                expr: node_memory_MemFree + node_memory_Buffers + node_memory_Cached < node_memory_MemTotal
                  * 0.1
                for: 30m
                labels:
                  severity: page
                annotations:
                  description: '{{$labels.alias}} is using at least 90% of its RAM for at least
                    30 minutes now.'
                  summary: '{{$labels.alias}}: Using lots of RAM.'
              - alert: node_swap_using_80percent
                expr: node_memory_SwapTotal - (node_memory_SwapFree + node_memory_SwapCached)
                  > node_memory_SwapTotal * 0.8
                for: 10m
                labels:
                  severity: page
                annotations:
                  description: '{{$labels.alias}} is using 80% of its swap space for at least
                    10 minutes now.'
                  summary: '{{$labels.alias}}: Running out of swap soon.'
              - alert: node_high_cpu_load
                expr: node_load15 / on(alias) count(node_cpu{mode="system"}) by (alias) >= 0
                for: 1m
                labels:
                  severity: warning
                annotations:
                  description: '{{$labels.alias}} is running with load15 > 1 for at least 5 minutes: {{$value}}'
                  summary: '{{$labels.alias}}: Running on high load: {{$value}}'
              - alert: node_high_memory_load
                expr: (sum(node_memory_MemTotal) - sum(node_memory_MemFree + node_memory_Buffers
                  + node_memory_Cached)) / sum(node_memory_MemTotal) * 100 > 85
                for: 1m
                labels:
                  severity: warning
                annotations:
                  description: Host memory usage is {{ humanize $value }}%. Reported by
                    instance {{ $labels.instance }} of job {{ $labels.job }}.
                  summary: Server memory is almost full
              - alert: node_high_storage_load
                expr: (node_filesystem_size{mountpoint="/"} - node_filesystem_free{mountpoint="/"})
                  / node_filesystem_size{mountpoint="/"} * 100 > 85
                for: 30s
                labels:
                  severity: warning
                annotations:
                  description: Host storage usage is {{ humanize $value }}%. Reported by
                    instance {{ $labels.instance }} of job {{ $labels.job }}.
                  summary: Server storage is almost full
              - alert: node_high_swap
                expr: (node_memory_SwapTotal - node_memory_SwapFree) < (node_memory_SwapTotal
                  * 0.4)
                for: 1m
                labels:
                  severity: warning
                annotations:
                  description: Host system has a high swap usage of {{ humanize $value }}. Reported
                    by instance {{ $labels.instance }} of job {{ $labels.job }}.
                  summary: Server has a high swap usage
              - alert: node_high_network_drop_rcv
                expr: node_network_receive_drop{device!="lo"} > 3000
                for: 30s
                labels:
                  severity: warning
                annotations:
                  description: Host system has an unusally high drop in network reception ({{
                    humanize $value }}). Reported by instance {{ $labels.instance }} of job {{
                    $labels.job }}
                  summary: Server has a high receive drop
              - alert: node_high_network_drop_send
                expr: node_network_transmit_drop{device!="lo"} > 3000
                for: 30s
                labels:
                  severity: warning
                annotations:
                  description: Host system has an unusally high drop in network transmission ({{
                    humanize $value }}). Reported by instance {{ $labels.instance }} of job {{
                    $labels.job }}
                  summary: Server has a high transmit drop
              - alert: node_high_network_errs_rcv
                expr: node_network_receive_errs{device!="lo"} > 3000
                for: 30s
                labels:
                  severity: warning
                annotations:
                  description: Host system has an unusally high error rate in network reception
                    ({{ humanize $value }}). Reported by instance {{ $labels.instance }} of job
                    {{ $labels.job }}
                  summary: Server has unusual high reception errors
              - alert: node_high_network_errs_send
                expr: node_network_transmit_errs{device!="lo"} > 3000
                for: 30s
                labels:
                  severity: warning
                annotations:
                  description: Host system has an unusally high error rate in network transmission
                    ({{ humanize $value }}). Reported by instance {{ $labels.instance }} of job
                    {{ $labels.job }}
                  summary: Server has unusual high transmission errors
              - alert: node_network_conntrack_usage_80percent
                expr: sort(node_nf_conntrack_entries{job="node-exporter"} > node_nf_conntrack_entries_limit{job="node-exporter"}  * 0.8)
                for: 5m
                labels:
                  severity: page
                annotations:
                  description: '{{$labels.instance}} has network conntrack entries of {{ $value }} which is more than 80% of maximum limit'
                  summary: '{{$labels.instance}}: available network conntrack entries are low.'
              - alert: node_entropy_available_low
                expr: node_entropy_available_bits < 300
                for: 5m
                labels:
                  severity: page
                annotations:
                  description: '{{$labels.instance}} has available entropy bits of {{ $value }} which is less than required of 300'
                  summary: '{{$labels.instance}}: is low on entropy bits.'
              - alert: node_hwmon_high_cpu_temp
                expr: node_hwmon_temp_crit_celsius*0.9 - node_hwmon_temp_celsius < 0 OR node_hwmon_temp_max_celsius*0.95 - node_hwmon_temp_celsius < 0
                for: 5m
                labels:
                  severity: page
                annotations:
                  description: '{{$labels.alias}} reports hwmon sensor {{$labels.sensor}}/{{$labels.chip}} temperature value is nearly critical: {{$value}}'
                  summary: '{{$labels.alias}}: Sensor {{$labels.sensor}}/{{$labels.chip}} temp is high: {{$value}}'
              - alert: node_vmstat_paging_rate_high
                expr: irate(node_vmstat_pgpgin[5m]) > 80
                for: 5m
                labels:
                  severity: page
                annotations:
                  description: '{{$labels.alias}} has a memory paging rate of change higher than 80%: {{$value}}'
                  summary: '{{$labels.alias}}: memory paging rate is high: {{$value}}'
              - alert: node_xfs_block_allocation_high
                expr: 100*(node_xfs_extent_allocation_blocks_allocated_total{job="node-exporter", instance=~"172.17.0.1.*"} / (node_xfs_extent_allocation_blocks_freed_total{job="node-exporter", instance=~"172.17.0.1.*"} + node_xfs_extent_allocation_blocks_allocated_total{job="node-exporter", instance=~"172.17.0.1.*"})) > 80
                for: 5m
                labels:
                  severity: page
                annotations:
                  description: '{{$labels.alias}} has xfs allocation blocks higher than 80%: {{$value}}'
                  summary: '{{$labels.alias}}: xfs block allocation high: {{$value}}'
              - alert: node_network_bond_slaves_down
                expr: node_net_bonding_slaves - node_net_bonding_slaves_active > 0
                for: 5m
                labels:
                  severity: page
                annotations:
                  description: '{{ $labels.master }} is missing {{ $value }} slave interface(s).'
                  summary: 'Instance {{ $labels.instance }}: {{ $labels.master }} missing {{ $value }} slave interface(s)'
              - alert: node_numa_memory_used
                expr: 100*node_memory_numa_MemUsed / node_memory_numa_MemTotal > 80
                for: 5m
                labels:
                  severity: page
                annotations:
                  description: '{{$labels.alias}} has more than 80% NUMA memory usage: {{ $value }}'
                  summary: '{{$labels.alias}}: has high NUMA memory usage: {{$value}}'
              - alert: node_ntp_clock_skew_high
                expr: abs(node_ntp_drift_seconds) > 2
                for: 5m
                labels:
                  severity: page
                annotations:
                  description: '{{$labels.alias}} has time difference of more than 2 seconds compared to NTP server: {{ $value }}'
                  summary: '{{$labels.alias}}: time is skewed by : {{$value}} seconds'
              - alert: node_disk_read_latency
                expr: (rate(node_disk_read_time_ms[5m]) / rate(node_disk_reads_completed[5m])) > 10
                for: 5m
                labels:
                  severity: page
                annotations:
                  description: '{{$labels.device}} has a high read latency of {{ $value }}'
                  summary: 'High read latency observed for device {{ $labels.device }}'
              - alert: node_disk_write_latency
                expr: (rate(node_disk_write_time_ms[5m]) / rate(node_disk_writes_completed[5m])) > 10
                for: 5m
                labels:
                  severity: page
                annotations:
                  description: '{{$labels.device}} has a high write latency of {{ $value }}'
                  summary: 'High write latency observed for device {{ $labels.device }}'
          openstack:
            groups:
            - name: openstack.rules
              rules:
              - alert: os_glance_api_availability
                expr:  check_glance_api != 1
                for: 5m
                labels:
                  severity: page
                annotations:
                  description: 'Glance API is not available at {{$labels.url}} for more than 5 minutes'
                  summary: 'Glance API is not available at {{$labels.url}}'
              - alert: os_nova_api_availability
                expr:  check_nova_api != 1
                for: 5m
                labels:
                  severity: page
                annotations:
                  description: 'Nova API is not available at {{$labels.url}} for more than 5 minutes'
                  summary: 'Nova API is not available at {{$labels.url}}'
              - alert: os_keystone_api_availability
                expr:  check_keystone_api != 1
                for: 5m
                labels:
                  severity: page
                annotations:
                  description: 'Keystone API is not available at {{$labels.url}} for more than 5 minutes'
                  summary: 'Keystone API is not available at {{$labels.url}}'
              - alert: os_neutron_api_availability
                expr:  check_neutron_api != 1
                for: 5m
                labels:
                  severity: page
                annotations:
                  description: 'Neutron API is not available at {{$labels.url}} for more than 5 minutes'
                  summary: 'Neutron API is not available at {{$labels.url}}'
              - alert: os_swift_api_availability
                expr:  check_swift_api != 1
                for: 5m
                labels:
                  severity: page
                annotations:
                  description: 'Swift API is not available at {{$labels.url}} for more than 5 minutes'
                  summary: 'Swift API is not available at {{$labels.url}}'
              - alert: os_nova_compute_disabled
                expr:  services_nova_compute_disabled_total > 0
                for: 5m
                labels:
                  severity: page
                annotations:
                  description: 'nova-compute is disabled on certain hosts for more than 5 minutes'
                  summary: 'Openstack compute service nova-compute is disabled on some hosts'
              - alert: os_nova_conductor_disabled
                expr:  services_nova_conductor_disabled_total > 0
                for: 5m
                labels:
                  severity: page
                annotations:
                  description: 'nova-conductor is disabled on certain hosts for more than 5 minutes'
                  summary: 'Openstack compute service nova-conductor is disabled on some hosts'
              - alert: os_nova_consoleauth_disabled
                expr:  services_nova_consoleauth_disabled_total > 0
                for: 5m
                labels:
                  severity: page
                annotations:
                  description: 'nova-consoleauth is disabled on certain hosts for more than 5 minutes'
                  summary: 'Openstack compute service nova-consoleauth is disabled on some hosts'
              - alert: os_nova_scheduler_disabled
                expr:  services_nova_scheduler_disabled_total > 0
                for: 5m
                labels:
                  severity: page
                annotations:
                  description: 'nova-scheduler is disabled on certain hosts for more than 5 minutes'
                  summary: 'Openstack compute service nova-scheduler is disabled on some hosts'
          ceph:
            groups:
            - name: ceph.rules
              rules:
              - alert: ceph_monitor_quorum_low
                expr:  ceph_monitor_quorum_count < 3
                for: 5m
                labels:
                  severity: page
                annotations:
                  description: 'ceph monitor quorum has been less than 3 for more than 5 minutes'
                  summary: 'ceph high availability is at risk'
              - alert: ceph_cluster_usage_high
                expr:  100* ceph_cluster_used_bytes/ceph_cluster_capacity_bytes > 80
                for: 5m
                labels:
                  severity: page
                annotations:
                  description: 'ceph cluster capacity usage more than 80 percent'
                  summary: 'ceph cluster usage is more than 80 percent'
              - alert: ceph_placement_group_degrade_pct_high
                expr:  100*ceph_degraded_pgs/ceph_total_pgs > 80
                for: 5m
                labels:
                  severity: page
                annotations:
                  description: 'ceph placement group degradation is more than 80 percent'
                  summary: 'ceph placement groups degraded'
              - alert: ceph_osd_down_pct_high
                expr:  100* ceph_osds_down/(ceph_osds_down+ceph_osds_up) > 80
                for: 5m
                labels:
                  severity: page
                annotations:
                  description: 'ceph OSDs down percent is more than 80 percent'
                  summary: 'ceph OSDs down percent is high'
              - alert: ceph_monitor_clock_skew_high
                expr:  ceph_monitor_clock_skew_seconds > 2
                for: 5m
                labels:
                  severity: page
                annotations:
                  description: 'ceph monitors clock skew on {{$labels.instance}} is more than 2 seconds'
                  summary: 'ceph monitor clock skew high'
          fluentd:
            groups:
            - name: fluentd.rules
              rules:
              - alert: fluentd_not_running
                expr:  fluentd_up == 0
                for: 5m
                labels:
                  severity: page
                annotations:
                  description: 'fluentd is down on {{$labels.instance}} for more than 5 minutes'
                  summary: 'Fluentd is down'
          calico:
            groups:
            - name: calico.rules
              rules:
              - alert: calico_datapane_failures_high_1h
                expr: absent(felix_int_dataplane_failures) OR increase(felix_int_dataplane_failures[1h]) > 5
                labels:
                  severity: page
                annotations:
                  description: 'Felix instance {{ $labels.instance }} has seen {{ $value }} dataplane failures within the last hour'
                  summary: 'A high number of dataplane failures within Felix are happening'
              - alert: calico_datapane_address_msg_batch_size_high_5m
                expr: absent(felix_int_dataplane_addr_msg_batch_size_sum) OR absent(felix_int_dataplane_addr_msg_batch_size_count) OR (felix_int_dataplane_addr_msg_batch_size_sum/felix_int_dataplane_addr_msg_batch_size_count) > 5
                for: 5m
                labels:
                  severity: page
                annotations:
                  description: 'Felix instance {{ $labels.instance }} has seen a high value of {{ $value }} dataplane address message batch size'
                  summary: 'Felix address message batch size is higher'
              - alert: calico_datapane_iface_msg_batch_size_high_5m
                expr: absent(felix_int_dataplane_iface_msg_batch_size_sum) OR absent(felix_int_dataplane_iface_msg_batch_size_count) OR (felix_int_dataplane_iface_msg_batch_size_sum/felix_int_dataplane_iface_msg_batch_size_count) > 5
                for: 5m
                labels:
                  severity: page
                annotations:
                  description: 'Felix instance {{ $labels.instance }} has seen a high value of {{ $value }} dataplane interface message batch size'
                  summary: 'Felix interface message batch size is higher'
              - alert: calico_ipset_errors_high_1h
                expr: absent(felix_ipset_errors) OR increase(felix_ipset_errors[1h]) > 5
                labels:
                  severity: page
                annotations:
                  description: 'Felix instance {{ $labels.instance }} has seen {{ $value }} ipset errors within the last hour'
                  summary: 'A high number of ipset errors within Felix are happening'
              - alert: calico_iptable_save_errors_high_1h
                expr: absent(felix_iptables_save_errors) OR increase(felix_iptables_save_errors[1h]) > 5
                labels:
                  severity: page
                annotations:
                  description: 'Felix instance {{ $labels.instance }} has seen {{ $value }} iptable save errors within the last hour'
                  summary: 'A high number of iptable save errors within Felix are happening'
              - alert: calico_iptable_restore_errors_high_1h
                expr: absent(felix_iptables_restore_errors) OR increase(felix_iptables_restore_errors[1h]) > 5
                labels:
                  severity: page
                annotations:
                  description: 'Felix instance {{ $labels.instance }} has seen {{ $value }} iptable restore errors within the last hour'
                  summary: 'A high number of iptable restore errors within Felix are happening'
          rabbitmq:
            groups:
            - name: rabbitmq.rules
              rules:
              - alert: rabbitmq_network_pratitions_detected
                expr: min(partitions) by(instance) > 0
                for: 10m
                labels:
                  severity: warning
                annotations:
                  description: 'RabbitMQ at {{ $labels.instance }} has {{ $value }} partitions'
                  summary: 'RabbitMQ Network partitions detected'
              - alert: rabbitmq_down
                expr:  min(rabbitmq_up) by(instance) != 1
                for: 10m
                labels:
                  severity: page
                annotations:
                  description: 'RabbitMQ Server instance {{ $labels.instance }} is down'
                  summary: 'The RabbitMQ Server instance at {{ $labels.instance }} has been down the last 10 mins'
              - alert: rabbitmq_file_descriptor_usage_high
                expr:  fd_used * 100 /fd_total > 80
                for: 10m
                labels:
                  severity: warning
                annotations:
                  description: 'RabbitMQ Server instance {{ $labels.instance }} has high file descriptor usage of {{ $value }} percent.'
                  summary: 'RabbitMQ file descriptors usage is high for last 10 mins'
              - alert: rabbitmq_node_disk_free_alarm
                expr:  node_disk_free_alarm > 0
                for: 10m
                labels:
                  severity: warning
                annotations:
                  description: 'RabbitMQ Server instance {{ $labels.instance }} has low disk free space available.'
                  summary: 'RabbitMQ disk space usage is high'
              - alert: rabbitmq_node_memory_alarm
                expr:  node_mem_alarm > 0
                for: 10m
                labels:
                  severity: warning
                annotations:
                  description: 'RabbitMQ Server instance {{ $labels.instance }} has low free memory.'
                  summary: 'RabbitMQ memory usage is high'
              - alert: rabbitmq_less_than_3_nodes
                expr:  running < 3
                for: 10m
                labels:
                  severity: warning
                annotations:
                  description: 'RabbitMQ Server has less than 3 nodes running.'
                  summary: 'RabbitMQ server is at risk of loosing data'
              - alert: rabbitmq_queue_messages_returned_high
                expr:  queue_messages_returned_total/queue_messages_published_total * 100 > 50
                for: 5m
                labels:
                  severity: warning
                annotations:
                  description: 'RabbitMQ Server is returing more than 50 percent of messages received.'
                  summary: 'RabbitMQ server is returning more than 50 percent of messages received.'
              - alert: rabbitmq_consumers_low_utilization
                expr:  queue_consumer_utilisation < .4
                for: 5m
                labels:
                  severity: warning
                annotations:
                  description: 'RabbitMQ consumers message consumption speed is low'
                  summary: 'RabbitMQ consumers message consumption speed is low'
              - alert: rabbitmq_high_message_load
                expr:  queue_messages_total > 17000 or increase(queue_messages_total[5m]) > 4000
                for: 5m
                labels:
                  severity: warning
                annotations:
                  description: 'RabbitMQ has high message load. Total Queue depth > 17000 or growth more than 4000 messages.'
                  summary: 'RabbitMQ has high message load'
          elasticsearch:
            groups:
            - name: elasticsearch.rules
              rules:
              - alert: es_high_process_open_files_count
                expr: sum(elasticsearch_process_open_files_count) by (host) > 64000
                for: 10m
                labels:
                  severity: warning
                annotations:
                  description: 'Elasticsearch at {{ $labels.host }} has more than 64000 process open file count.'
                  summary: 'Elasticsearch has a very high process open file count.'
              - alert: es_high_process_cpu_percent
                expr: elasticsearch_process_cpu_percent > 95
                for: 10m
                labels:
                  severity: warning
                annotations:
                  description: 'Elasticsearch at {{ $labels.instance }} has high process cpu percent of {{ $value }}.'
                  summary: 'Elasticsearch process cpu usage is more than 95 percent.'
              - alert: es_fs_usage_high
                expr: (100 * (elasticsearch_filesystem_data_size_bytes - elasticsearch_filesystem_data_free_bytes) / elasticsearch_filesystem_data_size_bytes) > 80
                for: 10m
                labels:
                  severity: warning
                annotations:
                  description: 'Elasticsearch at {{ $labels.instance }} has filesystem usage of {{ $value }}.'
                  summary: 'Elasticsearch filesystem usage is high.'
              - alert: es_unassigned_shards
                expr: elasticsearch_cluster_health_unassigned_shards > 0
                for: 10m
                labels:
                  severity: warning
                annotations:
                  description: 'Elasticsearch has {{ $value }} unassigned shards.'
                  summary: 'Elasticsearch has unassigned shards and hence a unhealthy cluster state.'
              - alert: es_cluster_health_timed_out
                expr: elasticsearch_cluster_health_timed_out > 0
                for: 10m
                labels:
                  severity: warning
                annotations:
                  description: 'Elasticsearch cluster health status call timedout {{ $value }} times.'
                  summary: 'Elasticsearch cluster health status calls are timing out.'
              - alert: es_cluster_health_status_alert
                expr: elasticsearch_cluster_health_status > 0
                for: 10m
                labels:
                  severity: warning
                annotations:
                  description: 'Elasticsearch cluster health status is not green. One or more shards or replicas are unallocated.'
                  summary: 'Elasticsearch cluster health status is not green.'
              - alert: es_cluster_health_too_few_nodes_running
                expr: elasticsearch_cluster_health_number_of_nodes < 3
                for: 10m
                labels:
                  severity: warning
                annotations:
                  description: 'There are only {{$value}} < 3 ElasticSearch nodes running'
                  summary: 'ElasticSearch running on less than 3 nodes'
              - alert: es_cluster_health_too_few_data_nodes_running
                expr: elasticsearch_cluster_health_number_of_data_nodes < 3
                for: 10m
                labels:
                  severity: warning
                annotations:
                  description: 'There are only {{$value}} < 3 ElasticSearch data nodes running'
                  summary: 'ElasticSearch running on less than 3 data nodes'
          mariadb:
            groups:
            - name: mariadb.rules
              rules:
              - alert: mariadb_table_lock_wait_high
                expr: 100 * mysql_global_status_table_locks_waited/(mysql_global_status_table_locks_waited + mysql_global_status_table_locks_immediate) > 30
                for: 10m
                labels:
                  severity: warning
                annotations:
                  description: 'Mariadb has high table lock waits of {{ $value }} percentage'
                  summary: 'Mariadb table lock waits are high'
              - alert: mariadb_node_not_ready
                expr:  mysql_global_status_wsrep_ready != 1
                for: 10m
                labels:
                  severity: warning
                annotations:
                  description: '{{$labels.job}} on {{$labels.instance}} is not ready.'
                  summary: 'Galera cluster node not ready'
              - alert: mariadb_galera_node_out_of_sync
                expr:  mysql_global_status_wsrep_local_state != 4 AND mysql_global_variables_wsrep_desync == 0
                for: 10m
                labels:
                  severity: warning
                annotations:
                  description: '{{$labels.job}} on {{$labels.instance}} is not in sync ({{$value}} != 4)'
                  summary: 'Galera cluster node out of sync'
              - alert: mariadb_innodb_replication_fallen_behind
                expr:  (mysql_global_variables_innodb_replication_delay > 30) AND on (instance) (predict_linear(mysql_global_variables_innodb_replication_delay[5m], 60*2) > 0)
                for: 10m
                labels:
                  severity: warning
                annotations:
                  description: 'The mysql innodb replication has fallen behind and is not recovering'
                  summary: 'MySQL innodb replication is lagging'
  dependencies:
    - osh-infra-helm-toolkit
...
---
schema: armada/ChartGroup/v1
metadata:
  schema: metadata/Document/v1
  name: osh-infra-monitoring
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
data:
  description: OSH Infra Monitoring
  chart_group:
    - prometheus
    - prometheus-alertmanager
    - prometheus-node-exporter
    - prometheus-process-exporter
    - prometheus-kube-state-metrics
    - nagios
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: prometheus-node-exporter
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    # Chart source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.osh_infra.prometheus_node_exporter
      dest:
        path: .source

    # Images
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.osh_infra.prometheus_node_exporter
      dest:
        path: .values.images.tags

    # Endpoints
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_infra_endpoints
        path: .osh_infra.node_metrics
      dest:
        path: .values.endpoints.node_metrics

data:
  chart_name: prometheus-node-exporter
  release: prometheus-node-exporter
  namespace: kube-system
  wait:
    timeout: 900
    labels:
      release_group: airship-prometheus-node-exporter
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
    pre:
      delete:
        - type: job
          labels:
            release_group: airship-prometheus-node-exporter
      create: []
    post:
      create: []
  values:
    labels:
      node_exporter:
        node_selector_key: node-exporter
        node_selector_value: enabled
      job:
        node_selector_key: openstack-control-plane
        node_selector_value: enabled
  dependencies:
    - osh-infra-helm-toolkit
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: nagios
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    # Chart source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.osh_infra.nagios
      dest:
        path: .source

    # Images
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.osh_infra.nagios
      dest:
        path: .values.images.tags

    # Endpoints
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_infra_endpoints
        path: .osh_infra.nagios
      dest:
        path: .values.endpoints.nagios
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_infra_endpoints
        path: .osh_infra.monitoring
      dest:
        path: .values.endpoints.monitoring
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_infra_endpoints
        path: .osh_infra.elasticsearch
      dest:
        path: .values.endpoints.elasticsearch
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_infra_endpoints
        path: .osh_infra.ldap
      dest:
        path: .values.endpoints.ldap

    # Accounts
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_infra_service_accounts
        path: .osh_infra.nagios.admin
      dest:
        path: .values.endpoints.nagios.auth.admin
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_infra_service_accounts
        path: .osh_infra.prometheus.admin
      dest:
        path: .values.endpoints.monitoring.auth.admin
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_infra_service_accounts
        path: .osh_infra.elasticsearch.admin
      dest:
        path: .values.endpoints.elasticsearch.auth.admin

    # Secrets
    - dest:
        path: .values.endpoints.nagios.auth.admin.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_infra_nagios_admin_password
        path: .
    - dest:
        path: .values.endpoints.elasticsearch.auth.admin.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_infra_elasticsearch_admin_password
        path: .
    - dest:
        path: .values.endpoints.monitoring.auth.admin.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_infra_prometheus_admin_password
        path: .

    # LDAP Details
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_infra_service_accounts
        path: .osh_infra.ldap.admin
      dest:
        path: .values.endpoints.ldap.auth.admin
    - dest:
        path: .values.endpoints.ldap.auth.admin.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_keystone_ldap_password
        path: .

data:
  chart_name: nagios
  release: nagios
  namespace: osh-infra
  wait:
    timeout: 900
    labels:
      release_group: airship-nagios
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
    pre:
      delete:
        - type: job
          labels:
            release_group: airship-nagios
      create: []
    post:
      create: []
  values:
    conf:
      apache:
        host: |
          <VirtualHost *:80>
            <Location />
                ProxyPass http://localhost:{{ tuple "nagios" "internal" "nagios" . | include "helm-toolkit.endpoints.endpoint_port_lookup" }}/
                ProxyPassReverse http://localhost:{{ tuple "nagios" "internal" "nagios" . | include "helm-toolkit.endpoints.endpoint_port_lookup" }}/
            </Location>
            <Proxy *>
                AuthName "Nagios"
                AuthType Basic
                AuthBasicProvider file ldap
                AuthUserFile /usr/local/apache2/conf/.htpasswd
                AuthLDAPBindDN {{ .Values.endpoints.ldap.auth.admin.bind }}
                AuthLDAPBindPassword {{ .Values.endpoints.ldap.auth.admin.password }}
                AuthLDAPURL {{ tuple "ldap" "public" "ldap" . | include "helm-toolkit.endpoints.keystone_endpoint_uri_lookup" }}
                Require valid-user
            </Proxy>
          </VirtualHost>
    labels:
      nagios:
        node_selector_key: openstack-control-plane
        node_selector_value: enabled
      job:
        node_selector_key: openstack-control-plane
        node_selector_value: enabled
    pod:
      replicas:
        nagios: 3
  dependencies:
    - osh-infra-helm-toolkit
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: prometheus-process-exporter
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    # Chart source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.osh_infra.prometheus_process_exporter
      dest:
        path: .source

    # Images
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.osh_infra.prometheus_process_exporter
      dest:
        path: .values.images.tags

    # Endpoints
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_infra_endpoints
        path: .osh_infra.process_exporter_metrics
      dest:
        path: .values.endpoints.process_exporter_metrics

data:
  chart_name: prometheus-process-exporter
  release: prometheus-process-exporter
  namespace: kube-system
  wait:
    timeout: 900
    labels:
      release_group: airship-prometheus-process-exporter
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
    pre:
      delete:
        - type: job
          labels:
            release_group: airship-prometheus-process-exporter
      create: []
    post:
      create: []
  values:
    labels:
      node_exporter:
        node_selector_key: node-exporter
        node_selector_value: enabled
      job:
        node_selector_key: openstack-control-plane
        node_selector_value: enabled
  dependencies:
    - osh-infra-helm-toolkit
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: prometheus-alertmanager
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    # Chart source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.osh_infra.prometheus_alertmanager
      dest:
        path: .source

    # Images
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.osh_infra.prometheus_alertmanager
      dest:
        path: .values.images.tags

    # Endpoints
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_infra_endpoints
        path: .osh_infra.alerts
      dest:
        path: .values.endpoints.alerts

data:
  chart_name: prometheus-alertmanager
  release: prometheus-alertmanager
  namespace: osh-infra
  wait:
    timeout: 900
    labels:
      release_group: airship-prometheus-alertmanager
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
    pre:
      delete:
        - type: job
          labels:
            release_group: airship-prometheus-alertmanager
      create: []
    post:
      create: []
  values:
    manifests:
      ingress: false
      service_ingress: false
    labels:
      alertmanager:
        node_selector_key: openstack-control-plane
        node_selector_value: enabled
      job:
        node_selector_key: openstack-control-plane
        node_selector_value: enabled
  dependencies:
    - osh-infra-helm-toolkit
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: nfs-provisioner
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    # Chart source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.osh_infra.nfs_provisioner
      dest:
        path: .source

    # Images
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.osh_infra.nfs_provisioner
      dest:
        path: .values.images.tags
data:
  chart_name: nfs-provisioner
  release: nfs-provisioner
  namespace: nfs
  wait:
    timeout: 900
    labels:
      release_group: airship-nfs-provisioner
  values:
    storageclass:
      name: general
  dependencies:
    - osh-infra-helm-toolkit
...
---
schema: armada/ChartGroup/v1
metadata:
  schema: metadata/Document/v1
  name: osh-infra-nfs-provisioner
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
data:
  description: OSH Infra NFS Provisioner
  chart_group:
    - nfs-provisioner
...
---
schema: armada/ChartGroup/v1
metadata:
  schema: metadata/Document/v1
  name: osh-infra-logging
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
data:
  description: OSH Infra Logging
  sequenced: True
  chart_group:
    - elasticsearch
    - fluentbit
    - fluentd
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: elasticsearch-global
  labels:
    hosttype: elasticsearch-global
  layeringDefinition:
    abstract: true
    layer: global
  storagePolicy: cleartext
  substitutions:
    # Chart source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.osh_infra.elasticsearch
      dest:
        path: .source

    # Images
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.osh_infra.elasticsearch
      dest:
        path: .values.images.tags

    # Endpoints
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_infra_endpoints
        path: .osh_infra.elasticsearch
      dest:
        path: .values.endpoints.elasticsearch
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_infra_endpoints
        path: .osh_infra.prometheus_elasticsearch_exporter
      dest:
        path: .values.endpoints.prometheus_elasticsearch_exporter

    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_infra_endpoints
        path: .osh_infra.ldap
      dest:
        path: .values.endpoints.ldap

    # Accounts
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_infra_service_accounts
        path: .osh_infra.elasticsearch.admin
      dest:
        path: .values.endpoints.elasticsearch.auth.admin
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_infra_service_accounts
        path: .osh_infra.ceph_object_store.admin
      dest:
        path: .values.endpoints.ceph_object_store.auth.admin
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_infra_service_accounts
        path: .osh_infra.ceph_object_store.elasticsearch
      dest:
        path: .values.endpoints.ceph_object_store.auth.elasticsearch

    # Secrets
    - dest:
        path: .values.endpoints.elasticsearch.auth.admin.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_infra_elasticsearch_admin_password
        path: .
    - dest:
        path: .values.endpoints.ceph_object_store.auth.admin.access_key
      src:
        schema: deckhand/Passphrase/v1
        name: osh_infra_rgw_s3_admin_access_key
        path: .
    - dest:
        path: .values.endpoints.ceph_object_store.auth.admin.secret_key
      src:
        schema: deckhand/Passphrase/v1
        name: osh_infra_rgw_s3_admin_secret_key
        path: .
    - dest:
        path: .values.endpoints.ceph_object_store.auth.elasticsearch.access_key
      src:
        schema: deckhand/Passphrase/v1
        name: osh_infra_rgw_s3_elasticsearch_access_key
        path: .
    - dest:
        path: .values.endpoints.ceph_object_store.auth.elasticsearch.secret_key
      src:
        schema: deckhand/Passphrase/v1
        name: osh_infra_rgw_s3_elasticsearch_secret_key
        path: .

    # LDAP Details
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_infra_service_accounts
        path: .osh_infra.ldap.admin
      dest:
        path: .values.endpoints.ldap.auth.admin
    - dest:
        path: .values.endpoints.ldap.auth.admin.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_keystone_ldap_password
        path: .
data:
  chart_name: elasticsearch
  release: elasticsearch
  namespace: osh-infra
  wait:
    timeout: 900
    labels:
      release_group: airship-elasticsearch
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
    pre:
      delete:
        - type: job
          labels:
            release_group: airship-elasticsearch
      create: []
    post:
      create: []
  values:
    pod:
      replicas:
        client: 5
      resources:
        enabled: true
        apache_proxy:
          limits:
            memory: "1024Mi"
            cpu: "2000m"
          requests:
            memory: "0"
            cpu: "0"
        client:
          requests:
            memory: "8Gi"
            cpu: "1000m"
          limits:
            memory: "16Gi"
            cpu: "2000m"
        master:
          requests:
            memory: "8Gi"
            cpu: "1000m"
          limits:
            memory: "16Gi"
            cpu: "2000m"
        data:
          requests:
            memory: "8Gi"
            cpu: "1000m"
          limits:
            memory: "16Gi"
            cpu: "2000m"
        prometheus_elasticsearch_exporter:
          requests:
            memory: "0"
            cpu: "0"
          limits:
            memory: "1024Mi"
            cpu: "2000m"
        jobs:
          curator:
            requests:
              memory: "0"
              cpu: "0"
            limits:
              memory: "1024Mi"
              cpu: "2000m"
          image_repo_sync:
            requests:
              memory: "0"
              cpu: "0"
            limits:
              memory: "1024Mi"
              cpu: "2000m"
          snapshot_repository:
            requests:
              memory: "0"
              cpu: "0"
            limits:
              memory: "1024Mi"
              cpu: "2000m"
          tests:
            requests:
              memory: "0"
              cpu: "0"
            limits:
              memory: "1024Mi"
              cpu: "2000m"
    labels:
      elasticsearch:
        node_selector_key: openstack-control-plane
        node_selector_value: enabled
      job:
        node_selector_key: openstack-control-plane
        node_selector_value: enabled
      test:
        node_selector_key: openstack-control-plane
        node_selector_value: enabled
    monitoring:
      prometheus:
        enabled: true
    conf:
      httpd: |
        ServerRoot "/usr/local/apache2"
        Listen 80
        LoadModule mpm_event_module modules/mod_mpm_event.so
        LoadModule authn_file_module modules/mod_authn_file.so
        LoadModule authn_core_module modules/mod_authn_core.so
        LoadModule authz_host_module modules/mod_authz_host.so
        LoadModule authz_groupfile_module modules/mod_authz_groupfile.so
        LoadModule authz_user_module modules/mod_authz_user.so
        LoadModule authz_core_module modules/mod_authz_core.so
        LoadModule access_compat_module modules/mod_access_compat.so
        LoadModule auth_basic_module modules/mod_auth_basic.so
        LoadModule ldap_module modules/mod_ldap.so
        LoadModule authnz_ldap_module modules/mod_authnz_ldap.so
        LoadModule reqtimeout_module modules/mod_reqtimeout.so
        LoadModule filter_module modules/mod_filter.so
        LoadModule proxy_html_module modules/mod_proxy_html.so
        LoadModule log_config_module modules/mod_log_config.so
        LoadModule env_module modules/mod_env.so
        LoadModule headers_module modules/mod_headers.so
        LoadModule setenvif_module modules/mod_setenvif.so
        LoadModule version_module modules/mod_version.so
        LoadModule proxy_module modules/mod_proxy.so
        LoadModule proxy_connect_module modules/mod_proxy_connect.so
        LoadModule proxy_http_module modules/mod_proxy_http.so
        LoadModule proxy_balancer_module modules/mod_proxy_balancer.so
        LoadModule slotmem_shm_module modules/mod_slotmem_shm.so
        LoadModule slotmem_plain_module modules/mod_slotmem_plain.so
        LoadModule unixd_module modules/mod_unixd.so
        LoadModule status_module modules/mod_status.so
        LoadModule autoindex_module modules/mod_autoindex.so
        <IfModule unixd_module>
        User daemon
        Group daemon
        </IfModule>
        <Directory />
            AllowOverride none
            Require all denied
        </Directory>
        <Files ".ht*">
            Require all denied
        </Files>
        ErrorLog /dev/stderr
        LogLevel warn
        <IfModule log_config_module>
            LogFormat "%h %l %u %t \"%r\" %>s %b \"%{Referer}i\" \"%{User-Agent}i\"" combined
            LogFormat "%h %l %u %t \"%r\" %>s %b" common
            <IfModule logio_module>
              LogFormat "%h %l %u %t \"%r\" %>s %b \"%{Referer}i\" \"%{User-Agent}i\" %I %O" combinedio
            </IfModule>
            CustomLog /dev/stdout common
            CustomLog /dev/stdout combined
        </IfModule>
        <Directory "/usr/local/apache2/cgi-bin">
            AllowOverride None
            Options None
            Require all granted
        </Directory>
        <IfModule headers_module>
            RequestHeader unset Proxy early
        </IfModule>
        <IfModule proxy_html_module>
        Include conf/extra/proxy-html.conf
        </IfModule>
        <VirtualHost *:80>
          <Location />
              ProxyPass http://localhost:{{ tuple "elasticsearch" "internal" "client" . | include "helm-toolkit.endpoints.endpoint_port_lookup" }}/
              ProxyPassReverse http://localhost:{{ tuple "elasticsearch" "internal" "client" . | include "helm-toolkit.endpoints.endpoint_port_lookup" }}/
          </Location>
          <Proxy *>
              AuthName "Elasticsearch"
              AuthType Basic
              AuthBasicProvider file ldap
              AuthUserFile /usr/local/apache2/conf/.htpasswd
              AuthLDAPBindDN {{ .Values.endpoints.ldap.auth.admin.bind }}
              AuthLDAPBindPassword {{ .Values.endpoints.ldap.auth.admin.password }}
              AuthLDAPURL {{ tuple "ldap" "public" "ldap" . | include "helm-toolkit.endpoints.keystone_endpoint_uri_lookup" | quote }}
              Require valid-user
          </Proxy>
        </VirtualHost>
      elasticsearch:
        config:
          http:
            max_content_length: 2gb
            pipelining: false
        env:
          java_opts:
            client: "-Xms8g -Xmx8g"
            data: "-Xms8g -Xmx8g"
            master: "-Xms8g -Xmx8g"
        snapshots:
          enabled: true
      curator:
        #run every 6th hour
        schedule:  "0 */6 * * *"
        action_file:
          # Remember, leave a key empty if there is no value.  None will be a string,
          # not a Python "NoneType"
          #
          # Also remember that all examples have 'disable_action' set to True.  If you
          # want to use this action as a template, be sure to set this to False after
          # copying it.
          actions:
            1:
              action: delete_indices
              description: >-
                "Delete indices older than 7 days"
              options:
                timeout_override:
                continue_if_exception: False
                ignore_empty_list: True
                disable_action: False
              filters:
              - filtertype: pattern
                kind: prefix
                value: logstash-
              - filtertype: age
                source: name
                direction: older
                timestring: '%Y.%m.%d'
                unit: days
                unit_count: 7
            2:
              action: delete_indices
              description: >-
                "Delete indices by age if available disk space is
                 less than 80% total disk"
              options:
                timeout_override: 600
                continue_if_exception: False
                ignore_empty_list: True
                disable_action: False
              filters:
              - filtertype: pattern
                kind: prefix
                value: logstash-
              - filtertype: space
                source: creation_date
                use_age: True
                disk_space: 1200
    storage:
      requests:
        storage: 500Gi
  dependencies:
    - osh-infra-helm-toolkit
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: fluentbit-global
  layeringDefinition:
    abstract: true
    layer: global
  labels:
    hosttype: fluentbit-global
  storagePolicy: cleartext
  substitutions:
    # Chart source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.osh_infra.fluentbit
      dest:
        path: .source

    # Images
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.osh_infra.fluentbit
      dest:
        path: .values.images.tags

    # Endpoints
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_infra_endpoints
        path: .osh_infra.fluentd # TODO change it in OSH repo
      dest:
        path: .values.endpoints.fluentbit

data:
  chart_name: fluentbit
  release: fluentbit
  namespace: osh-infra
  wait:
    timeout: 900
    labels:
      release_group: airship-fluentbit
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
    pre:
      delete:
        - type: job
          labels:
            release_group: airship-fluentbit
      create: []
    post:
      create: []
  values:
    monitoring:
      prometheus:
        enabled: true
    pod:
      resources:
        enabled: true
        fluentbit:
          limits:
            memory: '4Gi'
            cpu: '2000m'
          requests:
            memory: '2Gi'
            cpu: '1000m'
        jobs:
          image_repo_sync:
            requests:
              memory: '0'
              cpu: '0'
            limits:
              memory: '1024Mi'
              cpu: '2000m'
          tests:
            requests:
              memory: '0'
              cpu: '0'
            limits:
              memory: '1024Mi'
              cpu: '2000m'
    labels:
      fluentbit:
        node_selector_key: fluentbit
        node_selector_value: enabled
      job:
        node_selector_key: openstack-control-plane
        node_selector_value: enabled
    conf:
      fluentbit:
        template: |
          [SERVICE]
              Daemon false
              Flush 5
              Log_Level info
              Parsers_File parsers.conf

          [INPUT]
              Buffer_Chunk_Size 1M
              Buffer_Max_Size 1M
              Mem_Buf_Limit 5MB
              Name tail
              Path /var/log/kern.log
              Tag kernel

          [INPUT]
              Buffer_Chunk_Size 1M
              Buffer_Max_Size 1M
              Mem_Buf_Limit 5MB
              Name tail
              Parser docker
              Path /var/log/containers/*.log
              Tag kube.*

          [INPUT]
              Buffer_Chunk_Size 1M
              Buffer_Max_Size 1M
              Mem_Buf_Limit 5MB
              Name tail
              Path /var/log/libvirt/libvirtd.log
              Tag libvirt

          [INPUT]
              Buffer_Chunk_Size 1M
              Buffer_Max_Size 1M
              Mem_Buf_Limit 5MB
              Name tail
              Path /var/log/libvirt/qemu/*.log
              Tag qemu

          [INPUT]
              Buffer_Chunk_Size 1M
              Buffer_Max_Size 1M
              Mem_Buf_Limit 5MB
              Name systemd
              Path ${JOURNAL_PATH}
              Systemd_Filter _SYSTEMD_UNIT=kubelet.service
              Tag journal.*

          [INPUT]
              Buffer_Chunk_Size 1M
              Buffer_Max_Size 1M
              Mem_Buf_Limit 5MB
              Name systemd
              Path ${JOURNAL_PATH}
              Systemd_Filter _SYSTEMD_UNIT=docker.service
              Tag journal.*

          [INPUT]
              Buffer_Chunk_Size 1M
              Buffer_Max_Size 1M
              Mem_Buf_Limit 5MB
              Name tail
              Parsers syslog
              Path /var/log/ceph/airship-ucp-ceph-mon/ceph.log
              Tag ceph.cluster.*

          [INPUT]
              Buffer_Chunk_Size 1M
              Buffer_Max_Size 1M
              Mem_Buf_Limit 5MB
              Name tail
              Parsers syslog
              Path /var/log/ceph/airship-ucp-ceph-mon/ceph.audit.log
              Tag ceph.audit.*

          [INPUT]
              Buffer_Chunk_Size 1M
              Buffer_Max_Size 1M
              Mem_Buf_Limit 5MB
              Name tail
              Parsers syslog
              Path /var/log/ceph/airship-ucp-ceph-mon/ceph-mon**.log
              Tag ceph.mon.*

          [INPUT]
              Buffer_Chunk_Size 1M
              Buffer_Max_Size 1M
              Mem_Buf_Limit 5MB
              Name tail
              Parsers syslog
              Path /var/log/ceph/airship-ucp-ceph-osd/ceph-osd**.log
              Tag ceph.osd.*

          [FILTER]
              Interval 1s
              Match **
              Name throttle
              Rate 1000
              Window 300

          [FILTER]
              Match libvirt
              Name record_modifier
              Record hostname ${HOSTNAME}

          [FILTER]
              Match qemu
              Name record_modifier
              Record hostname ${HOSTNAME}

          [FILTER]
              Match kernel
              Name record_modifier
              Record hostname ${HOSTNAME}

          [FILTER]
              Match journal.**
              Name modify
              Rename _BOOT_ID BOOT_ID
              Rename _CAP_EFFECTIVE CAP_EFFECTIVE
              Rename _CMDLINE CMDLINE
              Rename _COMM COMM
              Rename _EXE EXE
              Rename _GID GID
              Rename _HOSTNAME HOSTNAME
              Rename _MACHINE_ID MACHINE_ID
              Rename _PID PID
              Rename _SYSTEMD_CGROUP SYSTEMD_CGROUP
              Rename _SYSTEMD_SLICE SYSTEMD_SLICE
              Rename _SYSTEMD_UNIT SYSTEMD_UNIT
              Rename _TRANSPORT TRANSPORT
              Rename _UID UID

          [OUTPUT]
              Match **.fluentd**
              Name null

          [FILTER]
              Match kube.*
              Merge_JSON_Log true
              Name kubernetes

          [OUTPUT]
              Host ${FLUENTD_HOST}
              Match *
              Name forward
              Port ${FLUENTD_PORT}
      parsers:
        template: |
          [PARSER]
            Name syslog
            Format regex
            Regex '^(?<time>.*[0-9]{2}:[0-9]{2}:[0-9]{2}) (?<host>[^ ]*) (?<app>[a-zA-Z0-9_\/\.\-]*)(?:\[(?<pid>[0-9]+)\])?(?:[^\:]*\:)? (?<log>.+)$'
            Time_Key time
            Time_Format "%Y-%m-%dT%H:%M:%S.%L"
            Time_Keep On
            Types "pid:integer"
  dependencies:
    - osh-infra-helm-toolkit
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: fluentd-global
  layeringDefinition:
    abstract: true
    layer: global
  labels:
    hosttype: fluentd-global
  storagePolicy: cleartext
  substitutions:
    # Chart source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.osh_infra.fluentd
      dest:
        path: .source

    # Images
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.osh_infra.fluentd
      dest:
        path: .values.images.tags

    # Endpoints
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_infra_endpoints
        path: .osh_infra.fluentd
      dest:
        path: .values.endpoints.fluentd
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_infra_endpoints
        path: .osh_infra.prometheus_fluentd_exporter
      dest:
        path: .values.endpoints.prometheus_fluentd_exporter

    # Accounts
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_infra_service_accounts
        path: .osh_infra.elasticsearch.admin
      dest:
        path: .values.endpoints.elasticsearch.auth.admin

    # Secrets
    - dest:
        path: .values.endpoints.elasticsearch.auth.admin.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_infra_elasticsearch_admin_password
        path: .

data:
  chart_name: fluentd
  release: fluentd
  namespace: osh-infra
  wait:
    timeout: 900
    labels:
      release_group: airship-fluentd
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
    pre:
      delete:
        - type: job
          labels:
            release_group: airship-fluentd
      create: []
    post:
      create: []
  values:
    monitoring:
      prometheus:
        enabled: true
    pod:
      resources:
        enabled: true
        fluentd:
          limits:
            memory: '4Gi'
            cpu: '2000m'
          requests:
            memory: '2Gi'
            cpu: '1000m'
        prometheus_fluentd_exporter:
          limits:
            memory: '1024Mi'
            cpu: '2000m'
          requests:
            memory: '0'
            cpu: '0'
        jobs:
          image_repo_sync:
            requests:
              memory: '0'
              cpu: '0'
            limits:
              memory: '1024Mi'
              cpu: '2000m'
          tests:
            requests:
              memory: '0'
              cpu: '0'
            limits:
              memory: '1024Mi'
              cpu: '2000m'
    labels:
      fluentd:
        node_selector_key: fluentd
        node_selector_value: enabled
      prometheus_fluentd_exporter:
        node_selector_key: openstack-control-plane
        node_selector_value: enabled
      job:
        node_selector_key: openstack-control-plane
        node_selector_value: enabled
    conf:
      fluentd:
        template: |
          <source>
            bind 0.0.0.0
            port 24220
            @type monitor_agent
          </source>

          <source>
            bind 0.0.0.0
            port "#{ENV['FLUENTD_PORT']}"
            @type forward
          </source>

          <match fluent.**>
            @type null
          </match>

          <match kube.var.log.containers.**.log>
            <rule>
              key log
              pattern /info/i
              tag info.${tag}
            </rule>
            <rule>
              key log
              pattern /warn/i
              tag warn.${tag}
            </rule>
            <rule>
              key log
              pattern /error/i
              tag error.${tag}
            </rule>
            <rule>
              key log
              pattern /critical/i
              tag critical.${tag}
            </rule>
            <rule>
              key log
              pattern (.+)
              tag info.${tag}
            </rule>
            @type rewrite_tag_filter
          </match>

          <filter **.kube.var.log.containers.**.log>
            enable_ruby true
            <record>
              application ${record["kubernetes"]["labels"]["application"]}
              level ${tag_parts[0]}
            </record>
            @type record_transformer
          </filter>

          <filter openstack.**>
            <record>
              application ${tag_parts[1]}
            </record>
            @type record_transformer
          </filter>

          <match openstack.**>
            <rule>
              key level
              pattern INFO
              tag info.${tag}
            </rule>
            <rule>
              key level
              pattern WARN
              tag warn.${tag}
            </rule>
            <rule>
              key level
              pattern ERROR
              tag error.${tag}
            </rule>
            <rule>
              key level
              pattern CRITICAL
              tag critical.${tag}
            </rule>
            @type rewrite_tag_filter
          </match>

          <match *.openstack.**>
            <rule>
              key application
              pattern keystone
              tag auth.${tag}
            </rule>
            <rule>
              key application
              pattern horizon
              tag auth.${tag}
            </rule>
            <rule>
              key application
              pattern mariadb
              tag auth.${tag}
            </rule>
            <rule>
              key application
              pattern memcached
              tag auth.${tag}
            </rule>
            <rule>
              key application
              pattern rabbitmq
              tag auth.${tag}
            </rule>
            @type rewrite_tag_filter
          </match>

          <match libvirt>
            <buffer>
              chunk_limit_size 8MB
              flush_interval 15s
              flush_thread_count 8
              queue_limit_length 256
              retry_forever false
              retry_max_interval 30
            </buffer>
            host "#{ENV['ELASTICSEARCH_HOST']}"
            reload_connections false
            reconnect_on_error true
            reload_on_failure true
            include_tag_key true
            logstash_format true
            logstash_prefix libvirt
            password "#{ENV['ELASTICSEARCH_PASSWORD']}"
            port "#{ENV['ELASTICSEARCH_PORT']}"
            @type elasticsearch
            user "#{ENV['ELASTICSEARCH_USERNAME']}"
          </match>

          <match qemu>
            <buffer>
              chunk_limit_size 8MB
              flush_interval 15s
              flush_thread_count 8
              queue_limit_length 256
              retry_forever false
              retry_max_interval 30
            </buffer>
            host "#{ENV['ELASTICSEARCH_HOST']}"
            reload_connections false
            reconnect_on_error true
            reload_on_failure true
            include_tag_key true
            logstash_format true
            logstash_prefix qemu
            password "#{ENV['ELASTICSEARCH_PASSWORD']}"
            port "#{ENV['ELASTICSEARCH_PORT']}"
            @type elasticsearch
            user "#{ENV['ELASTICSEARCH_USERNAME']}"
          </match>

          <match journal.**>
            <buffer>
              chunk_limit_size 8MB
              flush_interval 15s
              flush_thread_count 8
              queue_limit_length 256
              retry_forever false
              retry_max_interval 30
            </buffer>
            host "#{ENV['ELASTICSEARCH_HOST']}"
            reload_connections false
            reconnect_on_error true
            reload_on_failure true
            include_tag_key true
            logstash_format true
            logstash_prefix journal
            password "#{ENV['ELASTICSEARCH_PASSWORD']}"
            port "#{ENV['ELASTICSEARCH_PORT']}"
            @type elasticsearch
            user "#{ENV['ELASTICSEARCH_USERNAME']}"
          </match>

          <match kernel>
            <buffer>
              chunk_limit_size 8MB
              flush_interval 15s
              flush_thread_count 8
              queue_limit_length 256
              retry_forever false
              retry_max_interval 30
            </buffer>
            host "#{ENV['ELASTICSEARCH_HOST']}"
            reload_connections false
            reconnect_on_error true
            reload_on_failure true
            include_tag_key true
            logstash_format true
            logstash_prefix kernel
            password "#{ENV['ELASTICSEARCH_PASSWORD']}"
            port "#{ENV['ELASTICSEARCH_PORT']}"
            @type elasticsearch
            user "#{ENV['ELASTICSEARCH_USERNAME']}"
          </match>

          <match **>
            <buffer>
              chunk_limit_size 8MB
              flush_interval 15s
              flush_thread_count 8
              queue_limit_length 256
              retry_forever false
              retry_max_interval 30
            </buffer>
            host "#{ENV['ELASTICSEARCH_HOST']}"
            reload_connections false
            reconnect_on_error true
            reload_on_failure true
            include_tag_key true
            logstash_format true
            password "#{ENV['ELASTICSEARCH_PASSWORD']}"
            port "#{ENV['ELASTICSEARCH_PORT']}"
            @type elasticsearch
            type_name fluent
            user "#{ENV['ELASTICSEARCH_USERNAME']}"
          </match>

          <match *ceph-**.log>
            <buffer>
              chunk_limit_size 8MB
              flush_interval 15s
              flush_thread_count 8
              queue_limit_length 256
              retry_forever false
              retry_max_interval 30
            </buffer>
            host "#{ENV['ELASTICSEARCH_HOST']}"
            reload_connections false
            reconnect_on_error true
            reload_on_failure true
            include_tag_key true
            logstash_format true
            logstash_prefix ceph
            password "#{ENV['ELASTICSEARCH_PASSWORD']}"
            port "#{ENV['ELASTICSEARCH_PORT']}"
            @type elasticsearch
            user "#{ENV['ELASTICSEARCH_USERNAME']}"
          </match>
  dependencies:
    - osh-infra-helm-toolkit
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: osh-infra-ingress-controller
  labels:
    name: osh-infra-ingress-controller-global
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    # Chart source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.osh.ingress
      dest:
        path: .source
    # Images
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.osh.ingress
      dest:
        path: .values.images.tags
data:
  chart_name: osh-infra-ingress-controller
  release: osh-infra-ingress-controller
  namespace: osh-infra
  wait:
    timeout: 900
    labels:
      release_group: airship-osh-infra-ingress-controller
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
    pre:
      delete:
        - type: job
          labels:
            release_group: airship-osh-infra-ingress-controller
  values:
    labels:
      server:
        node_selector_key: openstack-control-plane
        node_selector_value: enabled
      error_server:
        node_selector_key: openstack-control-plane
        node_selector_value: enabled
    pod:
      replicas:
        ingress: 2
        error_page: 2
  dependencies:
    - osh-helm-toolkit
...
---
schema: armada/ChartGroup/v1
metadata:
  schema: metadata/Document/v1
  name: osh-infra-ingress-controller
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
data:
  description: OpenStack Namespace Ingress
  chart_group:
    - osh-infra-ingress-controller
...
---
schema: armada/ChartGroup/v1
metadata:
  schema: metadata/Document/v1
  name: osh-infra-prometheus-openstack-exporter
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
data:
  description: Prometheus OpenStack Exporter
  chart_group:
    - prometheus-openstack-exporter
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: prometheus-openstack-exporter
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    # Chart source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.osh_infra.prometheus_openstack_exporter
      dest:
        path: .source

    # Images
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.osh_infra.prometheus_openstack_exporter
      dest:
        path: .values.images.tags

    # Endpoints
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_infra_endpoints
        path: .osh_infra.prometheus_openstack_exporter
      dest:
        path: .values.endpoints.prometheus_openstack_exporter
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_endpoints
        path: .osh.identity
      dest:
        path: .values.endpoints.identity

    # Accounts
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_service_accounts
        path: .osh.keystone.admin
      dest:
        path: .values.endpoints.identity.auth.admin
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_infra_service_accounts
        path: .osh_infra.prometheus_openstack_exporter.user
      dest:
        path: .values.endpoints.identity.auth.user

    # Secrets
    - dest:
        path: .values.endpoints.identity.auth.admin.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_keystone_admin_password
        path: .
    - dest:
        path: .values.endpoints.identity.auth.user.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_infra_openstack_exporter_password
        path: .
data:
  chart_name: prometheus-openstack-exporter
  release: prometheus-openstack-exporter
  namespace: openstack
  wait:
    timeout: 900
    labels:
      release_group: airship-prometheus-openstack-exporter
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
    pre:
      delete:
        - type: job
          labels:
            release_group: airship-prometheus-openstack-exporter
  values:
    labels:
      openstack_exporter:
        node_selector_key: openstack-control-plane
        node_selector_value: enabled
      job:
        node_selector_key: openstack-control-plane
        node_selector_value: enabled
  dependencies:
    - osh-infra-helm-toolkit
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: kibana
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    # Chart source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.osh_infra.kibana
      dest:
        path: .source

    # Images
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.osh_infra.kibana
      dest:
        path: .values.images.tags

    # Endpoints
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_infra_endpoints
        path: .osh_infra.elasticsearch
      dest:
        path: .values.endpoints.elasticsearch
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_infra_endpoints
        path: .osh_infra.kibana
      dest:
        path: .values.endpoints.kibana
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_infra_endpoints
        path: .osh_infra.ldap
      dest:
        path: .values.endpoints.ldap
    # Accounts
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_infra_service_accounts
        path: .osh_infra.elasticsearch.admin
      dest:
        path: .values.endpoints.elasticsearch.auth.admin

    # Secrets
    - dest:
        path: .values.endpoints.elasticsearch.auth.admin.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_infra_elasticsearch_admin_password
        path: .

    # LDAP Details
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_infra_service_accounts
        path: .osh_infra.ldap.admin
      dest:
        path: .values.endpoints.ldap.auth.admin
    - dest:
        path: .values.endpoints.ldap.auth.admin.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_keystone_ldap_password
        path: .
data:
  chart_name: kibana
  release: kibana
  namespace: osh-infra
  wait:
    timeout: 900
    labels:
      release_group: airship-kibana
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
    pre:
      delete:
        - type: job
          labels:
            release_group: airship-kibana
      create: []
    post:
      create: []
  values:
    conf:
      apache:
        host: |
          <VirtualHost *:80>
            ProxyRequests off
            ProxyPreserveHost On
            <Location />
                ProxyPass http://localhost:{{ tuple "kibana" "internal" "kibana" . | include "helm-toolkit.endpoints.endpoint_port_lookup" }}/
                ProxyPassReverse http://localhost:{{ tuple "kibana" "internal" "kibana" . | include "helm-toolkit.endpoints.endpoint_port_lookup" }}/
            </Location>
            <Proxy *>
                AuthName "Kibana"
                AuthType Basic
                AuthBasicProvider file ldap
                AuthUserFile /usr/local/apache2/conf/.htpasswd
                AuthLDAPBindDN {{ .Values.endpoints.ldap.auth.admin.bind }}
                AuthLDAPBindPassword {{ .Values.endpoints.ldap.auth.admin.password }}
                AuthLDAPURL {{ tuple "ldap" "public" "ldap" . | include "helm-toolkit.endpoints.keystone_endpoint_uri_lookup" }}
                Require valid-user
            </Proxy>
          </VirtualHost>
    labels:
      kibana:
        node_selector_key: openstack-control-plane
        node_selector_value: enabled
      job:
        node_selector_key: openstack-control-plane
        node_selector_value: enabled
  dependencies:
    - osh-infra-helm-toolkit
...
---
schema: armada/ChartGroup/v1
metadata:
  schema: metadata/Document/v1
  name: osh-infra-dashboards
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
data:
  description: OSH Infra Dashboards
  chart_group:
    - kibana
    - grafana
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: grafana
  layeringDefinition:
    abstract: false
    layer: global
  labels:
    name: grafana-global
  storagePolicy: cleartext
  substitutions:
    # Chart source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.osh_infra.grafana
      dest:
        path: .source

    # Images
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.osh_infra.grafana
      dest:
        path: .values.images.tags

    # Endpoints
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_infra_endpoints
        path: .osh_infra.oslo_db
      dest:
        path: .values.endpoints.oslo_db
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_infra_endpoints
        path: .osh_infra.oslo_db
      dest:
        path: .values.endpoints.oslo_db_session
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_infra_endpoints
        path: .osh_infra.grafana
      dest:
        path: .values.endpoints.grafana
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_infra_endpoints
        path: .osh_infra.monitoring
      dest:
        path: .values.endpoints.monitoring
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_infra_endpoints
        path: .osh_infra.ldap
      dest:
        path: .values.endpoints.ldap
    # Accounts
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_infra_service_accounts
        path: .osh_infra.grafana.admin
      dest:
        path: .values.endpoints.grafana.auth.admin
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_infra_service_accounts
        path: .osh_infra.grafana.oslo_db
      dest:
        path: .values.endpoints.oslo_db.auth.user
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_infra_service_accounts
        path: .osh_infra.grafana.oslo_db.database
      dest:
        path: .values.endpoints.oslo_db.path
        pattern: DB_NAME
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_infra_service_accounts
        path: .osh_infra.grafana.oslo_db_session
      dest:
        path: .values.endpoints.oslo_db_session.auth.user
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_infra_service_accounts
        path: .osh_infra.grafana.oslo_db_session.database
      dest:
        path: .values.endpoints.oslo_db_session.path
        pattern: DB_NAME
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_infra_service_accounts
        path: .osh_infra.prometheus.admin
      dest:
        path: .values.endpoints.monitoring.auth.user

    # Secrets
    - dest:
        path: .values.endpoints.grafana.auth.admin.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_infra_grafana_admin_password
        path: .
    - dest:
        path: .values.endpoints.oslo_db.auth.user.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_infra_grafana_oslo_db_password
        path: .
    - dest:
        path: .values.endpoints.oslo_db_session.auth.user.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_infra_grafana_oslo_db_session_password
        path: .
    - dest:
        path: .values.endpoints.oslo_db.auth.admin.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_infra_oslo_db_admin_password
        path: .
    - dest:
        path: .values.endpoints.oslo_db_session.auth.admin.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_infra_oslo_db_admin_password
        path: .
    - dest:
        path: .values.endpoints.monitoring.auth.user.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_infra_prometheus_admin_password
        path: .

    # LDAP Configuration Details
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_infra_service_accounts
        path: .osh_infra.ldap.admin.bind
      dest:
        path: .values.endpoints.ldap.auth.admin.bind_dn
    - dest:
        path: .values.endpoints.ldap.auth.admin.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_keystone_ldap_password
        path: .
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .ldap.subdomain
      dest:
        path:  .values.conf.ldap.config.base_dns.search
        pattern: SUBDOMAIN
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .ldap.domain
      dest:
        path:  .values.conf.ldap.config.base_dns.search
        pattern: DOMAIN
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .ldap.subdomain
      dest:
        path:  .values.conf.ldap.config.base_dns.group_search
        pattern: SUBDOMAIN
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .ldap.domain
      dest:
        path:  .values.conf.ldap.config.base_dns.group_search
        pattern: DOMAIN
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .ldap.common_name
      dest:
        path:  .values.conf.ldap.config.filters.group_search
        pattern: COMMON_NAME
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .ldap.subdomain
      dest:
        path:  .values.conf.ldap.config.filters.group_search
        pattern: SUBDOMAIN
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .ldap.domain
      dest:
        path:  .values.conf.ldap.config.filters.group_search
        pattern: DOMAIN
data:
  chart_name: grafana
  release: grafana
  namespace: osh-infra
  wait:
    timeout: 900
    labels:
      release_group: airship-grafana
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
    pre:
      delete:
        - type: job
          labels:
            release_group: airship-grafana
    post:
      create: []
  values:
    labels:
      grafana:
        node_selector_key: openstack-control-plane
        node_selector_value: enabled
      job:
        node_selector_key: openstack-control-plane
        node_selector_value: enabled
    conf:
      provisioning:
        datasources:
          monitoring:
            url: http://prom-metrics.osh-infra.svc.cluster.local:80/
      ldap:
        config:
          base_dns:
            search: "DC=SUBDOMAIN,DC=DOMAIN,DC=com"
            group_search: "OU=Groups,DC=SUBDOMAIN,DC=DOMAIN,DC=com"
          filters:
            search: "(sAMAccountName=%s)"
            group_search: "(memberof=CN=COMMON_NAME,OU=Application,OU=Groups,DC=SUBDOMAIN,DC=DOMAIN,DC=com)"
        template: |
          verbose_logging = true
          [[servers]]
          host = "{{ tuple "ldap" "public" . | include "helm-toolkit.endpoints.hostname_fqdn_endpoint_lookup" }}"
          port = {{ tuple "ldap" "public" "ldap" . | include "helm-toolkit.endpoints.endpoint_port_lookup" }}
          use_ssl = false
          start_tls = false
          ssl_skip_verify = false
          bind_dn = "{{ .Values.endpoints.ldap.auth.admin.bind_dn }}"
          bind_password = '{{ .Values.endpoints.ldap.auth.admin.password }}'
          search_filter = "{{ .Values.conf.ldap.config.filters.search }}"
          search_base_dns = ["{{ .Values.conf.ldap.config.base_dns.search }}"]
          group_search_base_dns = ["{{ .Values.conf.ldap.config.base_dns.group_search }}"]
          [servers.attributes]
          username = "sAMAccountName"
          surname = "sn"
          member_of = "memberof"
          email = "mail"
          [[servers.group_mappings]]
          group_dn = "{{.Values.endpoints.ldap.auth.admin.bind_dn }}"
          org_role = "Admin"
          [[servers.group_mappings]]
          group_dn = "*"
          org_role = "Viewer"
    pod:
      replicas:
        grafana: 2
  dependencies:
    - osh-infra-helm-toolkit
...
---
schema: armada/ChartGroup/v1
metadata:
  schema: metadata/Document/v1
  name: podsecuritypolicy
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
data:
  description: Cluster Pod Security Policy definitions
  chart_group:
    - podsecuritypolicy
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: podsecuritypolicy
  labels:
    name: podsecuritypolicy-global
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    # Chart source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.osh_infra.podsecuritypolicy
      dest:
        path: .source
data:
  chart_name: podsecuritypolicy
  release: podsecuritypolicy
  namespace: ucp
  wait:
    resources: []
  install:
    no_hooks: true
  upgrade:
    no_hooks: true
  values:
    conf:
      # This defines creation of ClusterRoleBindings that configure
      # default PodSecurityPolicies for the subjects below.
      # `nil` avoids creation of a default binding for the subject.
      #
      defaults:
        serviceaccounts: psp-default
        authenticated: psp-default
        unauthenticated: nil
    data:
      # Each of these corresponds to the `spec` of a PodSecurityPolicy object.
      # Note that this default PodSecurityPolicy is incredibly permissive.  It is
      # intended to be tuned over time as a default, and to be overridden by
      # operators as appropriate.
      #
      # A ClusterRole will be created for the PSP, with the same `metadata.name`.
      #
      # Note: you can define as many PSPs here as you need.
      #
      psp-default:  # This will be the `metadata.name` of the PodSecurityPolicy
        privileged: true
        allowPrivilegeEscalation: true
        hostNetwork: true
        hostPID: true
        hostIPC: true
        seLinux:
          rule: RunAsAny
        supplementalGroups:
          rule: RunAsAny
        runAsUser:
          rule: RunAsAny
        fsGroup:
          rule: RunAsAny
        volumes:
        - '*'
        allowedCapabilities:
        - '*'
        hostPorts:
        - min: 1
          max: 65536
  dependencies:
    - osh-infra-helm-toolkit
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: ucp-barbican
  labels:
    name: ucp-barbican-global
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    # Chart source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.ucp.barbican
      dest:
        path: .source

    # Images
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.ucp.barbican
      dest:
        path: .values.images.tags

    # Endpoints
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: ucp_endpoints
        path: .ucp.identity
      dest:
        path: .values.endpoints.identity
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: ucp_endpoints
        path: .ucp.key_manager
      dest:
        path: .values.endpoints.key_manager
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: ucp_endpoints
        path: .ucp.oslo_db
      dest:
        path: .values.endpoints.oslo_db
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: ucp_endpoints
        path: .ucp.oslo_cache
      dest:
        path: .values.endpoints.oslo_cache
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: ucp_endpoints
        path: .ucp.oslo_messaging
      dest:
        path: .values.endpoints.oslo_messaging
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_infra_endpoints
        path: .osh_infra.fluentd
      dest:
        path: .values.endpoints.fluentd
    # Credentials
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: ucp_service_accounts
        path: .ucp.barbican.keystone
      dest:
        path: .values.endpoints.identity.auth.barbican
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: ucp_service_accounts
        path: .ucp.keystone.admin
      dest:
        path: .values.endpoints.identity.auth.admin
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: ucp_service_accounts
        path: .ucp.barbican.oslo_db
      dest:
        path: .values.endpoints.oslo_db.auth.barbican
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: ucp_service_accounts
        path: .ucp.barbican.oslo_db.database
      dest:
        path: .values.endpoints.oslo_db.path
        pattern: DB_NAME
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: ucp_service_accounts
        path: .ucp.barbican.oslo_messaging
      dest:
        path: .values.endpoints.oslo_messaging.auth

    # Secrets
    - dest:
        path: .values.endpoints.identity.auth.admin.password
      src:
        schema: deckhand/Passphrase/v1
        name: ucp_keystone_admin_password
        path: .
    - dest:
        path: .values.endpoints.oslo_db.auth.admin.password
      src:
        schema: deckhand/Passphrase/v1
        name: ucp_oslo_db_admin_password
        path: .
    - dest:
        path: .values.endpoints.identity.auth.barbican.password
      src:
        schema: deckhand/Passphrase/v1
        name: ucp_barbican_keystone_password
        path: .
    - dest:
        path: .values.endpoints.oslo_db.auth.barbican.password
      src:
        schema: deckhand/Passphrase/v1
        name: ucp_barbican_oslo_db_password
        path: .
    - dest:
        path: .values.endpoints.oslo_messaging.auth.admin.password
      src:
        schema: deckhand/Passphrase/v1
        name: ucp_oslo_messaging_password
        path: .
    - dest:
        path: .values.endpoints.oslo_messaging.auth.barbican.password
      src:
        schema: deckhand/Passphrase/v1
        name: ucp_oslo_messaging_password
        path: .
data:
  chart_name: ucp-barbican
  release: ucp-barbican
  namespace: ucp
  wait:
    timeout: 300
    labels:
      release_group: airship-ucp-barbican
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
    pre:
      delete:
        - type: job
          labels:
            release_group: airship-ucp-barbican
    post:
      create: []
  values:
    conf:
      logging:
        loggers:
          keys:
            - root
            - barbican
        handlers:
          keys:
            - stdout
            - stderr
            - 'null'
            - fluent
        formatters:
          keys:
            - context
            - default
            - fluent
        logger_root:
          level: WARNING
          handlers: stdout
        logger_barbican:
          level: INFO
          handlers:
            - stdout
            - fluent
          qualname: barbican
        logger_amqp:
          level: WARNING
          handlers: stderr
          qualname: amqp
        logger_amqplib:
          level: WARNING
          handlers: stderr
          qualname: amqplib
        logger_eventletwsgi:
          level: WARNING
          handlers: stderr
          qualname: eventlet.wsgi.server
        logger_sqlalchemy:
          level: WARNING
          handlers: stderr
          qualname: sqlalchemy
        logger_boto:
          level: WARNING
          handlers: stderr
          qualname: boto
        handler_null:
          class: logging.NullHandler
          formatter: default
          args: ()
        handler_stdout:
          class: StreamHandler
          args: (sys.stdout,)
          formatter: context
        handler_stderr:
          class: StreamHandler
          args: (sys.stderr,)
          formatter: context
        handler_fluent:
          class: fluent.handler.FluentHandler
          args: ('ucp.barbican', 'fluentd-logging.osh-infra', 24224)
          formatter: fluent
        formatter_fluent:
          class: oslo_log.formatters.FluentFormatter
        formatter_context:
          class: oslo_log.formatters.ContextFormatter
        formatter_default:
          format: "%(message)s"
    labels:
      api:
        node_selector_key: ucp-control-plane
        node_selector_value: enabled
      job:
        node_selector_key: ucp-control-plane
        node_selector_value: enabled
      test:
        node_selector_key: ucp-control-plane
        node_selector_value: enabled
    pod:
      replicas:
        api: 2
  dependencies:
    - ucp-barbican-htk
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: ucp-barbican-htk
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.ucp.barbican-htk
      dest:
        path: .source
data:
  chart_name: ucp-barbican-htk
  release: ucp-barbican-htk
  namespace: ucp-barbican-htk
  values: {}
  dependencies: []
...
---
schema: armada/ChartGroup/v1
metadata:
  schema: metadata/Document/v1
  name: ucp-deckhand
  labels:
    name: ucp-deckhand-chart-group-global
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
data:
  description: Deckhand
  chart_group:
    # NOTE: Find and add the dogtag chart
    # - ucp-dogtag
    - ucp-barbican
    - ucp-deckhand
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: ucp-deckhand
  labels:
    name: ucp-deckhand-global
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    # Chart source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.ucp.deckhand
      dest:
        path: .source

    # Images

    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.ucp.deckhand
      dest:
        path: .values.images.tags

    # Endpoints
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: ucp_endpoints
        path: .ucp.identity
      dest:
        path: .values.endpoints.identity
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: ucp_endpoints
        path: .ucp.postgresql
      dest:
        path: .values.endpoints.postgresql
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: ucp_endpoints
        path: .ucp.deckhand
      dest:
        path: .values.endpoints.deckhand
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: ucp_endpoints
        path: .ucp.key_manager
      dest:
        path: .values.endpoints.key_manager
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: ucp_endpoints
        path: .ucp.oslo_cache
      dest:
        path: .values.endpoints.oslo_cache
    # Credentials
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: ucp_service_accounts
        path: .ucp.postgres.admin
      dest:
        path: .values.endpoints.postgresql.auth.admin
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: ucp_service_accounts
        path: .ucp.deckhand.postgres
      dest:
        path: .values.endpoints.postgresql.auth.user
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: ucp_service_accounts
        path: .ucp.deckhand.postgres.database
      dest:
        path: .values.endpoints.postgresql.path
        pattern: DB_NAME
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: ucp_service_accounts
        path: .ucp.deckhand.keystone
      dest:
        path: .values.endpoints.identity.auth.user
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: ucp_service_accounts
        path: .ucp.keystone.admin
      dest:
        path: .values.endpoints.identity.auth.admin

    # Secrets
    - dest:
        path: .values.endpoints.identity.auth.admin.password
      src:
        schema: deckhand/Passphrase/v1
        name: ucp_keystone_admin_password
        path: .
    - dest:
        path: .values.endpoints.postgresql.auth.admin.password
      src:
        schema: deckhand/Passphrase/v1
        name: ucp_postgres_admin_password
        path: .
    - dest:
        path: .values.endpoints.identity.auth.user.password
      src:
        schema: deckhand/Passphrase/v1
        name: ucp_deckhand_keystone_password
        path: .
    - dest:
        path: .values.endpoints.postgresql.auth.user.password
      src:
        schema: deckhand/Passphrase/v1
        name: ucp_deckhand_postgres_password
        path: .
data:
  chart_name: ucp-deckhand
  release: ucp-deckhand
  namespace: ucp
  wait:
    timeout: 600
    labels:
      release_group: airship-ucp-deckhand
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
    pre:
      delete:
        - type: job
          labels:
            release_group: airship-ucp-deckhand
    post:
      create: []
  values:
    pod:
      replicas:
        deckhand: 2
    conf:
      deckhand:
        DEFAULT:
          debug: true
          use_stderr: true
          use_syslog: true
        keystone_authtoken:
          memcache_security_strategy: None
  dependencies:
    - deckhand-htk
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: deckhand-htk
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.ucp.deckhand-htk
      dest:
        path: .source
data:
  chart_name: deckhand-htk
  release: deckhand-htk
  namespace: deckhand-htk
  values: {}
  dependencies: []
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: ucp-promenade-global
  labels:
    name: ucp-promenade-global
  layeringDefinition:
    abstract: true
    layer: global
  storagePolicy: cleartext
  substitutions:

    # Chart source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.ucp.promenade
      dest:
        path: .source

    # Images
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.ucp.promenade
      dest:
        path: .values.images.tags

    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.ucp.promenade.monitoring_image
      dest:
        path: .values.images.tags.monitoring_image

    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.kubernetes.hyperkube
      dest:
        path: .values.images.tags.hyperkube

    # Files
    - src:
        schema: promenade/HostSystem/v1
        name: host-system
        path: .files[0].path
      dest:
        path: .values.pod.mount_path

    # Endpoints

    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: ucp_endpoints
        path: .ucp.identity
      dest:
        path: .values.endpoints.identity
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: ucp_endpoints
        path: .ucp.kubernetesprovisioner
      dest:
        path: .values.endpoints.kubernetesprovisioner

    # Credentials

    - src:
        schema: pegleg/AccountCatalogue/v1
        name: ucp_service_accounts
        path: .ucp.keystone.admin
      dest:
        path: .values.endpoints.identity.auth.admin
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: ucp_service_accounts
        path: .ucp.promenade.keystone
      dest:
        path: .values.endpoints.identity.auth.user

    # Secrets
    - dest:
        path: .values.endpoints.identity.auth.admin.password
      src:
        schema: deckhand/Passphrase/v1
        name: ucp_keystone_admin_password
        path: .
    - dest:
        path: .values.endpoints.identity.auth.user.password
      src:
        schema: deckhand/Passphrase/v1
        name: ucp_promenade_keystone_password
        path: .

data:
  chart_name: promenade
  release: ucp-promenade
  namespace: ucp
  wait:
    timeout: 600
    labels:
      release_group: airship-ucp-promenade
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
    pre:
      delete:
        - type: job
          labels:
            release_group: airship-ucp-promenade
  values:
    pod:
      replicas:
        api: 2
      env:
        promenade_api:
         # this aligns with drydocks timeouts and allows alow responses to
         # download the external kubernetes client .tgz to still succeed
         - name: UWSGI_TIMEOUT
           value: "900"
    conf:
      paste:
        filter:authtoken:
          paste.filter_factory: keystonemiddleware.auth_token:filter_factory
          admin_tenant_name: service
          admin_user: promenade
          delay_auth_decision: true
          identity_uri: http://keystone-api.ucp.svc.cluster.local/
          service_token_roles_required: true
  dependencies:
    - promenade-htk
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: promenade-htk
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.ucp.promenade-htk
      dest:
        path: .source
data:
  chart_name: promenade-htk
  release: promenade-htk
  namespace: promenade-htk
  values: {}
  dependencies: []
...
---
schema: armada/ChartGroup/v1
metadata:
  schema: metadata/Document/v1
  name: ucp-promenade
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
data:
  description: Promenade
  chart_group:
    - ucp-promenade
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: ucp-armada
  labels:
    name: ucp-armada-global
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    # Chart source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.ucp.armada
      dest:
        path: .source

    # Images
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.ucp.armada
      dest:
        path: .values.images.tags

    # Endpoints
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: ucp_endpoints
        path: .ucp.identity
      dest:
        path: .values.endpoints.identity
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: ucp_endpoints
        path: .ucp.armada
      dest:
        path: .values.endpoints.armada

    # Credentials
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: ucp_service_accounts
        path: .ucp.keystone.admin
      dest:
        path: .values.endpoints.identity.auth.admin
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: ucp_service_accounts
        path: .ucp.armada.keystone
      dest:
        path: .values.endpoints.identity.auth.user

    # Secrets
    - dest:
        path: .values.endpoints.identity.auth.admin.password
      src:
        schema: deckhand/Passphrase/v1
        name: ucp_keystone_admin_password
        path: .
    - dest:
        path: .values.endpoints.identity.auth.user.password
      src:
        schema: deckhand/Passphrase/v1
        name: ucp_armada_keystone_password
        path: .
data:
  chart_name: armada
  release: ucp-armada
  namespace: ucp
  wait:
    timeout: 100
    labels:
      release_group: airship-ucp-armada
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
    pre:
      delete:
        - type: job
          labels:
            release_group: airship-ucp-armada
  values:
    pod:
      replicas:
        api: 2
      env:
        armada_api:
          - name: ARMADA_UWSGI_TIMEOUT
            value: 14400
    conf:
      armada:
        DEFAULT:
          debug: true
          tiller_namespace: kube-system
    manifests:
      deployment_tiller: false
      service_tiller_deploy: false
  dependencies:
    - armada-htk
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: armada-htk
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.ucp.armada-htk
      dest:
        path: .source
data:
  chart_name: armada-htk
  release: armada-htk
  namespace: armada-htk
  values: {}
  dependencies: []
...
---
schema: armada/ChartGroup/v1
metadata:
  schema: metadata/Document/v1
  name: ucp-armada
  labels:
    name: ucp-armada-chart-group-global
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
data:
  description: Armada
  sequenced: true
  chart_group:
    - ucp-tiller
    - ucp-armada
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: ucp-tiller
  labels:
    name: ucp-tiller-global
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    # Chart source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.ucp.tiller
      dest:
        path: .source

    # Images
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.ucp.armada.tiller
      dest:
        path: .values.images.tags.tiller

data:
  chart_name: tiller
  release: ucp-tiller
  namespace: kube-system
  wait:
    timeout: 100
    labels:
      release_group: airship-ucp-tiller
    native:
      # Allows tiller to update its own release's status to DEPLOYED before it
      # goes away during an upgrade, otherwise it can get stuck in
      # PENDING_UPGRADE status.
      enabled: false
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
    pre:
      delete:
        - type: job
          labels:
            release_group: airship-ucp-tiller
  values: {}
  dependencies:
    - tiller-htk
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: tiller-htk
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.ucp.tiller-htk
      dest:
        path: .source
data:
  chart_name: tiller-htk
  release: tiller-htk
  namespace: tiller-htk
  values: {}
  dependencies: []
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: ucp-keystone
  labels:
    name: ucp-keystone-global
    component: keystone
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    # Chart source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.ucp.keystone
      dest:
        path: .source

    # Images
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.ucp.keystone
      dest:
        path: .values.images.tags

    # Endpoints
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: ucp_endpoints
        path: .ucp.identity
      dest:
        path: .values.endpoints.identity
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: ucp_endpoints
        path: .ucp.oslo_db
      dest:
        path: .values.endpoints.oslo_db
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: ucp_endpoints
        path: .ucp.oslo_messaging
      dest:
        path: .values.endpoints.oslo_messaging
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: ucp_endpoints
        path: .ucp.oslo_cache
      dest:
        path: .values.endpoints.oslo_cache
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_infra_endpoints
        path: .osh_infra.fluentd
      dest:
        path: .values.endpoints.fluentd
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: ucp_service_accounts
        path: .ucp.keystone.admin
      dest:
        path: .values.endpoints.identity.auth.admin
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: ucp_service_accounts
        path: .ucp.keystone.oslo_messaging
      dest:
        path: .values.endpoints.oslo_messaging.auth
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: ucp_service_accounts
        path: .ucp.keystone.oslo_db
      dest:
        path: .values.endpoints.oslo_db.auth.keystone
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: ucp_service_accounts
        path: .ucp.keystone.oslo_db.database
      dest:
        path: .values.endpoints.oslo_db.path
        pattern: DB_NAME

    # Secrets
    - dest:
        path: .values.endpoints.identity.auth.admin.password
      src:
        schema: deckhand/Passphrase/v1
        name: ucp_keystone_admin_password
        path: .
    - dest:
        path: .values.endpoints.oslo_messaging.auth.keystone.password
      src:
        schema: deckhand/Passphrase/v1
        name: ucp_oslo_messaging_password
        path: .
    - dest:
        path: .values.endpoints.oslo_messaging.auth.admin.password
      src:
        schema: deckhand/Passphrase/v1
        name: ucp_oslo_messaging_password
        path: .
    - dest:
        path: .values.endpoints.oslo_db.auth.keystone.password
      src:
        schema: deckhand/Passphrase/v1
        name: ucp_keystone_oslo_db_password
        path: .
    - dest:
        path: .values.endpoints.oslo_db.auth.admin.password
      src:
        schema: deckhand/Passphrase/v1
        name: ucp_oslo_db_admin_password
        path: .
data:
  chart_name: ucp-keystone
  release: ucp-keystone
  namespace: ucp
  test:
    timeout: 600
  wait:
    timeout: 600
    labels:
      release_group: airship-ucp-keystone
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
    pre:
      delete:
        - type: job
          labels:
            release_group: airship-ucp-keystone
    post:
      create: []
  values:
    conf:
      logging:
        loggers:
          keys:
            - root
            - keystone
        handlers:
          keys:
            - stdout
            - stderr
            - 'null'
            - fluent
        formatters:
          keys:
            - context
            - default
            - fluent
        logger_root:
          level: WARNING
          handlers: stdout
        logger_keystone:
          level: INFO
          handlers:
            - stdout
            - fluent
          qualname: keystone
        logger_amqp:
          level: WARNING
          handlers: stderr
          qualname: amqp
        logger_amqplib:
          level: WARNING
          handlers: stderr
          qualname: amqplib
        logger_eventletwsgi:
          level: WARNING
          handlers: stderr
          qualname: eventlet.wsgi.server
        logger_sqlalchemy:
          level: WARNING
          handlers: stderr
          qualname: sqlalchemy
        logger_boto:
          level: WARNING
          handlers: stderr
          qualname: boto
        handler_null:
          class: logging.NullHandler
          formatter: default
          args: ()
        handler_stdout:
          class: StreamHandler
          args: (sys.stdout,)
          formatter: context
        handler_stderr:
          class: StreamHandler
          args: (sys.stderr,)
          formatter: context
        handler_fluent:
          class: fluent.handler.FluentHandler
          args: ('ucp.keystone', 'fluentd-logging.osh-infra', 24224)
          formatter: fluent
        formatter_fluent:
          class: oslo_log.formatters.FluentFormatter
        formatter_context:
          class: oslo_log.formatters.ContextFormatter
        formatter_default:
          format: "%(message)s"
    pod:
      replicas:
        api: 2
    labels:
      api:
        node_selector_key: ucp-control-plane
        node_selector_value: enabled
      job:
        node_selector_key: ucp-control-plane
        node_selector_value: enabled


  dependencies:
    - ucp-keystone-htk
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name:  ucp-keystone-htk
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.ucp.keystone-htk
      dest:
        path: .source
data:
  chart_name: ucp-keystone-htk
  release: ucp-keystone-htk
  namespace: ucp-keystone-htk
  values: {}
  dependencies: []
...
---
schema: armada/ChartGroup/v1
metadata:
  schema: metadata/Document/v1
  name: ucp-keystone
  labels:
    name: ucp-keystone-chart-group-global
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
data:
  description: Airship Keystone components
  chart_group:
    - ucp-keystone-memcached
    - ucp-keystone
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: ucp-keystone-memcached
  labels:
    name: ucp-keystone-memcached-global
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    # Chart source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.ucp.memcached
      dest:
        path: .source

    # Images
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.ucp.memcached
      dest:
        path: .values.images.tags

    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: ucp_endpoints
        path: .ucp.oslo_cache
      dest:
        path: .values.endpoints.oslo_cache
data:
  chart_name: ucp-keystone-memcached
  release: ucp-keystone-memcached
  namespace: ucp
  wait:
    timeout: 600
    labels:
      release_group: airship-ucp-keystone-memcached
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
    pre:
      delete:
          - type: job
            labels:
              release_group: airship-ucp-keystone-memcached
  values:
    labels:
      server:
        node_selector_key: ucp-control-plane
        node_selector_value: enabled
  dependencies:
    - ucp-memcached-htk
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: ucp-memcached-htk
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.ucp.memcached-htk
      dest:
        path: .source
data:
  chart_name: ucp-memcached-htk
  release: ucp-memcached-htk
  namespace: ucp-memcached-htk
  values: {}
  dependencies: []
...
---
schema: armada/ChartGroup/v1
metadata:
  schema: metadata/Document/v1
  name: ucp-ceph-config
  labels:
    name: ucp-ceph-config-chart-group-global
    component: ceph
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
data:
  description: Ceph config for Airship namespace(s)
  chart_group:
    # NOTE: This will probably expand into one config per Airship namespace
    # that requires ceph access.
    - ucp-ceph-config
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: ucp-ceph-config
  labels:
    name: ucp-ceph-config-global
    components: ceph
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    # Chart source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.ucp.ceph-provisioners
      dest:
        path: .source

    # Images
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.ceph.ceph-provisioners
      dest:
        path: .values.images.tags

    # IP addresses
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .storage.ceph.public_cidr
      dest:
        path: .values.network.public
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .storage.ceph.cluster_cidr
      dest:
        path: .values.network.cluster

    # Endpoints
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: ucp_endpoints
        path: .ceph.ceph_mon
      dest:
        path: .values.endpoints.ceph_mon


data:
  chart_name: ucp-ceph-config
  release: ucp-ceph-config
  namespace: ucp
  wait:
    timeout: 900
    labels:
      release_group: airship-ucp-ceph-config
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
    pre:
      delete:
        - type: job
          labels:
            release_group: airship-ucp-ceph-config
  values:
    labels:
      job:
        node_selector_key: ucp-control-plane
        node_selector_value: enabled
      provisioner:
        node_selector_key: ucp-control-plane
        node_selector_value: enabled
    deployment:
      ceph: false
      client_secrets: true
      rbd_provisioner: false
      cephfs_provisioner: false
      rgw_keystone_user_and_endpoints: false
    bootstrap:
      enabled: false
    conf:
      rgw_ks:
        enabled: true
    storageclass:
      rbd:
        ceph_configmap_name: ceph-etc
        parameters:
          userSecretName: pvc-ceph-client-key
      cephfs:
        provision_storage_class: false
  dependencies:
    - ceph-htk
...
---
schema: armada/ChartGroup/v1
metadata:
  schema: metadata/Document/v1
  name: ucp-prometheus-openstack-exporter
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
data:
  description: Prometheus OpenStack Exporter for UCP Components
  chart_group:
    - ucp-prometheus-openstack-exporter
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: ucp-prometheus-openstack-exporter
  labels:
    name: ucp-prometheus-openstack-exporter-global
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    # Chart source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.osh_infra.prometheus_openstack_exporter
      dest:
        path: .source

    # Images
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.osh_infra.prometheus_openstack_exporter
      dest:
        path: .values.images.tags

    # Endpoints
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: ucp_endpoints
        path: .ucp.prometheus_openstack_exporter
      dest:
        path: .values.endpoints.prometheus_openstack_exporter
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: ucp_endpoints
        path: .ucp.identity
      dest:
        path: .values.endpoints.identity

    # Accounts
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: ucp_service_accounts
        path: .ucp.keystone.admin
      dest:
        path: .values.endpoints.identity.auth.admin
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: ucp_service_accounts
        path: .ucp.prometheus_openstack_exporter.user
      dest:
        path: .values.endpoints.identity.auth.user

    # Secrets
    - dest:
        path: .values.endpoints.identity.auth.admin.password
      src:
        schema: deckhand/Passphrase/v1
        name: ucp_keystone_admin_password
        path: .
    - dest:
        path: .values.endpoints.identity.auth.user.password
      src:
        schema: deckhand/Passphrase/v1
        name: ucp_openstack_exporter_keystone_password
        path: .
data:
  chart_name: ucp-prometheus-openstack-exporter
  release: ucp-prometheus-openstack-exporter
  namespace: ucp
  wait:
    timeout: 900
    labels:
      release_group: airship-ucp-prometheus-openstack-exporter
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
    pre:
      delete:
        - type: job
          labels:
            release_group: airship-ucp-prometheus-openstack-exporter
  values:
    labels:
      openstack_exporter:
        node_selector_key: openstack-control-plane
        node_selector_value: enabled
      job:
        node_selector_key: openstack-control-plane
        node_selector_value: enabled
  dependencies:
    - osh-infra-helm-toolkit
...
---
schema: armada/ChartGroup/v1
metadata:
  schema: metadata/Document/v1
  name: ucp-drydock
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
data:
  description: Drydock
  chart_group:
    - ucp-maas
    - ucp-drydock
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: ucp-drydock
  labels:
    name: ucp-drydock-global
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:

    # Chart source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.ucp.drydock
      dest:
        path: .source

    # Images
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.ucp.drydock
      dest:
        path: .values.images.tags

    # Endpoints

    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: ucp_endpoints
        path: .ucp.identity
      dest:
        path: .values.endpoints.identity
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: ucp_endpoints
        path: .ucp.postgresql
      dest:
        path: .values.endpoints.postgresql
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: ucp_endpoints
        path: .ucp.physicalprovisioner
      dest:
        path: .values.endpoints.physicalprovisioner
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: ucp_endpoints
        path: .ucp.maas_region
      dest:
        path: .values.endpoints.maas_region

    # Drydock IPs
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .node_ports.drydock_api
      dest:
        path: .values.network.drydock.node_port.port
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .node_ports.drydock_api
      dest:
        path: .values.endpoints.physicalprovisioner.port.api.nodeport

    # Credentials

    - src:
        schema: pegleg/AccountCatalogue/v1
        name: ucp_service_accounts
        path: .ucp.postgres.admin
      dest:
        path: .values.endpoints.postgresql.auth.admin
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: ucp_service_accounts
        path: .ucp.drydock.postgres
      dest:
        path: .values.endpoints.postgresql.auth.user
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: ucp_service_accounts
        path: .ucp.drydock.postgres.database
      dest:
        path: .values.endpoints.postgresql.path
        pattern: DB_NAME
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: ucp_service_accounts
        path: .ucp.keystone.admin
      dest:
        path: .values.endpoints.identity.auth.admin
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: ucp_service_accounts
        path: .ucp.drydock.keystone
      dest:
        path: .values.endpoints.identity.auth.drydock

    # Secrets
    - dest:
        path: .values.endpoints.identity.auth.admin.password
      src:
        schema: deckhand/Passphrase/v1
        name: ucp_keystone_admin_password
        path: .
    - dest:
        path: .values.endpoints.postgresql.auth.admin.password
      src:
        schema: deckhand/Passphrase/v1
        name: ucp_postgres_admin_password
        path: .
    - dest:
        path: .values.endpoints.identity.auth.drydock.password
      src:
        schema: deckhand/Passphrase/v1
        name: ucp_drydock_keystone_password
        path: .
    - dest:
        path: .values.endpoints.postgresql.auth.user.password
      src:
        schema: deckhand/Passphrase/v1
        name: ucp_drydock_postgres_password
        path: .

data:
  chart_name: drydock
  release: drydock
  namespace: ucp
  wait:
    timeout: 600
    labels:
      release_group: airship-drydock
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
    pre:
      delete:
        - type: job
          labels:
            release_group: airship-drydock
  values:
    labels:
      node_selector_key: ucp-control-plane
      node_selector_value: enabled
    network:
      api:
        nodeport:
          enabled: false
    conf:
      drydock:
        database:
          pool_size: 200
        plugins:
          ingester: drydock_provisioner.ingester.plugins.deckhand.DeckhandIngester
  dependencies:
    - drydock-htk
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: drydock-htk
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.ucp.drydock-htk
      dest:
        path: .source
data:
  chart_name: drydock-htk
  release: drydock-htk
  namespace: drydock-htk
  values: {}
  dependencies: []
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: ucp-maas-global
  labels:
    name: ucp-maas-global
  layeringDefinition:
    abstract: true
    layer: global
  storagePolicy: cleartext
  substitutions:
    # Chart source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.ucp.maas
      dest:
        path: .source

    # Images
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.ucp.maas
      dest:
        path: .values.images.tags

    # MaaS Config
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .dns.upstream_servers_joined
      dest:
        path: .values.conf.maas.dns.dns_servers
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .ntp.servers_joined
      dest:
        path: .values.conf.maas.ntp.ntp_servers
    - src:
        schema: deckhand/Passphrase/v1
        name: maas-region-key
        path: .
      dest:
        path: .values.secrets.maas_region.value

    # Endpoint substitutions
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: ucp_endpoints
        path: .ucp.postgresql
      dest:
        path: .values.endpoints.maas_db
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: ucp_endpoints
        path: .ucp.maas_region
      dest:
        path: .values.endpoints.maas_region
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: ucp_endpoints
        path: .ucp.maas_ingress
      dest:
        path: .values.endpoints.maas_ingress
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: ucp_endpoints
        path: .ucp.physicalprovisioner
      dest:
        path: .values.endpoints.physicalprovisioner
    # Account and credential substitutions
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: ucp_service_accounts
        path: .ucp.postgres.admin
      dest:
        path: .values.endpoints.maas_db.auth.admin
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: ucp_service_accounts
        path: .ucp.maas.postgres
      dest:
        path: .values.endpoints.maas_db.auth.user
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: ucp_service_accounts
        path: .ucp.maas.postgres.database
      dest:
        path: .values.endpoints.maas_db.path
        pattern: DB_NAME
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: ucp_service_accounts
        path: .ucp.maas.admin
      dest:
        path: .values.endpoints.maas_region.auth.admin

    # Secrets
    - dest:
        path: .values.endpoints.maas_region.auth.admin.password
      src:
        schema: deckhand/Passphrase/v1
        name: ucp_maas_admin_password
        path: .
    - dest:
        path: .values.endpoints.maas_db.auth.admin.password
      src:
        schema: deckhand/Passphrase/v1
        name: ucp_postgres_admin_password
        path: .
    - dest:
        path: .values.endpoints.maas_db.auth.user.password
      src:
        schema: deckhand/Passphrase/v1
        name: ucp_maas_postgres_password
        path: .
data:
  chart_name: maas
  release: maas
  namespace: ucp
  wait:
    timeout: 1800
    labels:
      release_group: airship-maas
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
    pre:
      delete:
        - type: job
          labels:
            release_group: airship-maas
  values:
    pod:
      replicas:
        region: 1
        rack: 1
      security_context:
        ingress:
          container:
            maas_ingress:
              runAsUser: 0
            maas_ingress_vip:
              runAsUser: 0
    labels:
      rack:
        node_selector_key: maas-rack
        node_selector_value: enabled
      region:
        node_selector_key: maas-region
        node_selector_value: enabled
    jobs:
      import_boot_resources:
        timeout: 1800

    conf:
      cache:
        enabled: true
      maas:
        images:
          default_os: 'ubuntu'
          default_image: 'xenial'
          default_kernel: 'hwe-16.04'
        credentials:
          secret:
            namespace: ucp
        proxy:
          # Use MAAS Built-in proxy. This supports environments where
          # the PXE interface can not reach the internet.
          # Also improves efficiency due to caching via MAAS.
          proxy_enabled: 'true'
        ntp:
          use_external_only: 'true'
          disable_ntpd_region: true
          disable_ntpd_rack: true
        dns:
          require_dnssec: 'no'
  dependencies:
    - maas-htk
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: maas-htk
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.ucp.maas-htk
      dest:
        path: .source
data:
  chart_name: maas-htk
  release: maas-htk
  namespace: maas-htk
  values: {}
  dependencies: []
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: ucp-divingbell-global
  labels:
    name: ucp-divingbell-global
  layeringDefinition:
    abstract: true
    layer: global
  storagePolicy: cleartext
  substitutions:
    # Chart source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.ucp.divingbell
      dest:
        path: .source
    # Image Source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.ucp.divingbell
      dest:
        path: .values.images

    - src:
        schema: pegleg/Script/v1
        name: rbd-roomba-scanner
        path: .
      dest:
        path: .values.conf.exec.X005-rbd-roomba-scanner.data
    - src:
        schema: pegleg/Script/v1
        name: hanging-cgroup-release
        path: .
      dest:
        path: .values.conf.exec.X005-hanging-cgroup-release.data
data:
  chart_name: ucp-divingbell
  release: ucp-divingbell
  namespace: ucp
  wait:
    timeout: 300
    labels:
      release_group: airship-ucp-divingbell
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
    pre:
      delete:
        - type: job
          labels:
            release_group: airship-ucp-divingbell
  values:
    conf:
      sysctl:
        # Larger connection tracking table
        net.nf_conntrack_max: '1048576'
        # Reboot the node 60 seconds after a kernel panic, instead of default
        # value of 0 (i.e. never reboot)
        kernel.panic: '60'
        # Randomize stack space to prevent buffer overflow exploits
        kernel.randomize_va_space: '2'
        # Accept gratuitous ARP to support failover scenarios
        # https://bugs.launchpad.net/fuel/+bug/1456272
        net.ipv4.conf.default.arp_accept: '1'
        net.ipv4.conf.all.arp_accept: '1'
        # Increased network backlog to optimize performance on fast networks
        net.core.netdev_max_backlog: '261144'
        # Optimizations for RabbitMQ failover
        # https://bugs.launchpad.net/oslo.messaging/+bug/856764/comments/19
        net.ipv4.tcp_keepalive_intvl: '3'
        net.ipv4.tcp_keepalive_time: '30'
        net.ipv4.tcp_keepalive_probes: '8'
        net.ipv4.tcp_retries2: '5'
        # Larger thresholds
        # "Neighbour table overflow" errors that filled kernel logs
        net.ipv4.neigh.default.gc_thresh1: '4096'
        net.ipv4.neigh.default.gc_thresh2: '8192'
        net.ipv4.neigh.default.gc_thresh3: '16384'
        # It was necessary to set rp_filter to zero to support certain
        # multi-homed storage backends
        net.ipv4.conf.default.rp_filter: '0'
        # Enable byte/packet count for new connections to enable creation of
        # rules for the connbytes netfilter module
        net.netfilter.nf_conntrack_acct: '1'
        # Added in response to error messages seen on genesis host when services
        # were restarted. "Failed to add /run/systemd/ask-password to directory
        # watch: No space left on device". https://bit.ly/2Mj5qn2 TDP bug 427616
        fs.inotify.max_user_watches: '1048576'
      exec:
        X005-rbd-roomba-scanner:
          rerun_policy: always
          # 300 = 5 minutes
          rerun_interval: 300
          timeout: 300
        X005-hanging-cgroup-release:
          rerun_policy: always
          # 300 = 5 minutes
          rerun_interval: 3600
          timeout: 600
  dependencies:
    - ucp-divingbell-htk
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: ucp-divingbell-htk
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.ucp.divingbell-htk
      dest:
        path: .source
data:
  chart_name: ucp-divingbell-htk
  release: ucp-divingbell-htk
  namespace: ucp-divingbell-htk
  values: {}
  dependencies: []
...
---
schema: armada/ChartGroup/v1
metadata:
  schema: metadata/Document/v1
  name: ucp-divingbell
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
data:
  description: Divingbell
  chart_group:
    - ucp-divingbell
...
---
schema: armada/ChartGroup/v1
metadata:
  schema: metadata/Document/v1
  name: ucp-shipyard
  labels:
    name: ucp-shipyard-chart-group-global
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
data:
  description: Shipyard
  chart_group:
    - ucp-shipyard
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: ucp-shipyard
  labels:
    name: ucp-shipyard-global
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    # Chart source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.ucp.shipyard
      dest:
        path: .source

    # Images
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.ucp.shipyard
      dest:
        path: .values.images.tags

    # Endpoints
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: ucp_endpoints
        path: .ucp.identity
      dest:
        path: .values.endpoints.identity
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: ucp_endpoints
        path: .ucp.postgresql
      dest:
        path: .values.endpoints.postgresql_shipyard_db
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: ucp_endpoints
        path: .ucp.postgresql
      dest:
        path: .values.endpoints.postgresql_airflow_db
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: ucp_endpoints
        path: .ucp.postgresql_airflow_celery
      dest:
        path: .values.endpoints.postgresql_airflow_celery_db
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: ucp_endpoints
        path: .ucp.shipyard
      dest:
        path: .values.endpoints.shipyard
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: ucp_endpoints
        path: .ucp.airflow_oslo_messaging
      dest:
        path: .values.endpoints.oslo_messaging
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: ucp_endpoints
        path: .ucp.oslo_cache
      dest:
        path: .values.endpoints.oslo_cache

    # Database path
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: ucp_service_accounts
        path: .ucp.shipyard.postgres.database
      dest:
        path: .values.endpoints.postgresql_shipyard_db.path
        pattern: 'DB_NAME'
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: ucp_service_accounts
        path: .ucp.airflow.postgres.database
      dest:
        path: .values.endpoints.postgresql_airflow_db.path
        pattern: 'DB_NAME'
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: ucp_service_accounts
        path: .ucp.airflow.postgres.database
      dest:
        path: .values.endpoints.postgresql_airflow_celery_db.path
        pattern: 'DB_NAME'
    # Credentials
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: ucp_service_accounts
        path: .ucp.postgres.admin
      dest:
        path: .values.endpoints.postgresql_shipyard_db.auth.admin
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: ucp_service_accounts
        path: .ucp.postgres.admin
      dest:
        path: .values.endpoints.postgresql_airflow_db.auth.admin
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: ucp_service_accounts
        path: .ucp.postgres.admin
      dest:
        path: .values.endpoints.postgresql_airflow_celery_db.auth.admin
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: ucp_service_accounts
        path: .ucp.shipyard.postgres
      dest:
        path: .values.endpoints.postgresql_shipyard_db.auth.user
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: ucp_service_accounts
        path: .ucp.airflow.postgres
      dest:
        path: .values.endpoints.postgresql_airflow_db.auth.user
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: ucp_service_accounts
        path: .ucp.airflow.postgres
      dest:
        path: .values.endpoints.postgresql_airflow_celery_db.auth.user
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: ucp_service_accounts
        path: .ucp.airflow.oslo_messaging.user
      dest:
        path: .values.endpoints.oslo_messaging.auth.user
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: ucp_service_accounts
        path: .ucp.airflow.oslo_messaging.admin
      dest:
        path: .values.endpoints.oslo_messaging.auth.admin
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: ucp_service_accounts
        path: .ucp.keystone.admin
      dest:
        path: .values.endpoints.identity.auth.admin
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: ucp_service_accounts
        path: .ucp.shipyard.keystone
      dest:
        path: .values.endpoints.identity.auth.shipyard

    # Secrets
    - dest:
        path: .values.endpoints.identity.auth.admin.password
      src:
        schema: deckhand/Passphrase/v1
        name: ucp_keystone_admin_password
        path: .
    - dest:
        path: .values.endpoints.postgresql_shipyard_db.auth.admin.password
      src:
        schema: deckhand/Passphrase/v1
        name: ucp_postgres_admin_password
        path: .
    - dest:
        path: .values.endpoints.postgresql_airflow_db.auth.admin.password
      src:
        schema: deckhand/Passphrase/v1
        name: ucp_postgres_admin_password
        path: .
    - dest:
        path: .values.endpoints.postgresql_airflow_celery_db.auth.admin.password
      src:
        schema: deckhand/Passphrase/v1
        name: ucp_postgres_admin_password
        path: .
    - dest:
        path: .values.endpoints.identity.auth.shipyard.password
      src:
        schema: deckhand/Passphrase/v1
        name: ucp_shipyard_keystone_password
        path: .
    - dest:
        path: .values.endpoints.postgresql_shipyard_db.auth.user.password
      src:
        schema: deckhand/Passphrase/v1
        name: ucp_shipyard_postgres_password
        path: .
    - dest:
        path: .values.endpoints.postgresql_airflow_db.auth.user.password
      src:
        schema: deckhand/Passphrase/v1
        name: ucp_airflow_postgres_password
        path: .
    - dest:
        path: .values.endpoints.postgresql_airflow_celery_db.auth.user.password
      src:
        schema: deckhand/Passphrase/v1
        name: ucp_airflow_postgres_password
        path: .
    - src:
        schema: deckhand/Passphrase/v1
        name: ucp_airflow_oslo_messaging_password
        path: .
      dest:
        path: .values.endpoints.oslo_messaging.auth.user.password
    - src:
        schema: deckhand/Passphrase/v1
        name: ucp_oslo_messaging_password
        path: .
      dest:
        path: .values.endpoints.oslo_messaging.auth.admin.password

data:
  chart_name: shipyard
  release: ucp-shipyard
  namespace: ucp
  wait:
    timeout: 600
    labels:
      release_group: airship-ucp-shipyard
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
    pre:
      delete:
        - type: job
          labels:
            release_group: airship-ucp-shipyard
  values:
    endpoints:
      postgresql_airflow_db:
        name: postgresql
        hosts:
          default: postgresql
        path: /DB_NAME
        scheme: postgresql+psycopg2
        port:
          postgresql:
            default: 5432
        host_fqdn_override:
          default: null
      postgresql_shipyard_db:
        name: postgresql
        hosts:
          default: postgresql
        path: /DB_NAME
        scheme: postgresql+psycopg2
        port:
          postgresql:
            default: 5432
        host_fqdn_override:
          default: null
    prod_environment: true
    pod:
      replicas:
        shipyard:
          api: 2
        airflow:
          worker: 2
          # TODO: Scheduler can be set to 0 with a future version - here to
          # accommodate the upgrade to the current configuration with the
          # Scheduler being a sidecar with the Worker.
          scheduler: 1
    labels:
      job:
        node_selector_key: ucp-control-plane
        node_selector_value: enabled
    conf:
      shipyard:
        keystone_authtoken:
          memcache_security_strategy: None
  dependencies:
    - shipyard-htk
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: shipyard-htk
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.ucp.shipyard-htk
      dest:
        path: .source
data:
  chart_name: shipyard-htk
  release: shipyard-htk
  namespace: shipyard-htk
  values: {}
  dependencies: []
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: ucp-postgresql
  labels:
    name: ucp-postgresql-global
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    # Chart source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.ucp.postgresql
      dest:
        path: .source

    # Images
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.ucp.postgresql
      dest:
        path: .values.images.tags

    # Endpoints
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: ucp_endpoints
        path: .ucp.postgresql
      dest:
        path: .values.endpoints.postgresql
    # Credentials
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: ucp_service_accounts
        path: .ucp.postgres.admin
      dest:
        path: .values.endpoints.postgresql.auth.admin
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: ucp_service_accounts
        path: .ucp.postgres.replica
      dest:
        path: .values.endpoints.postgresql.auth.replica
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: ucp_service_accounts
        path: .ucp.postgres.exporter
      dest:
        path: .values.endpoints.postgresql.auth.exporter

    # Secrets
    - dest:
        path: .values.endpoints.postgresql.auth.admin.password
      src:
        schema: deckhand/Passphrase/v1
        name: ucp_postgres_admin_password
        path: .
    - dest:
        path: .values.endpoints.postgresql.auth.replica.password
      src:
        schema: deckhand/Passphrase/v1
        name: ucp_postgres_replication_password
        path: .
    - dest:
        path: .values.endpoints.postgresql.auth.exporter.password
      src:
        schema: deckhand/Passphrase/v1
        name: ucp_postgres_exporter_password
        path: .
data:
  chart_name: ucp-postgresql
  release: ucp-postgresql
  namespace: ucp
  wait:
    timeout: 1800
    labels:
      release_group: airship-ucp-postgresql
  install:
    no_hooks: false
  upgrade:
    options:
      force: true
    no_hooks: false
    pre:
      delete:
        - type: job
          labels:
            release_group: airship-ucp-postgresql
      create: []
    post:
      create: []
  values:
    pod:
      replicas:
        server: 3
    conf:
      postgresql:
        max_connections: 1000
        shared_buffers: 2GB
    development:
      enabled: false
    labels:
      server:
        node_selector_key: ucp-control-plane
        node_selector_value: enabled
  dependencies:
    - postgres-htk
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: postgres-htk
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.ucp.postgresql-htk
      dest:
        path: .source
data:
  chart_name: postgres-htk
  release: postgres-htk
  namespace: postgres-htk
  values: {}
  dependencies: []
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: ucp-ingress
  labels:
    name: ucp-ingress-global
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    # Chart source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.ucp.ingress
      dest:
        path: .source
    # Images
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.ucp.ingress
      dest:
        path: .values.images.tags
data:
  chart_name: ingress
  release: ingress
  namespace: ucp
  wait:
    timeout: 300
    labels:
      release_group: airship-ingress
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
    pre:
      delete:
        - type: job
          labels:
            release_group: airship-ingress
  values:
    conf:
      ingress:
        proxy-body-size: 20m
    labels:
      server:
        node_selector_key: ucp-control-plane
        node_selector_value: enabled
      error_server:
        node_selector_key: ucp-control-plane
        node_selector_value: enabled
    pod:
      replicas:
        ingress: 2
        error_page: 2
    network:
      ingress:
        annotations:
          nginx.ingress.kubernetes.io/proxy-body-size: 20m
          nginx.ingress.kubernetes.io/proxy-read-timeout: "600"
  dependencies:
    - ucp-ingress-htk
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: ucp-ingress-htk
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.ucp.ingress-htk
      dest:
        path: .source
data:
  chart_name: ucp-ingress-htk
  release: ucp-ingress-htk
  namespace: ucp-ingress-htk
  values: {}
  dependencies: []
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: ucp-rabbitmq
  labels:
    name: ucp-rabbitmq-global
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    # Chart source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.ucp.rabbitmq
      dest:
        path: .source

    # Images
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.ucp.rabbitmq
      dest:
        path: .values.images.tags

    # Endpoints
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: ucp_endpoints
        path: .ucp.oslo_messaging
      dest:
        path: .values.endpoints.oslo_messaging

    # Credentials
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: ucp_service_accounts
        path: .ucp.oslo_messaging.admin
      dest:
        path: .values.endpoints.oslo_messaging.auth.user

    # Secrets

    - src:
        schema: deckhand/Passphrase/v1
        name: ucp_rabbitmq_erlang_cookie
        path: .
      dest:
        path: .values.endpoints.oslo_messaging.auth.erlang_cookie
    - src:
        schema: deckhand/Passphrase/v1
        name: ucp_oslo_messaging_password
        path: .
      dest:
        path: .values.endpoints.oslo_messaging.auth.user.password
data:
  chart_name: ucp-rabbitmq
  release: ucp-rabbitmq
  namespace: ucp
  wait:
    timeout: 300
    labels:
      release_group: airship-ucp-rabbitmq
    resources:
      - type: statefulset
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
    pre:
      delete:
        - type: job
          labels:
            release_group: airship-ucp-rabbitmq
  values:
    labels:
      server:
        node_selector_key: ucp-control-plane
        node_selector_value: enabled
      prometheus_rabbitmq_exporter:
        node_selector_key: ucp-control-plane
        node_selector_value: enabled
    monitoring:
      prometheus:
        enabled: true
  dependencies:
    - ucp-rabbitmq-htk
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: ucp-rabbitmq-htk
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.ucp.rabbitmq-htk
      dest:
        path: .source
data:
  chart_name: ucp-rabbitmq-htk
  release: ucp-rabbitmq-htk
  namespace: ucp-rabbitmq-htk
  values: {}
  dependencies: []
...
---
schema: armada/ChartGroup/v1
metadata:
  schema: metadata/Document/v1
  name: ucp-core
  labels:
    name: ucp-core-chart-group-global
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
data:
  description: Common Airship Components
  chart_group:
    - ucp-ingress
    - ucp-mariadb
    - ucp-postgresql
    - ucp-rabbitmq
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: ucp-mariadb
  labels:
    name: ucp-mariadb-global
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    # Chart source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.ucp.mariadb
      dest:
        path: .source

    # Images
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.ucp.mariadb
      dest:
        path: .values.images.tags

    # Endpoints
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: ucp_endpoints
        path: .ucp.oslo_db
      dest:
        path: .values.endpoints.oslo_db
    # Accounts
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: ucp_service_accounts
        path: .ucp.oslo_db.admin
      dest:
        path: .values.endpoints.oslo_db.auth.admin

    # Secrets
    - dest:
        path: .values.endpoints.oslo_db.auth.admin.password
      src:
        schema: deckhand/Passphrase/v1
        name: ucp_oslo_db_admin_password
        path: .

data:
  chart_name: ucp-mariadb
  release: ucp-mariadb
  namespace: ucp
  wait:
    timeout: 300
    labels:
      release_group: airship-ucp-mariadb
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
    pre:
      delete:
        - type: job
          labels:
            release_group: airship-ucp-mariadb
  values:
    labels:
      server:
        node_selector_key: ucp-control-plane
        node_selector_value: enabled
      prometheus_mysql_exporter:
        node_selector_key: ucp-control-plane
        node_selector_value: enabled
      ingress:
        node_selector_key: ucp-control-plane
        node_selector_value: enabled
      error_server:
        node_selector_key: ucp-control-plane
        node_selector_value: enabled
  dependencies:
    - mariadb-htk
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: mariadb-htk
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.ucp.mariadb-htk
      dest:
        path: .source
data:
  chart_name: mariadb-htk
  release: mariadb-htk
  namespace: mariadb-htk
  values: {}
  dependencies: []
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: ucp-ceph-client-global
  layeringDefinition:
    abstract: true
    layer: global
  storagePolicy: cleartext
  labels:
    name: ucp-ceph-client-global
  substitutions:
    # Chart source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.ucp.ceph-client
      dest:
        path: .source
    # Images
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.ceph.ceph-client
      dest:
        path: .values.images.tags

    # IP addresses
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .storage.ceph.public_cidr
      dest:
        path: .values.network.public
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .storage.ceph.cluster_cidr
      dest:
        path: .values.network.cluster
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: ucp_endpoints
        path: .ceph.ceph_mon
      dest:
        path: .values.endpoints.ceph_mon
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: ucp_endpoints
        path: .ceph.ceph_mgr
      dest:
        path: .values.endpoints.ceph_mgr

    # Secrets
    - dest:
       path: .values.conf.ceph.global.fsid
      src:
        schema: deckhand/Passphrase/v1
        name: ceph_fsid
        path: .

data:
  chart_name: ucp-ceph-client
  release: ucp-ceph-client
  namespace: ceph
  protected:
    continue_processing: true
  wait:
    timeout: 900
    labels:
      release_group: airship-ucp-ceph-client
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
    pre:
      delete:
        - type: job
          labels:
            release_group: airship-ucp-ceph-client
  values:
    labels:
      job:
        node_selector_key: ucp-control-plane
        node_selector_value: enabled
      mds:
        node_selector_key: ceph-mds
        node_selector_value: enabled
      mgr:
        node_selector_key: ceph-mgr
        node_selector_value: enabled
    endpoints:
      ceph_mon:
        namespace: ceph
    deployment:
      ceph: true
    bootstrap:
      enabled: true
    pod:
      replicas:
        mds: 1
        mgr: 1

    conf:
      pool:

        # NOTE(alanmeadows) spport 4.x 16.04 kernels (non-HWE)
        crush:
          tunables: 'hammer'

        # NOTE(alanmeadows): This is required ATM for bootstrapping a Ceph
        # cluster with only one OSD.  Depending on OSD targeting & site
        # configuration this can be changed.
        target:
          osd: 1
          pg_per_osd: 100
          protected: true

        default:
          # NOTE(alanmeadows): This is required ATM for bootstrapping a Ceph
          # cluster with only one OSD.  Depending on OSD targeting & site
          # configuration this can be changed.
          crush_rule: same_host

      ceph:
        global:
          # NOTE: This is required ATM for bootstrapping a Ceph
          # cluster with only one OSD.  Depending on OSD targeting & site
          # configuration this can be changed.
          osd_pool_default_size: 1

  dependencies:
    - ceph-htk
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: ucp-ceph-mon
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  labels:
    name: ucp-ceph-mon
  substitutions:
    # Chart source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.ucp.ceph-mon
      dest:
        path: .source

    # Images
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.ceph.ceph-mon
      dest:
        path: .values.images.tags

    # IP addresses
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .storage.ceph.public_cidr
      dest:
        path: .values.network.public
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .storage.ceph.cluster_cidr
      dest:
        path: .values.network.cluster
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: ucp_endpoints
        path: .ceph.ceph_mon
      dest:
        path: .values.endpoints.ceph_mon

    # Secrets
    - dest:
       path: .values.conf.ceph.global.fsid
      src:
        schema: deckhand/Passphrase/v1
        name: ceph_fsid
        path: .

data:
  chart_name: ucp-ceph-mon
  release: ucp-ceph-mon
  namespace: ceph
  protected:
    continue_processing: true
  wait:
    timeout: 1800
    labels:
      release_group: airship-ucp-ceph-mon
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
    pre:
      delete:
        - type: job
          labels:
            release_group: airship-ucp-ceph-mon
  values:
    labels:
      job:
        node_selector_key: ucp-control-plane
        node_selector_value: enabled
      mon:
        node_selector_key: ceph-mon
        node_selector_value: enabled
    endpoints:
      ceph_mon:
        namespace: ceph
    deployment:
      ceph: true
      storage_secrets: true
    bootstrap:
      enabled: true
  dependencies:
    - ceph-htk
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: ucp-ceph-client-update-global
  layeringDefinition:
    abstract: true
    layer: global
  storagePolicy: cleartext
  labels:
    name: ucp-ceph-client-update-global
  substitutions:
    # Chart source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.ucp.ceph-client
      dest:
        path: .source
    # Images
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.ceph.ceph-client
      dest:
        path: .values.images.tags

    # IP addresses
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .storage.ceph.public_cidr
      dest:
        path: .values.network.public
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .storage.ceph.cluster_cidr
      dest:
        path: .values.network.cluster
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: ucp_endpoints
        path: .ceph.ceph_mon
      dest:
        path: .values.endpoints.ceph_mon

    # Secrets
    - dest:
       path: .values.conf.ceph.global.fsid
      src:
        schema: deckhand/Passphrase/v1
        name: ceph_fsid
        path: .

data:
  chart_name: ucp-ceph-client
  release: ucp-ceph-client
  namespace: ceph
  protected:
    continue_processing: true
  wait:
    timeout: 900
    labels:
      release_group: airship-ucp-ceph-client
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
    pre:
      delete:
        - type: job
          labels:
            release_group: airship-ucp-ceph-client
  values:
    labels:
      job:
        node_selector_key: ucp-control-plane
        node_selector_value: enabled
      mds:
        node_selector_key: ceph-mds
        node_selector_value: enabled
      mgr:
        node_selector_key: ceph-mgr
        node_selector_value: enabled
    endpoints:
      ceph_mon:
        namespace: ceph
    deployment:
      ceph: true
    bootstrap:
      enabled: true
    pod:
      replicas:
        mds: 2
        mgr: 2

    conf:
      pool:

        # NOTE(alanmeadows) spport 4.x 16.04 kernels (non-HWE)
        crush:
          tunables: 'hammer'

        # NOTE(alanmeadows): This is required ATM for bootstrapping a Ceph
        # cluster with only one OSD.  Depending on OSD targeting & site
        # configuration this can be changed.
        target:
          osd: 1
          pg_per_osd: 100
          protected: true

        default:
          # NOTE(alanmeadows): This is required ATM for bootstrapping a Ceph
          # cluster with only one OSD.  Depending on OSD targeting & site
          # configuration this can be changed.
          crush_rule: replicated_rule

      ceph:
        global:
        # NOTE: This is required ATM for bootstrapping a Ceph
        # cluster with only one OSD.  Depending on OSD targeting & site
        # configuration this can be changed.
          osd_pool_default_size: 1

  dependencies:
    - ceph-htk
...
---
schema: armada/ChartGroup/v1
metadata:
  schema: metadata/Document/v1
  name: ucp-ceph
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
data:
  description: Ceph Storage
  sequenced: true
  chart_group:
    - ucp-ceph-ingress
    - ucp-ceph-mon
    - ucp-ceph-osd
    - ucp-ceph-client
    - ucp-ceph-provisioners
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: ucp-ceph-provisioners
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  labels:
    name: ucp-ceph-provisioners
  substitutions:
    # Chart source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.ucp.ceph-provisioners
      dest:
        path: .source
    # Images
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.ceph.ceph-provisioners
      dest:
        path: .values.images.tags

    # IP addresses
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .storage.ceph.public_cidr
      dest:
        path: .values.network.public
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .storage.ceph.cluster_cidr
      dest:
        path: .values.network.cluster
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: ucp_endpoints
        path: .ceph.ceph_mon
      dest:
        path: .values.endpoints.ceph_mon

    # Secrets
    - dest:
       path: .values.conf.ceph.global.fsid
      src:
        schema: deckhand/Passphrase/v1
        name: ceph_fsid
        path: .

data:
  chart_name: ucp-ceph-provisioners
  release: ucp-ceph-provisioners
  namespace: ceph
  protected:
    continue_processing: true
  wait:
    timeout: 900
    labels:
      release_group: airship-ucp-ceph-provisioners
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
    pre:
      delete:
        - type: job
          labels:
            release_group: airship-ucp-ceph-provisioners
  values:
    labels:
      job:
        node_selector_key: ucp-control-plane
        node_selector_value: enabled
      provisioner:
        node_selector_key: ucp-control-plane
        node_selector_value: enabled
    endpoints:
      ceph_mon:
        namespace: ceph
    deployment:
      ceph: true
      client_secrets: false
      rbd_provisioner: true
      cephfs_provisioner: true
    bootstrap:
      enabled: true
    pod:
      replicas:
        cephfs_provisioner: 2
        rbd_provisioner: 2

    conf:
      ceph:
        global:
          osd_mkfs_type: xfs
    storageclass:
      rbd:
        ceph_configmap_name: ceph-client-keys-etc
        parameters:
          userSecretName: pvc-ceph-client-key
  dependencies:
    - ceph-htk
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: ceph-htk
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.ucp.ceph-htk
      dest:
        path: .source
data:
  chart_name: ceph-htk
  release: ceph-htk
  namespace: ceph-htk
  values: {}
  dependencies: []
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: ucp-ceph-ingress
  layeringDefinition:
    abstract: false
    layer: global
  labels:
    name: ucp-ceph-ingress-global
  storagePolicy: cleartext
  substitutions:
    # Chart source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.ucp.ingress
      dest:
        path: .source
    # Images
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.ucp.ingress
      dest:
        path: .values.images.tags
data:
  chart_name: ucp-ceph-ingress
  release: ucp-ceph-ingress
  namespace: ceph
  wait:
    timeout: 300
    labels:
      release_group: airship-ucp-ceph-ingress
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
    pre:
      delete:
        - type: job
          labels:
            release_group: airship-ucp-ceph-ingress
  values:
    conf:
      ingress:
        proxy-body-size: 20m
    labels:
      server:
        node_selector_key: ucp-control-plane
        node_selector_value: enabled
      error_server:
        node_selector_key: ucp-control-plane
        node_selector_value: enabled
    pod:
      replicas:
        ingress: 2
        error_page: 2
    network:
      ingress:
        annotations:
          nginx.ingress.kubernetes.io/proxy-body-size: 20m
          nginx.ingress.kubernetes.io/proxy-read-timeout: "600"
  dependencies:
    - ucp-ingress-htk
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: ucp-ceph-rgw
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    # Chart source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.ucp.ceph-rgw
      dest:
        path: .source

    # Images
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.ceph.ceph-rgw
      dest:
        path: .values.images.tags

    # IP addresses
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .storage.ceph.public_cidr
      dest:
        path: .values.network.public
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .storage.ceph.cluster_cidr
      dest:
        path: .values.network.cluster

    # Endpoints
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_endpoints
        path: .osh.identity
      dest:
        path: .values.endpoints.identity
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: ucp_endpoints
        path: .ceph.object_store
      dest:
        path: .values.endpoints.object_store
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: ucp_endpoints
        path: .ceph.ceph_mon
      dest:
        path: .values.endpoints.ceph_mon

    # Credentials
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_service_accounts
        path: .osh.keystone.admin
      dest:
        path: .values.endpoints.identity.auth.admin
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_service_accounts
        path: .osh.swift.keystone
      dest:
        path: .values.endpoints.identity.auth.swift

    # Secrets
    - dest:
        path: .values.endpoints.identity.auth.admin.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_keystone_admin_password
        path: .
    - dest:
        path: .values.endpoints.identity.auth.swift.password
      src:
        schema: deckhand/Passphrase/v1
        name: ceph_swift_keystone_password
        path: .

data:
  chart_name: ucp-ceph-rgw
  release: ucp-ceph-rgw
  namespace: ceph
  wait:
    timeout: 900
    labels:
      release_group: airship-ucp-ceph-rgw
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
    pre:
      delete:
        - type: job
          labels:
            release_group: airship-ucp-ceph-rgw
  values:
    labels:
      job:
        node_selector_key: ucp-control-plane
        node_selector_value: enabled
      rgw:
        node_selector_key: ceph-rgw
        node_selector_value: enabled
    endpoints:
      identity:
        namespace: openstack
      object_store:
        namespace: ceph
      ceph_mon:
        namespace: ceph
    deployment:
      ceph: true
      rgw_keystone_user_and_endpoints: false
    bootstrap:
      enabled: false
    pod:
      replicas:
        rgw: 2
    conf:
      rgw_ks:
        enabled: true
      config:
          #NOTE (portdirect): See http://tracker.ceph.com/issues/21226
          rgw_keystone_token_cache_size: '0'
    ceph_client:
      configmap: ceph-client-keys-etc
  dependencies:
    - ceph-htk
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: ucp-ceph-osd-global
  layeringDefinition:
    abstract: true
    layer: global
  storagePolicy: cleartext
  labels:
    name: ucp-ceph-osd-global
  substitutions:
    # Chart source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.ucp.ceph-osd
      dest:
        path: .source

    # Images
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.ceph.ceph-osd
      dest:
        path: .values.images.tags

    # IP addresses
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .storage.ceph.public_cidr
      dest:
        path: .values.network.public
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .storage.ceph.cluster_cidr
      dest:
        path: .values.network.cluster
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: ucp_endpoints
        path: .ceph.ceph_mon
      dest:
        path: .values.endpoints.ceph_mon

    # Secrets
    - dest:
       path: .values.conf.ceph.global.fsid
      src:
        schema: deckhand/Passphrase/v1
        name: ceph_fsid
        path: .

data:
  chart_name: ucp-ceph-osd
  release: ucp-ceph-osd
  namespace: ceph
  protected:
    continue_processing: true
  wait:
    timeout: 900
    labels:
      release_group: airship-ucp-ceph-osd
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
  values:
    labels:
      osd:
        node_selector_key: ceph-osd
        node_selector_value: enabled
    endpoints:
      ceph_mon:
        namespace: ceph
    bootstrap:
      enabled: true
    conf:
      storage:
        osd:
          - data:
              type: directory
              location: /var/lib/openstack-helm/ceph/osd/osd-one
            journal:
              type: directory
              location: /var/lib/openstack-helm/ceph/osd/journal-one
      osd:
        # NOTE(alanmeadows): This is required ATM for bootstrapping a Ceph
        # cluster with only one OSD.  Depending on OSD targeting & site
        # configuration this can be changed.
        osd_crush_chooseleaf_type: 0
  dependencies:
    - ceph-htk
...
---
schema: armada/ChartGroup/v1
metadata:
  schema: metadata/Document/v1
  name: ucp-ceph-update
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
data:
  description: Ceph post-install update
  sequenced: true
  chart_group:
    - ucp-ceph-ingress
    - ucp-ceph-mon
    - ucp-ceph-osd
    - ucp-ceph-client-update
    - ucp-ceph-provisioners
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: osh-helm-toolkit
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.osh.helm_toolkit
      dest:
        path: .source
data:
  chart_name: helm-toolkit
  release: osh-helm-toolkit
  namespace: osh-helm-toolkit
  wait:
    timeout: 600
    labels:
      release_group: airship-osh-helm-toolkit
  upgrade:
    no_hooks: true
  values: {}
  dependencies: []
...
---
schema: armada/ChartGroup/v1
metadata:
  schema: metadata/Document/v1
  name: openstack-horizon
  labels:
    name: openstack-horizon-chart-group-global
    component: horizon
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
data:
  description: Deploy Horizon
  chart_group:
    - horizon
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: horizon
  labels:
    name: horizon-global
    component: horizon
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    # Chart source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.osh.horizon
      dest:
        path: .source

    # Images
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.osh.horizon
      dest:
        path: .values.images.tags

    # Endpoints
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_endpoints
        path: .osh.identity
      dest:
        path: .values.endpoints.identity
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_endpoints
        path: .osh.dashboard
      dest:
        path: .values.endpoints.dashboard
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_endpoints
        path: .osh.oslo_db
      dest:
        path: .values.endpoints.oslo_db
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_endpoints
        path: .osh.oslo_cache
      dest:
        path: .values.endpoints.oslo_cache

    # Service Accounts
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_service_accounts
        path: .osh.horizon.oslo_db
      dest:
        path: .values.endpoints.oslo_db.auth.horizon
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_service_accounts
        path: .osh.horizon.oslo_db.database
      dest:
        path: .values.endpoints.oslo_db.path
        pattern: DB_NAME

    # Secrets
    - dest:
        path: .values.endpoints.identity.auth.admin.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_keystone_admin_password
        path: .
    - dest:
        path: .values.endpoints.oslo_db.auth.keystone.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_horizon_oslo_db_password
        path: .
    - dest:
        path: .values.endpoints.oslo_db.auth.admin.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_oslo_db_admin_password
        path: .
    - dest:
        path: .values.endpoints.oslo_cache.auth.memcache_secret_key
      src:
        schema: deckhand/Passphrase/v1
        name: osh_oslo_cache_secret_key
        path: .
data:
  chart_name: horizon
  release: horizon
  namespace: openstack
  install:
    no_hooks: false
  wait:
    timeout: 900
    labels:
      release_group: airship-horizon
  upgrade:
    no_hooks: false
    pre:
      delete:
        - type: job
          labels:
            release_group: airship-horizon
    post:
      create: []
  values:
    labels:
      node_selector_key: openstack-control-plane
      node_selector_value: enabled
    pod:
      replicas:
        server: 2
  dependencies:
    - horizon-htk
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: horizon-htk
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.osh.horizon-htk
      dest:
        path: .source
data:
  chart_name: horizon-htk
  release: horizon-htk
  namespace: horizon-htk
  values: {}
  dependencies: []
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: keystone
  labels:
    name: keystone-global
    component: keystone
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    # Chart source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.osh.keystone
      dest:
        path: .source

    # Images
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.osh.keystone
      dest:
        path: .values.images.tags

    # Endpoints
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_endpoints
        path: .osh.identity
      dest:
        path: .values.endpoints.identity
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_endpoints
        path: .osh.oslo_db
      dest:
        path: .values.endpoints.oslo_db
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_endpoints
        path: .osh.oslo_messaging
      dest:
        path: .values.endpoints.oslo_messaging
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_endpoints
        path: .osh.oslo_cache
      dest:
        path: .values.endpoints.oslo_cache
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_infra_endpoints
        path: .osh_infra.fluentd
      dest:
        path: .values.endpoints.fluentd

    # Service Accounts
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_service_accounts
        path: .osh.keystone.admin
      dest:
        path: .values.endpoints.identity.auth.admin
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_service_accounts
        path: .osh.oslo_messaging.admin
      dest:
        path: .values.endpoints.oslo_messaging.auth.admin
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_service_accounts
        path: .osh.keystone.oslo_messaging.keystone
      dest:
        path: .values.endpoints.oslo_messaging.auth.keystone
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_service_accounts
        path: .osh.keystone.oslo_db
      dest:
        path: .values.endpoints.oslo_db.auth.keystone
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_service_accounts
        path: .osh.keystone.oslo_db.database
      dest:
        path: .values.endpoints.oslo_db.path
        pattern: DB_NAME

    # Secrets
    - dest:
        path: .values.endpoints.identity.auth.admin.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_keystone_admin_password
        path: .
    - dest:
        path: .values.endpoints.oslo_messaging.auth.admin.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_oslo_messaging_admin_password
        path: .
    - dest:
        path: .values.endpoints.oslo_messaging.auth.keystone.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_oslo_messaging_admin_password
        path: .
    - dest:
        path: .values.endpoints.oslo_db.auth.keystone.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_keystone_oslo_db_password
        path: .
    - dest:
        path: .values.endpoints.oslo_db.auth.admin.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_oslo_db_admin_password
        path: .
    - dest:
        path: .values.endpoints.oslo_cache.auth.memcache_secret_key
      src:
        schema: deckhand/Passphrase/v1
        name: osh_oslo_cache_secret_key
        path: .

data:
  chart_name: keystone
  release: keystone
  namespace: openstack
  wait:
    timeout: 900
    labels:
      release_group: airship-keystone
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
    pre:
      delete:
        - type: job
          labels:
            release_group: airship-keystone
    post:
      create: []
  values:
    dependencies:
      static:
        api:
          jobs:
            - keystone-db-sync
            - keystone-credential-setup
            - keystone-fernet-setup
        db_sync:
          jobs:
            - keystone-db-init
            - keystone-credential-setup
            - keystone-fernet-setup
    bootstrap:
      script: |
        openstack role create --or-show _member_
        openstack role add \
              --user="${OS_USERNAME}" \
              --user-domain="${OS_USER_DOMAIN_NAME}" \
              --project-domain="${OS_PROJECT_DOMAIN_NAME}" \
              --project="${OS_PROJECT_NAME}" \
              "_member_"

        #NOTE(portdirect): required for all users who operate heat stacks
        openstack role create --or-show heat_stack_owner
        openstack role add \
              --user="${OS_USERNAME}" \
              --user-domain="${OS_USER_DOMAIN_NAME}" \
              --project-domain="${OS_PROJECT_DOMAIN_NAME}" \
              --project="${OS_PROJECT_NAME}" \
              "heat_stack_owner"
    conf:
      logging:
        loggers:
          keys:
            - root
            - keystone
        handlers:
          keys:
            - stdout
            - stderr
            - 'null'
            - fluent
        formatters:
          keys:
            - context
            - default
            - fluent
        logger_root:
          level: WARNING
          handlers: stdout
        logger_keystone:
          level: INFO
          handlers:
            - stdout
            - fluent
          qualname: keystone
        logger_eventletwsgi:
          level: WARNING
          handlers: stderr
          qualname: eventlet.wsgi.server
        logger_sqlalchemy:
          level: WARNING
          handlers: stderr
          qualname: sqlalchemy
        logger_boto:
          level: WARNING
          handlers: stderr
          qualname: boto
        handler_null:
          class: logging.NullHandler
          formatter: default
          args: ()
        handler_stdout:
          class: StreamHandler
          args: (sys.stdout,)
          formatter: context
        handler_stderr:
          class: StreamHandler
          args: (sys.stderr,)
          formatter: context
        handler_fluent:
          class: fluent.handler.FluentHandler
          args: ('openstack.keystone', 'fluentd-logging.osh-infra', 24224)
          formatter: fluent
        formatter_fluent:
          class: oslo_log.formatters.FluentFormatter
        formatter_context:
          class: oslo_log.formatters.ContextFormatter
        formatter_default:
          format: "%(message)s"
      keystone:
        DEFAULT:
          transport_url: localhost
        identity:
          driver: sql
          default_domain_id: default
          domain_specific_drivers_enabled: True
          domain_configurations_from_database: True
          domain_config_dir: /etc/keystonedomains
    pod:
      replicas:
        api: 2
    labels:
      api:
        node_selector_key: openstack-control-plane
        node_selector_value: enabled
      job:
        node_selector_key: openstack-control-plane
        node_selector_value: enabled
    manifests:
      job_rabbit_init: false
      secret_rabbitmq: false
  dependencies:
    - keystone-htk
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: keystone-htk
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.osh.keystone-htk
      dest:
        path: .source
data:
  chart_name: keystone-htk
  release: keystone-htk
  namespace: keystone-htk
  values: {}
  dependencies: []
...
---
schema: armada/ChartGroup/v1
metadata:
  schema: metadata/Document/v1
  name: openstack-keystone
  labels:
    name: openstack-keystone-chart-group-global
    component: keystone
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
data:
  description: Deploy Keystone
  chart_group:
    - keystone
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: heat
  labels:
    name: heat-global
    component: heat
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    # Chart source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.osh.heat
      dest:
        path: .source

    # Images
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.osh.heat
      dest:
        path: .values.images.tags

    # Endpoints
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_endpoints
        path: .osh.identity
      dest:
        path: .values.endpoints.identity
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_endpoints
        path: .osh.orchestration
      dest:
        path: .values.endpoints.orchestration
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_endpoints
        path: .osh.cloudformation
      dest:
        path: .values.endpoints.cloudformation
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_endpoints
        path: .osh.cloudwatch
      dest:
        path: .values.endpoints.cloudwatch
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_endpoints
        path: .osh.oslo_db
      dest:
        path: .values.endpoints.oslo_db
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_endpoints
        path: .osh.oslo_messaging
      dest:
        path: .values.endpoints.oslo_messaging
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_endpoints
        path: .osh.oslo_cache
      dest:
        path: .values.endpoints.oslo_cache
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_infra_endpoints
        path: .osh_infra.fluentd
      dest:
        path: .values.endpoints.fluentd
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_service_accounts
        path: .osh.keystone.admin
      dest:
        path: .values.endpoints.identity.auth.admin
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_service_accounts
        path: .osh.heat.heat
      dest:
        path: .values.endpoints.identity.auth.heat
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_service_accounts
        path: .osh.heat.heat_trustee
      dest:
        path: .values.endpoints.identity.auth.heat_trustee
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_service_accounts
        path: .osh.heat.heat_stack_user
      dest:
        path: .values.endpoints.identity.auth.heat_stack_user
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_service_accounts
        path: .osh.oslo_messaging.admin
      dest:
        path: .values.endpoints.oslo_messaging.auth.admin
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_service_accounts
        path: .osh.heat.oslo_messaging.heat
      dest:
        path: .values.endpoints.oslo_messaging.auth.heat
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_service_accounts
        path: .osh.heat.oslo_db
      dest:
        path: .values.endpoints.oslo_db.auth.heat
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_service_accounts
        path: .osh.heat.oslo_db.database
      dest:
        path: .values.endpoints.oslo_db.path
        pattern: DB_NAME
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_endpoints
        path: .osh.orchestration.name
      dest:
        path: .values.endpoints.oslo_messaging.path
        pattern: VHOST_NAME

    # Secrets
    - dest:
        path: .values.endpoints.identity.auth.admin.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_keystone_admin_password
        path: .
    - dest:
        path: .values.endpoints.identity.auth.heat.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_heat_password
        path: .
    - dest:
        path: .values.endpoints.identity.auth.heat_trustee.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_heat_trustee_password
        path: .
    - dest:
        path: .values.endpoints.identity.auth.heat_stack_user.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_heat_stack_user_password
        path: .
    - dest:
        path: .values.endpoints.oslo_messaging.auth.admin.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_oslo_messaging_admin_password
        path: .
    - dest:
        path: .values.endpoints.oslo_messaging.auth.heat.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_oslo_messaging_admin_password
        path: .
    - dest:
        path: .values.endpoints.oslo_db.auth.heat.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_heat_oslo_db_password
        path: .
    - dest:
        path: .values.endpoints.oslo_db.auth.admin.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_oslo_db_admin_password
        path: .
    - dest:
        path: .values.endpoints.oslo_cache.auth.memcache_secret_key
      src:
        schema: deckhand/Passphrase/v1
        name: osh_oslo_cache_secret_key
        path: .
data:
  chart_name: heat
  release: heat
  namespace: openstack
  wait:
    timeout: 900
    labels:
      release_group: airship-heat
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
    pre:
      delete:
        - type: job
          labels:
            release_group: airship-heat
    post:
      create: []
  values:
    pod:
      replicas:
        api: 2
        cfn: 2
        cloudwatch: 2
        engine: 4
    labels:
      api:
        node_selector_key: openstack-control-plane
        node_selector_value: enabled
      cfn:
        node_selector_key: openstack-control-plane
        node_selector_value: enabled
      cloudwatch:
        node_selector_key: openstack-control-plane
        node_selector_value: enabled
      engine:
        node_selector_key: openstack-control-plane
        node_selector_value: enabled
      job:
        node_selector_key: openstack-control-plane
        node_selector_value: enabled
    conf:
      logging:
        loggers:
          keys:
            - root
            - heat
        handlers:
          keys:
            - stdout
            - stderr
            - 'null'
            - fluent
        formatters:
          keys:
            - context
            - default
            - fluent
        logger_root:
          level: WARNING
          handlers: stdout
        logger_heat:
          level: INFO
          handlers:
            - stdout
            - fluent
          qualname: heat
        logger_amqp:
          level: WARNING
          handlers: stderr
          qualname: amqp
        logger_amqplib:
          level: WARNING
          handlers: stderr
          qualname: amqplib
        logger_eventletwsgi:
          level: WARNING
          handlers: stderr
          qualname: eventlet.wsgi.server
        logger_sqlalchemy:
          level: WARNING
          handlers: stderr
          qualname: sqlalchemy
        logger_boto:
          level: WARNING
          handlers: stderr
          qualname: boto
        handler_null:
          class: logging.NullHandler
          formatter: default
          args: ()
        handler_stdout:
          class: StreamHandler
          args: (sys.stdout,)
          formatter: context
        handler_stderr:
          class: StreamHandler
          args: (sys.stderr,)
          formatter: context
        handler_fluent:
          class: fluent.handler.FluentHandler
          args: ('openstack.heat', 'fluentd-logging.osh-infra', 24224)
          formatter: fluent
        formatter_fluent:
          class: oslo_log.formatters.FluentFormatter
        formatter_context:
          class: oslo_log.formatters.ContextFormatter
        formatter_default:
          format: "%(message)s"
  dependencies:
    - heat-htk
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: heat-htk
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.osh.heat-htk
      dest:
        path: .source
data:
  chart_name: heat-htk
  release: heat-htk
  namespace: heat-htk
  values: {}
  dependencies: []
...
---
schema: armada/ChartGroup/v1
metadata:
  schema: metadata/Document/v1
  name: openstack-heat
  labels:
    name: openstack-heat-chart-group-global
    component: heat
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
data:
  description: Deploy Heat
  chart_group:
    - heat
...
---
schema: armada/ChartGroup/v1
metadata:
  schema: metadata/Document/v1
  name: openstack-mariadb
  labels:
    name: openstack-mariadb-chart-group-global
    component: mariadb
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
data:
  description: Deploy MariaDB
  chart_group:
    - openstack-mariadb
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: openstack-mariadb
  labels:
    name: openstack-mariadb-global
    component: mariadb
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    # Chart source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.osh.mariadb
      dest:
        path: .source

    # Images
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.osh.mariadb
      dest:
        path: .values.images.tags

    # Endpoints
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_endpoints
        path: .osh.oslo_db
      dest:
        path: .values.endpoints.oslo_db
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_endpoints
        path: .osh.prometheus_mysql_exporter
      dest:
        path: .values.endpoints.prometheus_mysql_exporter

    # Accounts
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_service_accounts
        path: .osh.oslo_db.admin
      dest:
        path: .values.endpoints.oslo_db.auth.admin
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_service_accounts
        path: .osh.prometheus_mysql_exporter.user
      dest:
        path: .values.endpoints.prometheus_mysql_exporter.auth.user

    # Secrets
    - dest:
        path: .values.endpoints.oslo_db.auth.admin.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_oslo_db_admin_password
        path: .
    - dest:
        path: .values.endpoints.oslo_db.auth.exporter.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_oslo_db_exporter_password
        path: .

data:
  chart_name: openstack-mariadb
  release: openstack-mariadb
  namespace: openstack
  wait:
    timeout: 900
    labels:
      release_group: airship-openstack-mariadb
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
    pre:
      delete:
        - type: job
          labels:
            release_group: airship-openstack-mariadb
  values:
    labels:
      server:
        node_selector_key: openstack-control-plane
        node_selector_value: enabled
      prometheus_mysql_exporter:
        node_selector_key: openstack-control-plane
        node_selector_value: enabled
    monitoring:
      prometheus:
        enabled: true
  dependencies:
    - openstack-mariadb-htk
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: openstack-mariadb-htk
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.osh.mariadb-htk
      dest:
        path: .source
data:
  chart_name: openstack-mariadb-htk
  release: openstack-mariadb-htk
  namespace: openstack-mariadb-htk
  values: {}
  dependencies: []
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: nova-global
  labels:
    name: nova-global
    component: nova
  layeringDefinition:
    abstract: true
    layer: global
  storagePolicy: cleartext
  substitutions:
    # Chart source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.osh.nova
      dest:
        path: .source

    # Images
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.osh.nova
      dest:
        path: .values.images.tags

    # Endpoints
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_endpoints
        path: .osh.identity
      dest:
        path: .values.endpoints.identity
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_endpoints
        path: .osh.oslo_db
      dest:
        path: .values.endpoints.oslo_db
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_endpoints
        path: .osh.oslo_db
      dest:
        path: .values.endpoints.oslo_db_api
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_endpoints
        path: .osh.oslo_db
      dest:
        path: .values.endpoints.oslo_db_cell0
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_endpoints
        path: .osh.oslo_messaging
      dest:
        path: .values.endpoints.oslo_messaging
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_endpoints
        path: .osh.oslo_cache
      dest:
        path: .values.endpoints.oslo_cache
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_endpoints
        path: .osh.image
      dest:
        path: .values.endpoints.image
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_endpoints
        path: .osh.compute
      dest:
        path: .values.endpoints.compute
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_endpoints
        path: .osh.compute_metadata
      dest:
        path: .values.endpoints.compute_metadata
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_endpoints
        path: .osh.compute_novnc_proxy
      dest:
        path: .values.endpoints.compute_novnc_proxy
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_endpoints
        path: .osh.compute_spice_proxy
      dest:
        path: .values.endpoints.compute_spice_proxy
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_endpoints
        path: .osh.placement
      dest:
        path: .values.endpoints.placement
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_endpoints
        path: .osh.network
      dest:
        path: .values.endpoints.network
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_infra_endpoints
        path: .osh_infra.fluentd
      dest:
        path: .values.endpoints.fluentd

    # Service Accounts
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_service_accounts
        path: .osh.keystone.admin
      dest:
        path: .values.endpoints.identity.auth.admin
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_service_accounts
        path: .osh.nova.nova
      dest:
        path: .values.endpoints.identity.auth.nova
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_service_accounts
        path: .osh.neutron.neutron
      dest:
        path: .values.endpoints.identity.auth.neutron
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_service_accounts
        path: .osh.nova.placement
      dest:
        path: .values.endpoints.identity.auth.placement
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_service_accounts
        path: .osh.oslo_messaging.admin
      dest:
        path: .values.endpoints.oslo_messaging.auth.admin
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_service_accounts
        path: .osh.nova.oslo_messaging.nova
      dest:
        path: .values.endpoints.oslo_messaging.auth.nova
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_service_accounts
        path: .osh.nova.oslo_db.username
      dest:
        path: .values.endpoints.oslo_db.auth.nova.username
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_service_accounts
        path: .osh.nova.oslo_db.database
      dest:
        path: .values.endpoints.oslo_db.path
        pattern: DB_NAME
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_service_accounts
        path: .osh.nova.oslo_db_api
      dest:
        path: .values.endpoints.oslo_db_api.auth.nova
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_service_accounts
        path: .osh.nova.oslo_db_api.database
      dest:
        path: .values.endpoints.oslo_db_api.path
        pattern: DB_NAME
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_service_accounts
        path: .osh.nova.oslo_db_cell0
      dest:
        path: .values.endpoints.oslo_db_cell0.auth.nova
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_service_accounts
        path: .osh.nova.oslo_db_cell0.database
      dest:
        path: .values.endpoints.oslo_db_cell0.path
        pattern: DB_NAME
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_endpoints
        path: .osh.compute.name
      dest:
        path: .values.endpoints.oslo_messaging.path
        pattern: VHOST_NAME

    # Secrets
    - dest:
        path: .values.endpoints.identity.auth.admin.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_keystone_admin_password
        path: .
    - dest:
        path: .values.endpoints.identity.auth.nova.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_nova_password
        path: .
    - dest:
        path: .values.endpoints.identity.auth.neutron.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_neutron_password
        path: .
    - dest:
        path: .values.endpoints.identity.auth.placement.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_placement_password
        path: .
    - dest:
        path: .values.endpoints.oslo_messaging.auth.admin.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_oslo_messaging_admin_password
        path: .
    - dest:
        path: .values.endpoints.oslo_messaging.auth.nova.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_oslo_messaging_admin_password
        path: .
    - dest:
        path: .values.endpoints.oslo_db.auth.nova.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_nova_oslo_db_password
        path: .
    - dest:
        path: .values.endpoints.oslo_db_api.auth.nova.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_nova_oslo_db_password
        path: .
    - dest:
        path: .values.endpoints.oslo_db_cell0.auth.nova.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_nova_oslo_db_password
        path: .
    - dest:
        path: .values.endpoints.oslo_db.auth.admin.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_oslo_db_admin_password
        path: .
    - dest:
        path: .values.endpoints.oslo_db_api.auth.admin.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_oslo_db_admin_password
        path: .
    - dest:
        path: .values.endpoints.oslo_db_cell0.auth.admin.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_oslo_db_admin_password
        path: .
    - dest:
        path: .values.endpoints.oslo_cache.auth.memcache_secret_key
      src:
        schema: deckhand/Passphrase/v1
        name: osh_oslo_cache_secret_key
        path: .
    - dest:
        path: .values.conf.nova.neutron.metadata_proxy_shared_secret
      src:
        schema: deckhand/Passphrase/v1
        name: osh_nova_metadata_proxy_shared_secret
        path: .
data:
  chart_name: nova
  release: nova
  namespace: openstack
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
    pre:
      delete:
        - type: job
          labels:
            release_group: airship-nova
    post:
      create: []
  wait:
    timeout: 2700
    labels:
      release_group: airship-nova
  values:
    labels:
      agent:
        compute:
          node_selector_key: openstack-nova-compute
          node_selector_value: enabled
      api_metadata:
        node_selector_key: openstack-control-plane
        node_selector_value: enabled
      conductor:
        node_selector_key: openstack-control-plane
        node_selector_value: enabled
      consoleauth:
        node_selector_key: openstack-control-plane
        node_selector_value: enabled
      job:
        node_selector_key: openstack-control-plane
        node_selector_value: enabled
      novncproxy:
        node_selector_key: openstack-control-plane
        node_selector_value: enabled
      osapi:
        node_selector_key: openstack-control-plane
        node_selector_value: enabled
      placement:
        node_selector_key: openstack-control-plane
        node_selector_value: enabled
      scheduler:
        node_selector_key: openstack-control-plane
        node_selector_value: enabled
      spiceproxy:
        node_selector_key: openstack-control-plane
        node_selector_value: enabled
      test:
        node_selector_key: openstack-control-plane
        node_selector_value: enabled
    pod:
      replicas:
        api_metadata: 4
        placement: 2
        osapi: 4
        conductor: 4
        consoleauth: 2
        scheduler: 2
        novncproxy: 2
    ceph_client:
      configmap: tenant-ceph-etc
      user_secret_name: pvc-tceph-client-key
    conf:
      nova:
        libvirt:
          cpu_mode: 'host-passthrough'
          virt_type: kvm
        database:
          max_pool_size: 30
          max_retries: -1
          max_overflow: 60
      logging:
        loggers:
          keys:
            - root
            - nova
        handlers:
          keys:
            - stdout
            - stderr
            - 'null'
            - fluent
        formatters:
          keys:
            - context
            - default
            - fluent
        logger_root:
          level: WARNING
          handlers: stdout
        logger_nova:
          level: INFO
          handlers:
            - stdout
            - fluent
          qualname: nova
        logger_amqp:
          level: WARNING
          handlers: stderr
          qualname: amqp
        logger_amqplib:
          level: WARNING
          handlers: stderr
          qualname: amqplib
        logger_eventletwsgi:
          level: WARNING
          handlers: stderr
          qualname: eventlet.wsgi.server
        logger_sqlalchemy:
          level: WARNING
          handlers: stderr
          qualname: sqlalchemy
        logger_boto:
          level: WARNING
          handlers: stderr
          qualname: boto
        handler_null:
          class: logging.NullHandler
          formatter: default
          args: ()
        handler_stdout:
          class: StreamHandler
          args: (sys.stdout,)
          formatter: context
        handler_stderr:
          class: StreamHandler
          args: (sys.stderr,)
          formatter: context
        handler_fluent:
          class: fluent.handler.FluentHandler
          args: ('openstack.nova', 'fluentd-logging.osh-infra', 24224)
          formatter: fluent
        formatter_fluent:
          class: oslo_log.formatters.FluentFormatter
        formatter_context:
          class: oslo_log.formatters.ContextFormatter
        formatter_default:
          format: "%(message)s"
  dependencies:
    - nova-htk
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: nova-htk
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.osh.nova-htk
      dest:
        path: .source
data:
  chart_name: nova-htk
  release: nova-htk
  namespace: nova-htk
  values: {}
  dependencies: []
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: neutron-global
  labels:
    name: neutron-global
    component: neutron
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    # Chart source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.osh.neutron
      dest:
        path: .source

    # Images
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.osh.neutron
      dest:
        path: .values.images.tags

    # Endpoints
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_endpoints
        path: .osh.identity
      dest:
        path: .values.endpoints.identity
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_endpoints
        path: .osh.compute
      dest:
        path: .values.endpoints.compute
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_endpoints
        path: .osh.compute_metadata
      dest:
        path: .values.endpoints.image_registry
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_endpoints
        path: .osh.oslo_db
      dest:
        path: .values.endpoints.oslo_db
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_endpoints
        path: .osh.oslo_messaging
      dest:
        path: .values.endpoints.oslo_messaging
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_endpoints
        path: .osh.oslo_cache
      dest:
        path: .values.endpoints.oslo_cache
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_endpoints
        path: .osh.network
      dest:
        path: .values.endpoints.network
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_infra_endpoints
        path: .osh_infra.fluentd
      dest:
        path: .values.endpoints.fluentd
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_service_accounts
        path: .osh.keystone.admin
      dest:
        path: .values.endpoints.identity.auth.admin
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_service_accounts
        path: .osh.neutron.neutron
      dest:
        path: .values.endpoints.identity.auth.neutron
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_service_accounts
        path: .osh.nova.nova
      dest:
        path: .values.endpoints.identity.auth.nova
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_service_accounts
        path: .osh.oslo_messaging.admin
      dest:
        path: .values.endpoints.oslo_messaging.auth.admin
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_service_accounts
        path: .osh.neutron.oslo_messaging.neutron
      dest:
        path: .values.endpoints.oslo_messaging.auth.neutron
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_service_accounts
        path: .osh.neutron.oslo_db
      dest:
        path: .values.endpoints.oslo_db.auth.neutron
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_service_accounts
        path: .osh.neutron.oslo_db.database
      dest:
        path: .values.endpoints.oslo_db.path
        pattern: DB_NAME
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_endpoints
        path: .osh.network.name
      dest:
        path: .values.endpoints.oslo_messaging.path
        pattern: VHOST_NAME

    # Secrets
    - dest:
        path: .values.endpoints.identity.auth.admin.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_keystone_admin_password
        path: .
    - dest:
        path: .values.endpoints.identity.auth.neutron.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_neutron_password
        path: .
    - dest:
        path: .values.endpoints.identity.auth.nova.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_nova_password
        path: .
    - dest:
        path: .values.endpoints.oslo_messaging.auth.admin.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_oslo_messaging_admin_password
        path: .
    - dest:
        path: .values.endpoints.oslo_messaging.auth.neutron.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_oslo_messaging_admin_password
        path: .
    - dest:
        path: .values.endpoints.oslo_db.auth.neutron.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_neutron_oslo_db_password
        path: .
    - dest:
        path: .values.endpoints.oslo_db.auth.admin.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_oslo_db_admin_password
        path: .
    - dest:
        path: .values.endpoints.oslo_cache.auth.memcache_secret_key
      src:
        schema: deckhand/Passphrase/v1
        name: osh_oslo_cache_secret_key
        path: .
    - dest:
        path: .values.conf.metadata_agent.DEFAULT.metadata_proxy_shared_secret
      src:
        schema: deckhand/Passphrase/v1
        name: osh_nova_metadata_proxy_shared_secret
        path: .

    # Interfaces for neutron configuration
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .neutron.tunnel_device
      dest:
        path: .values.network.interface.tunnel
        pattern: 'TUNNEL_DEVICE'
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .neutron.external_iface
      dest:
        path: .values.conf.auto_bridge_add.br-ex
        pattern: 'EXTERNAL_INTERFACE'

data:
  chart_name: neutron
  release: neutron
  namespace: openstack
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
    pre:
      delete:
        - type: job
          labels:
            release_group: airship-neutron
    post:
      create: []
  test:
    enabled: false
    timeout: 500
  wait:
    labels:
      release_group: airship-neutron
  values:
    pod:
      replicas:
        server: 2
    labels:
      agent:
        dhcp:
          node_selector_key: openstack-control-plane
          node_selector_value: enabled
        l3:
          # To enable the forcing of routers onto controllers that have
          # a public cidr so that tenant floating IPs can route properly
          node_selector_key: openstack-l3-agent
          node_selector_value: enabled
        metadata:
          node_selector_key: openstack-control-plane
          node_selector_value: enabled
      job:
        node_selector_key: openstack-control-plane
        node_selector_value: enabled
      lb:
        node_selector_key: linuxbridge
        node_selector_value: enabled
      ovs:
        node_selector_key: openvswitch
        node_selector_value: enabled
      server:
        node_selector_key: openstack-control-plane
        node_selector_value: enabled
      test:
        node_selector_key: openstack-control-plane
        node_selector_value: enabled
    network:
      interface:
        tunnel: 'TUNNEL_DEVICE'
    conf:
      ovs_dpdk:
        enabled: true
        driver: uio_pci_generic
        nics:
          - name: dpdk0
            pci_id: '0000:00:05.0'
            bridge: br-phy
        bridges:
          - name: br-phy
        bonds: []
      auto_bridge_add:
        br-ex: 'EXTERNAL_INTERFACE'
      logging:
        loggers:
          keys:
            - root
            - neutron
        handlers:
          keys:
            - stdout
            - stderr
            - 'null'
            - fluent
        formatters:
          keys:
            - context
            - default
            - fluent
        logger_root:
          level: WARNING
          handlers: stdout
        logger_neutron:
          level: INFO
          handlers:
            - stdout
            - fluent
          qualname: neutron
        logger_amqp:
          level: WARNING
          handlers: stderr
          qualname: amqp
        logger_amqplib:
          level: WARNING
          handlers: stderr
          qualname: amqplib
        logger_eventletwsgi:
          level: WARNING
          handlers: stderr
          qualname: eventlet.wsgi.server
        logger_sqlalchemy:
          level: WARNING
          handlers: stderr
          qualname: sqlalchemy
        logger_boto:
          level: WARNING
          handlers: stderr
          qualname: boto
        handler_null:
          class: logging.NullHandler
          formatter: default
          args: ()
        handler_stdout:
          class: StreamHandler
          args: (sys.stdout,)
          formatter: context
        handler_stderr:
          class: StreamHandler
          args: (sys.stderr,)
          formatter: context
        handler_fluent:
          class: fluent.handler.FluentHandler
          args: ('openstack.neutron', 'fluentd-logging.osh-infra', 24224)
          formatter: fluent
        formatter_fluent:
          class: oslo_log.formatters.FluentFormatter
        formatter_context:
          class: oslo_log.formatters.ContextFormatter
        formatter_default:
          format: "%(message)s"
      neutron:
        DEFAULT:
          l3_ha: True
          max_l3_agents_per_router: 3
          l3_ha_network_type: vxlan
          dhcp_agents_per_network: 2
        oslo_messaging_rabbit:
          heartbeat_timeout_threshold: 0
      plugins:
        openvswitch_agent:
          ovs:
            bridge_mappings: "external:br-ex,phynet:br-phy"
        ml2_conf:
          ml2:
            extension_drivers: port_security
            mechanism_drivers: l2population,openvswitch
            type_drivers: vlan,flat,vxlan
            tenant_network_types: vxlan
          ml2_type_vlan:
            network_vlan_ranges: external
  dependencies:
    - neutron-htk
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: neutron-htk
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.osh.neutron-htk
      dest:
        path: .source
data:
  chart_name: neutron-htk
  release: neutron-htk
  namespace: neutron-htk
  values: {}
  dependencies: []
...
---
schema: armada/ChartGroup/v1
metadata:
  schema: metadata/Document/v1
  name: openstack-compute-kit
  labels:
    name: openstack-compute-kit-chart-group-global
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
data:
  description: Deploy Nova, Neutron, Openvswitch, and Libvirt
  chart_group:
    - libvirt
    - openvswitch
    - neutron
    - nova
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: openvswitch
  layeringDefinition:
    abstract: false
    layer: global
  labels:
    name: openvswitch-global
  storagePolicy: cleartext
  substitutions:
    # Chart source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.osh.openvswitch
      dest:
        path: .source
    # Images
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.osh.openvswitch
      dest:
        path: .values.images.tags
data:
  chart_name: openvswitch
  release: openvswitch
  namespace: openstack
  wait:
    timeout: 900
    labels:
      release_group: airship-openvswitch
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
    pre:
      delete:
        - type: job
          labels:
            release_group: airship-openvswitch
  values:
    labels:
      ovs:
        node_selector_key: openvswitch
        node_selector_value: enabled
    pod:
      resources:
        enabled: true
        ovs:
          vswitchd:
            requests:
              memory: "500Mi"
              cpu: "2"
            limits:
              memory: "1Gi"
              cpu: "2"
              hugepages-2Mi: "1Gi"
    conf:
      ovs_dpdk:
        enabled: true
        hugepages_mountpath: /dev/hugepages
        vhostuser_socket_dir: vhostuser
        socket_memory: 1024
  dependencies:
    - openvswitch-htk
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: openvswitch-htk
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.osh.openvswitch-htk
      dest:
        path: .source
data:
  chart_name: openvswitch-htk
  release: openvswitch-htk
  namespace: openvswitch-htk
  values: {}
  dependencies: []
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: libvirt
  labels:
    name: libvirt-global
    component: libvirt
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    # Chart source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.osh.libvirt
      dest:
        path: .source
    # Images
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.osh.libvirt
      dest:
        path: .values.images.tags
data:
  chart_name: libvirt
  release: libvirt
  namespace: openstack
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
    pre:
      delete:
        - type: job
          labels:
            release_group: airship-libvirt
  values:
    labels:
      agent:
        libvirt:
          node_selector_key: openstack-libvirt
          node_selector_value: kernel
    ceph_client:
      configmap: tenant-ceph-etc
      user_secret_name: pvc-tceph-client-key
  dependencies:
    - libvirt-htk
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: libvirt-htk
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.osh.libvirt-htk
      dest:
        path: .source
data:
  chart_name: libvirt-htk
  release: libvirt-htk
  namespace: libvirt-htk
  values: {}
  dependencies: []
...
---
schema: armada/ChartGroup/v1
metadata:
  schema: metadata/Document/v1
  name: openstack-ceph-config
  labels:
    name: openstack-ceph-config-chart-group-global
    component: ceph
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
data:
  description: Ceph config for OpenStack namespace(s)
  chart_group:
    - openstack-ceph-config
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: openstack-ceph-config
  labels:
    name: openstack-ceph-config-global
    component: ceph
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    # Chart source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.ucp.ceph-provisioners
      dest:
        path: .source

    # Images
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.ceph.ceph-provisioners
      dest:
        path: .values.images.tags

    # IP addresses
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .storage.ceph.public_cidr
      dest:
        path: .values.network.public
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .storage.ceph.cluster_cidr
      dest:
        path: .values.network.cluster

    # Endpoints
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: ucp_endpoints
        path: .ceph.ceph_mon
      dest:
        path: .values.endpoints.ceph_mon

data:
  chart_name: openstack-ceph-config
  release: openstack-ceph-config
  namespace: openstack
  wait:
    timeout: 900
    labels:
      release_group: airship-openstack-ceph-config
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
    pre:
      delete:
        - type: job
          labels:
            release_group: airship-openstack-ceph-config
  values:
    labels:
      job:
        node_selector_key: openstack-control-plane
        node_selector_value: enabled
      provisioner:
        node_selector_key: openstack-control-plane
        node_selector_value: enabled
    deployment:
      ceph: false
      client_secrets: true
      rbd_provisioner: false
      cephfs_provisioner: false
      rgw_keystone_user_and_endpoints: false
    bootstrap:
      enabled: false
    storageclass:
      rbd:
        ceph_configmap_name: ceph-etc
        parameters:
          userSecretName: pvc-ceph-client-key
      cephfs:
        provision_storage_class: false
  dependencies:
    - ceph-htk
...
---
schema: armada/ChartGroup/v1
metadata:
  schema: metadata/Document/v1
  name: openstack-tempest
  labels:
    name: openstack-tempest-chart-group-global
    component: tempest
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
data:
  description: Deploy Tempest
  chart_group:
    - tempest
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: tempest
  labels:
    name: tempest-global
    component: tempest
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    # Chart source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.osh.tempest
      dest:
        path: .source

    # Images
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.osh.tempest
      dest:
        path: .values.images.tags

    # Endpoints
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_endpoints
        path: .osh.identity
      dest:
        path: .values.endpoints.identity

    # Service Accounts
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_service_accounts
        path: .osh.keystone.admin
      dest:
        path: .values.endpoints.identity.auth.admin
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_service_accounts
        path: .osh.tempest.tempest
      dest:
        path: .values.endpoints.identity.auth.tempest

    # Secrets
    - dest:
        path: .values.endpoints.identity.auth.admin.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_keystone_admin_password
        path: .
    - dest:
        path: .values.endpoints.identity.auth.tempest.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_tempest_password
        path: .
data:
  chart_name: tempest
  release: tempest
  namespace: openstack
  wait:
    timeout: 900
    labels:
      release_group: airship-tempest
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
    pre:
      delete:
        - type: job
          labels:
            release_group: airship-tempest
    post:
      create: []
  values:
    labels:
      job:
        node_selector_key: openstack-control-plane
        node_selector_value: enabled
    ceph_client:
      configmap: tenant-ceph-etc
      user_secret_name: pvc-tceph-client-key
  dependencies:
    - tempest-htk
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: tempest-htk
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.osh.tempest-htk
      dest:
        path: .source
data:
  chart_name: tempest-htk
  release: tempest-htk
  namespace: tempest-htk
  values: {}
  dependencies: []
...
---
schema: armada/ChartGroup/v1
metadata:
  schema: metadata/Document/v1
  name: openstack-memcached
  labels:
    name: openstack-memcached-chart-group-global
    component: memcached
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
data:
  description: Deploy Memcached
  chart_group:
    - openstack-memcached
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: openstack-memcached
  labels:
    name: openstack-memcached-global
    component: memcached
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    # Chart source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.osh.memcached
      dest:
        path: .source

    # Images
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.osh.memcached
      dest:
        path: .values.images.tags

    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_endpoints
        path: .osh.oslo_cache
      dest:
        path: .values.endpoints.oslo_cache
data:
  chart_name: openstack-memcached
  release: openstack-memcached
  namespace: openstack
  wait:
    timeout: 900
    labels:
      release_group: airship-openstack-memcached
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
    pre:
      delete:
        - type: job
          labels:
            release_group: airship-openstack-memcached
  values:
    labels:
      server:
        node_selector_key: openstack-control-plane
        node_selector_value: enabled
  dependencies:
    - memcached-htk
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: memcached-htk
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.osh.memcached-htk
      dest:
        path: .source
data:
  chart_name: memcached-htk
  release: memcached-htk
  namespace: memcached-htk
  values: {}
  dependencies: []
...
---
schema: armada/ChartGroup/v1
metadata:
  schema: metadata/Document/v1
  name: openstack-cinder
  labels:
    name: openstack-cinder-chart-group-global
    component: cinder
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
data:
  description: Deploy Cinder
  chart_group:
    - cinder
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: cinder
  labels:
    name: cinder-global
    component: cinder
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    # Chart source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.osh.cinder
      dest:
        path: .source

    # Images
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.osh.cinder
      dest:
        path: .values.images.tags

    # Endpoints
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_endpoints
        path: .osh.identity
      dest:
        path: .values.endpoints.identity
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_endpoints
        path: .osh.image
      dest:
        path: .values.endpoints.image
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_endpoints
        path: .osh.image_registry
      dest:
        path: .values.endpoints.image_registry
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_endpoints
        path: .osh.volume
      dest:
        path: .values.endpoints.volume
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_endpoints
        path: .osh.volumev2
      dest:
        path: .values.endpoints.volumev2
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_endpoints
        path: .osh.volumev3
      dest:
        path: .values.endpoints.volumev3
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_endpoints
        path: .osh.oslo_db
      dest:
        path: .values.endpoints.oslo_db
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_endpoints
        path: .osh.oslo_messaging
      dest:
        path: .values.endpoints.oslo_messaging
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_endpoints
        path: .osh.oslo_cache
      dest:
        path: .values.endpoints.oslo_cache
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_infra_endpoints
        path: .osh_infra.fluentd
      dest:
        path: .values.endpoints.fluentd
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_service_accounts
        path: .osh.keystone.admin
      dest:
        path: .values.endpoints.identity.auth.admin
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_service_accounts
        path: .osh.cinder.cinder
      dest:
        path: .values.endpoints.identity.auth.cinder
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_service_accounts
        path: .osh.oslo_messaging.admin
      dest:
        path: .values.endpoints.oslo_messaging.auth.admin
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_service_accounts
        path: .osh.cinder.oslo_messaging.cinder
      dest:
        path: .values.endpoints.oslo_messaging.auth.cinder
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_service_accounts
        path: .osh.cinder.oslo_db
      dest:
        path: .values.endpoints.oslo_db.auth.cinder
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_service_accounts
        path: .osh.cinder.oslo_db.database
      dest:
        path: .values.endpoints.oslo_db.path
        pattern: DB_NAME
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_endpoints
        path: .osh.volume.name
      dest:
        path: .values.endpoints.oslo_messaging.path
        pattern: VHOST_NAME

    # Secrets
    - dest:
        path: .values.endpoints.identity.auth.admin.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_keystone_admin_password
        path: .
    - dest:
        path: .values.endpoints.identity.auth.cinder.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_cinder_password
        path: .
    - dest:
        path: .values.endpoints.oslo_messaging.auth.admin.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_oslo_messaging_admin_password
        path: .
    - dest:
        path: .values.endpoints.oslo_messaging.auth.cinder.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_oslo_messaging_admin_password
        path: .
    - dest:
        path: .values.endpoints.oslo_db.auth.cinder.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_cinder_oslo_db_password
        path: .
    - dest:
        path: .values.endpoints.oslo_db.auth.admin.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_oslo_db_admin_password
        path: .
    - dest:
        path: .values.endpoints.oslo_cache.auth.memcache_secret_key
      src:
        schema: deckhand/Passphrase/v1
        name: osh_oslo_cache_secret_key
        path: .
data:
  chart_name: cinder
  release: cinder
  namespace: openstack
  wait:
    timeout: 900
    labels:
      release_group: airship-cinder
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
    pre:
      delete:
        - type: job
          labels:
            release_group: airship-cinder
    post:
      create: []
  values:
    pod:
      replicas:
        api: 2
        volume: 2
        scheduler: 2
        backup: 2
    labels:
      api:
        node_selector_key: openstack-control-plane
        node_selector_value: enabled
      backup:
        node_selector_key: openstack-control-plane
        node_selector_value: enabled
      job:
        node_selector_key: openstack-control-plane
        node_selector_value: enabled
      scheduler:
        node_selector_key: openstack-control-plane
        node_selector_value: enabled
      test:
        node_selector_key: openstack-control-plane
        node_selector_value: enabled
      volume:
        node_selector_key: openstack-control-plane
        node_selector_value: enabled
    ceph_client:
      configmap: tenant-ceph-etc
      user_secret_name: pvc-tceph-client-key
    conf:
      logging:
        loggers:
          keys:
            - root
            - cinder
        handlers:
          keys:
            - stdout
            - stderr
            - 'null'
            - fluent
        formatters:
          keys:
            - context
            - default
            - fluent
        logger_root:
          level: WARNING
          handlers: stdout
        logger_cinder:
          level: INFO
          handlers:
            - stdout
            - fluent
          qualname: cinder
        logger_amqp:
          level: WARNING
          handlers: stderr
          qualname: amqp
        logger_amqplib:
          level: WARNING
          handlers: stderr
          qualname: amqplib
        logger_eventletwsgi:
          level: WARNING
          handlers: stderr
          qualname: eventlet.wsgi.server
        logger_sqlalchemy:
          level: WARNING
          handlers: stderr
          qualname: sqlalchemy
        logger_boto:
          level: WARNING
          handlers: stderr
          qualname: boto
        handler_null:
          class: logging.NullHandler
          formatter: default
          args: ()
        handler_stdout:
          class: StreamHandler
          args: (sys.stdout,)
          formatter: context
        handler_stderr:
          class: StreamHandler
          args: (sys.stderr,)
          formatter: context
        handler_fluent:
          class: fluent.handler.FluentHandler
          args: ('openstack.cinder', 'fluentd-logging.osh-infra', 24224)
          formatter: fluent
        formatter_fluent:
          class: oslo_log.formatters.FluentFormatter
        formatter_context:
          class: oslo_log.formatters.ContextFormatter
        formatter_default:
          format: "%(message)s"
  dependencies:
    - cinder-htk
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: cinder-htk
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.osh.cinder-htk
      dest:
        path: .source
data:
  chart_name: cinder-htk
  release: cinder-htk
  namespace: cinder-htk
  values: {}
  dependencies: []
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: tenant-ceph-client-global
  layeringDefinition:
    abstract: true
    layer: global
  storagePolicy: cleartext
  labels:
    name: tenant-ceph-client-global
  substitutions:
    # Chart source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.ucp.tenant-ceph-client
      dest:
        path: .source
    # Images
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.ceph.tenant-ceph-client
      dest:
        path: .values.images.tags

    # IP addresses
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .storage.ceph.public_cidr
      dest:
        path: .values.network.public
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .storage.ceph.cluster_cidr
      dest:
        path: .values.network.cluster
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: ucp_endpoints
        path: .ceph.tenant_ceph_mon
      dest:
        path: .values.endpoints.ceph_mon
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: ucp_endpoints
        path: .ceph.tenant_ceph_mgr
      dest:
        path: .values.endpoints.ceph_mgr

    # Secrets
    - dest:
       path: .values.conf.ceph.global.fsid
      src:
        schema: deckhand/Passphrase/v1
        name: tenant_ceph_fsid
        path: .

data:
  chart_name: tenant-ceph-client
  release: tenant-ceph-client
  namespace: tenant-ceph
  protected:
    continue_processing: true
  wait:
    timeout: 900
    labels:
      release_group: airship-tenant-ceph-client
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
    pre:
      delete:
        - type: job
          labels:
            release_group: airship-tenant-ceph-client
  values:
    labels:
      job:
        node_selector_key: tenant-ceph-control-plane
        node_selector_value: enabled
      mds:
        node_selector_key: tenant-ceph-mds
        node_selector_value: enabled
      mgr:
        node_selector_key: tenant-ceph-mgr
        node_selector_value: enabled
    endpoints:
      ceph_mon:
        namespace: tenant-ceph
      ceph_mgr:
        namespace: tenant-ceph
    monitoring:
      prometheus:
        ceph_mgr:
          port: 9284
    ceph_mgr_modules_config:
      prometheus:
        server_port: 9284
    deployment:
      ceph: true
    bootstrap:
      enabled: true
    manifests:
      deployment_mds: false
    conf:
      features:
        mds: false
      pool:
        spec:
          # RBD pool
          - name: rbd
            application: rbd
            replication: 3
            percent_total_data: 10
          # Cinder volumes  pool
          - name: cinder.volumes
            application: cinder-volume
            replication: 3
            percent_total_data: 40
          # RadosGW pools
          - name: .rgw.root
            application: rgw
            replication: 3
            percent_total_data: 0.1
          - name: default.rgw.control
            application: rgw
            replication: 3
            percent_total_data: 0.1
          - name: default.rgw.data.root
            application: rgw
            replication: 3
            percent_total_data: 0.1
          - name: default.rgw.gc
            application: rgw
            replication: 3
            percent_total_data: 0.1
          - name: default.rgw.log
            application: rgw
            replication: 3
            percent_total_data: 0.1
          - name: default.rgw.intent-log
            application: rgw
            replication: 3
            percent_total_data: 0.1
          - name: default.rgw.meta
            application: rgw
            replication: 3
            percent_total_data: 0.1
          - name: default.rgw.usage
            application: rgw
            replication: 3
            percent_total_data: 0.1
          - name: default.rgw.users.keys
            application: rgw
            replication: 3
            percent_total_data: 0.1
          - name: default.rgw.users.email
            application: rgw
            replication: 3
            percent_total_data: 0.1
          - name: default.rgw.users.swift
            application: rgw
            replication: 3
            percent_total_data: 0.1
          - name: default.rgw.users.uid
            application: rgw
            replication: 3
            percent_total_data: 0.1
          - name: default.rgw.buckets.extra
            application: rgw
            replication: 3
            percent_total_data: 0.1
          - name: default.rgw.buckets.index
            application: rgw
            replication: 3
            percent_total_data: 3
          - name: default.rgw.buckets.data
            application: rgw
            replication: 3
            percent_total_data: 30
        # NOTE(alanmeadowS) spport 4.x 16.04 kernels (non-HWE)
        crush:
          tunables: 'hammer'

        # NOTE(alanmeadows): This is required ATM for bootstrapping a Ceph
        # cluster with only one OSD.  Depending on OSD targeting & site
        # configuration this can be changed.
        target:
          osd: 1
          pg_per_osd: 100
          protected: true

        default:
          # NOTE(alanmeadows): This is required ATM for bootstrapping a Ceph
          # cluster with only one OSD.  Depending on OSD targeting & site
          # configuration this can be changed.
          crush_rule: replicated_rule

      ceph:
        global:
        # NOTE(mb874d): This is required ATM for bootstrapping a Ceph
        # cluster with only one OSD.  Depending on OSD targeting & site
        # configuration this can be changed.
          osd_pool_default_size: 1

  dependencies:
    - tenant-ceph-htk
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: tenant-ceph-mon
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  labels:
    name: tenant-ceph-mon
  substitutions:
    # Chart source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.ucp.tenant-ceph-mon
      dest:
        path: .source

    # Images
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.ceph.tenant-ceph-mon
      dest:
        path: .values.images.tags

    # IP addresses
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .storage.ceph.public_cidr
      dest:
        path: .values.network.public
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .storage.ceph.cluster_cidr
      dest:
        path: .values.network.cluster
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: ucp_endpoints
        path: .ceph.tenant_ceph_mon
      dest:
        path: .values.endpoints.ceph_mon

    # Secrets
    - dest:
       path: .values.conf.ceph.global.fsid
      src:
        schema: deckhand/Passphrase/v1
        name: tenant_ceph_fsid
        path: .

data:
  chart_name: tenant-ceph-mon
  release: tenant-ceph-mon
  namespace: tenant-ceph
  protected:
    continue_processing: true
  wait:
    timeout: 1800
    labels:
      release_group: airship-tenant-ceph-mon
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
    pre:
      delete:
        - type: job
          labels:
            release_group: airship-tenant-ceph-mon
  values:
    labels:
      job:
        node_selector_key: tenant-ceph-control-plane
        node_selector_value: enabled
      mon:
        node_selector_key: tenant-ceph-mon
        node_selector_value: enabled
    endpoints:
      ceph_mon:
        namespace: tenant-ceph
      ceph_mgr:
        namespace: tenant-ceph
      fluentd:
        namespace: osh-infra
    monitoring:
      ceph_mgr:
        port: 9284
    conf:
      storage:
        mon:
          directory: /var/lib/openstack-helm/tenant-ceph/mon
          log_directory: /var/log/tenant-ceph
    storageclass:
      rbd:
        parameters:
          adminSecretNamespace: tenant-ceph
    deployment:
      ceph: true
      storage_secrets: true
    bootstrap:
      enabled: true
  dependencies:
    - tenant-ceph-htk
...
---
schema: armada/ChartGroup/v1
metadata:
  schema: metadata/Document/v1
  name: openstack-tenant-ceph
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
data:
  description: Ceph Storage
  sequenced: true
  chart_group:
    - tenant-ceph-ingress
    - tenant-ceph-mon
    - tenant-ceph-osd
    - tenant-ceph-client
    - tenant-ceph-config
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: tenant-ceph-htk
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.ucp.tenant-ceph-htk
      dest:
        path: .source
data:
  chart_name: tenant-ceph-htk
  release: tenant-ceph-htk
  namespace: tenant-ceph-htk
  values: {}
  dependencies: []
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: tenant-ceph-ingress
  labels:
    name: tenant-ceph-ingress-global
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    # Chart source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.ucp.ingress
      dest:
        path: .source
    # Images
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.ucp.ingress
      dest:
        path: .values.images.tags
data:
  chart_name: tenant-ceph-ingress
  release: tenant-ceph-ingress
  namespace: tenant-ceph
  wait:
    timeout: 300
    labels:
      release_group: airship-tenant-ceph-ingress
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
    pre:
      delete:
        - type: job
          labels:
            release_group: airship-tenant-ceph-ingress
  values:
    conf:
      ingress:
        proxy-body-size: 20m
    labels:
      server:
        node_selector_key: tenant-ceph-control-plane
        node_selector_value: enabled
      error_server:
        node_selector_key: tenant-ceph-control-plane
        node_selector_value: enabled
    pod:
      replicas:
        ingress: 2
        error_page: 2
    network:
      ingress:
        annotations:
          nginx.ingress.kubernetes.io/proxy-body-size: 20m
          nginx.ingress.kubernetes.io/proxy-read-timeout: "600"
  dependencies:
    - ucp-ingress-htk
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: tenant-ceph-osd-global
  layeringDefinition:
    abstract: true
    layer: global
  storagePolicy: cleartext
  labels:
    name: tenant-ceph-osd-global
  substitutions:
    # Chart source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.ucp.tenant-ceph-osd
      dest:
        path: .source

    # Images
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.ceph.tenant-ceph-osd
      dest:
        path: .values.images.tags

    # IP addresses
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .storage.ceph.public_cidr
      dest:
        path: .values.network.public
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .storage.ceph.cluster_cidr
      dest:
        path: .values.network.cluster
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: ucp_endpoints
        path: .ceph.tenant_ceph_mon
      dest:
        path: .values.endpoints.ceph_mon

    # Secrets
    - dest:
       path: .values.conf.ceph.global.fsid
      src:
        schema: deckhand/Passphrase/v1
        name: tenant_ceph_fsid
        path: .

data:
  chart_name: tenant-ceph-osd
  release: tenant-ceph-osd
  namespace: tenant-ceph
  protected:
    continue_processing: true
  wait:
    timeout: 900
    labels:
      release_group: airship-tenant-ceph-osd
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
  values:
    labels:
      osd:
        node_selector_key: tenant-ceph-osd
        node_selector_value: enabled
    endpoints:
      ceph_mon:
        namespace: tenant-ceph
      ceph_mgr:
        namespace: tenant-ceph
      fluentd:
        namespace: osh-infra
    monitoring:
      ceph_mgr:
        port: 9284
    bootstrap:
      enabled: true
    conf:
      storage:
        osd_log_directory: /var/log/tenant-ceph
        mon:
          directory: /var/lib/openstack-helm/tenant-ceph/mon
        osd:
          - data:
              type: directory
              location: /var/lib/openstack-helm/tenant-ceph/osd/osd-one
            journal:
              type: directory
              location: /var/lib/openstack-helm/tenant-ceph/osd/journal-one
      osd:
        # NOTE(alanmeadows): This is required ATM for bootstrapping a Ceph
        # cluster with only one OSD.  Depending on OSD targeting & site
        # configuration this can be changed.
        osd_crush_chooseleaf_type: 0
      ceph:
        osd:
          osd_op_num_threads_per_hdd: 2
          osd_op_num_threads_per_ssd: 4
  dependencies:
    - tenant-ceph-htk
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: tenant-ceph-config
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    # Chart source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.ucp.tenant-ceph-provisioners
      dest:
        path: .source

    # Images
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.ceph.tenant-ceph-provisioners
      dest:
        path: .values.images.tags

    # IP addresses
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .storage.ceph.public_cidr
      dest:
        path: .values.network.public
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .storage.ceph.cluster_cidr
      dest:
        path: .values.network.cluster

    # Endpoints
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: ucp_endpoints
        path: .ceph.tenant_ceph_mon
      dest:
        path: .values.endpoints.ceph_mon

data:
  chart_name: tenant-ceph-config
  release: tenant-ceph-config
  namespace: openstack
  wait:
    timeout: 900
    labels:
      release_group: airship-tenant-ceph-config
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
    pre:
      delete:
        - type: job
          labels:
            release_group: airship-tenant-ceph-config
  values:
    labels:
      job:
        node_selector_key: tenant-ceph-control-plane
        node_selector_value: enabled
      mds:
        node_selector_key: tenant-ceph-mds
        node_selector_value: enabled
      mgr:
        node_selector_key: tenant-ceph-mgr
        node_selector_value: enabled
    deployment:
      ceph: false
      client_secrets: true
      rbd_provisioner: false
      cephfs_provisioner: false
      rgw_keystone_user_and_endpoints: false
    bootstrap:
      enabled: false
    storageclass:
      rbd:
        ceph_configmap_name: tenant-ceph-etc
        parameters:
          userSecretName: pvc-tceph-client-key
          adminSecretNamespace: tenant-ceph
      cephfs:
        provision_storage_class: false
    manifests:
      helm_tests: false
  dependencies:
    - tenant-ceph-htk
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: openstack-ingress-controller
  labels:
    name: openstack-ingress-controller-global
    component: ingress
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    # Chart source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.osh.ingress
      dest:
        path: .source
    # Images
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.osh.ingress
      dest:
        path: .values.images.tags
data:
  chart_name: openstack-ingress-controller
  release: openstack-ingress-controller
  namespace: openstack
  wait:
    timeout: 900
    labels:
      release_group: airship-openstack-ingress-controller
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
    pre:
      delete:
        - type: job
          labels:
            release_group: airship-openstack-ingress-controller
  values:
    labels:
      server:
        node_selector_key: openstack-control-plane
        node_selector_value: enabled
      error_server:
        node_selector_key: openstack-control-plane
        node_selector_value: enabled
    pod:
      replicas:
        ingress: 2
        error_page: 2
  dependencies:
    - ingress-htk
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: ingress-htk
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.osh.ingress-htk
      dest:
        path: .source
data:
  chart_name: ingress-htk
  release: ingress-htk
  namespace: ingress-htk
  values: {}
  dependencies: []
...
---
schema: armada/ChartGroup/v1
metadata:
  schema: metadata/Document/v1
  name: openstack-ingress-controller
  labels:
    name: openstack-ingress-controller-chart-group-global
    component: ingress
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
data:
  description: OpenStack Namespace Ingress
  chart_group:
    - openstack-ingress-controller
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: openstack-rabbitmq
  labels:
    name: openstack-rabbitmq-global
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    # Chart source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.osh.rabbitmq
      dest:
        path: .source

    # Images
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.osh.rabbitmq
      dest:
        path: .values.images.tags

    # Endpoints
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_endpoints
        path: .osh.oslo_messaging
      dest:
        path: .values.endpoints.oslo_messaging
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_endpoints
        path: .osh.openstack_rabbitmq_exporter
      dest:
        path: .values.endpoints.prometheus_rabbitmq_exporter
    # Credentials
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_service_accounts
        path: .osh.oslo_messaging.admin
      dest:
        path: .values.endpoints.oslo_messaging.auth.user

    # Secrets

    - src:
        schema: deckhand/Passphrase/v1
        name: osh_rabbitmq_erlang_cookie
        path: .
      dest:
        path: .values.endpoints.oslo_messaging.auth.erlang_cookie
    - src:
        schema: deckhand/Passphrase/v1
        name: osh_oslo_messaging_admin_password
        path: .
      dest:
        path: .values.endpoints.oslo_messaging.auth.user.password
data:
  chart_name: openstack-rabbitmq
  release: openstack-rabbitmq
  namespace: openstack
  wait:
    timeout: 900
    labels:
      release_group: airship-openstack-rabbitmq
    resources:
      - type: statefulset
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
    pre:
      delete:
        - type: job
          labels:
            release_group: airship-openstack-rabbitmq
  values:
    pod:
      replicas:
        server: 2
    labels:
      server:
        node_selector_key: openstack-control-plane
        node_selector_value: enabled
      prometheus_rabbitmq_exporter:
        node_selector_key: openstack-control-plane
        node_selector_value: enabled
    monitoring:
      prometheus:
        enabled: true
  dependencies:
    - openstack-rabbitmq-htk
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: openstack-rabbitmq-htk
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.osh.rabbitmq-htk
      dest:
        path: .source
data:
  chart_name: openstack-rabbitmq-htk
  release: openstack-rabbitmq-htk
  namespace: openstack-rabbitmq-htk
  values: {}
  dependencies: []
...
---
schema: armada/ChartGroup/v1
metadata:
  schema: metadata/Document/v1
  name: openstack-rabbitmq
  labels:
    name: openstack-rabbitmq-chart-group-global
    component: keystone
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
data:
  description: Deploy Keystone
  chart_group:
    - openstack-rabbitmq
...
---
schema: armada/ChartGroup/v1
metadata:
  schema: metadata/Document/v1
  name: openstack-radosgw
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
data:
  description: Deploy Radosgw
  chart_group:
    - tenant-ceph-rgw
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: tenant-ceph-rgw
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    # Chart source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.ucp.tenant-ceph-rgw
      dest:
        path: .source

    # Images
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.ceph.tenant-ceph-rgw
      dest:
        path: .values.images.tags

    # IP addresses
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .storage.ceph.public_cidr
      dest:
        path: .values.network.public
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .storage.ceph.cluster_cidr
      dest:
        path: .values.network.cluster

    # Endpoints
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_endpoints
        path: .osh.identity
      dest:
        path: .values.endpoints.identity
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_endpoints
        path: .osh.object_store
      dest:
        path: .values.endpoints.object_store
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: ucp_endpoints
        path: .ceph.tenant_ceph_mon
      dest:
        path: .values.endpoints.ceph_mon

    # Credentials
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_service_accounts
        path: .osh.keystone.admin
      dest:
        path: .values.endpoints.identity.auth.admin
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_service_accounts
        path: .osh.swift.keystone
      dest:
        path: .values.endpoints.identity.auth.swift

    # Secrets
    - dest:
        path: .values.endpoints.identity.auth.admin.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_keystone_admin_password
        path: .
    - dest:
        path: .values.endpoints.identity.auth.swift.password
      src:
        schema: deckhand/Passphrase/v1
        name: ceph_swift_keystone_password
        path: .

data:
  chart_name: tenant-ceph-rgw
  release: tenant-ceph-rgw
  namespace: openstack
  wait:
    timeout: 900
    labels:
      release_group: airship-tenant-ceph-rgw
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
    pre:
      delete:
        - type: job
          labels:
            release_group: airship-tenant-ceph-rgw
  values:
    labels:
      job:
        node_selector_key: tenant-ceph-control-plane
        node_selector_value: enabled
      rgw:
        node_selector_key: tenant-ceph-rgw
        node_selector_value: enabled
    endpoints:
      identity:
        namespace: openstack
      object_store:
        namespace: openstack
      ceph_mon:
        namespace: tenant-ceph
    deployment:
      ceph: true
      client_secrets: false
      rbd_provisioner: false
      cephfs_provisioner: false
      rgw_keystone_user_and_endpoints: true
    bootstrap:
      enabled: false
    pod:
      replicas:
        rgw: 2
    conf:
      rgw_ks:
        enabled: true
      config:
          #NOTE (portdirect): See http://tracker.ceph.com/issues/21226
          rgw_keystone_token_cache_size: '0'
    ceph_client:
      configmap: tenant-ceph-etc
    secrets:
      keyrings:
        admin: pvc-tceph-client-key
  dependencies:
    - tenant-ceph-htk
...
---
schema: armada/ChartGroup/v1
metadata:
  schema: metadata/Document/v1
  name: openstack-glance
  labels:
    name: openstack-glance-chart-group-global
    component: glance
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
data:
  description: Deploy Glance
  chart_group:
    - glance
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: glance
  labels:
    name: glance-global
    component: glance
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    # Chart source
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.osh.glance
      dest:
        path: .source

    # Images
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.osh.glance
      dest:
        path: .values.images.tags

    # Endpoints
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_endpoints
        path: .osh.identity
      dest:
        path: .values.endpoints.identity
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_endpoints
        path: .osh.image
      dest:
        path: .values.endpoints.image
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_endpoints
        path: .osh.image_registry
      dest:
        path: .values.endpoints.image_registry
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_endpoints
        path: .osh.oslo_db
      dest:
        path: .values.endpoints.oslo_db
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_endpoints
        path: .osh.oslo_messaging
      dest:
        path: .values.endpoints.oslo_messaging
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_endpoints
        path: .osh.oslo_cache
      dest:
        path: .values.endpoints.oslo_cache
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_endpoints
        path: .osh.ceph_object_store
      dest:
        path: .values.endpoints.ceph_object_store
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_endpoints
        path: .osh.object_store
      dest:
        path: .values.endpoints.object_store
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_infra_endpoints
        path: .osh_infra.fluentd
      dest:
        path: .values.endpoints.fluentd
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_service_accounts
        path: .osh.keystone.admin
      dest:
        path: .values.endpoints.identity.auth.admin
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_service_accounts
        path: .osh.glance.glance
      dest:
        path: .values.endpoints.identity.auth.glance
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_service_accounts
        path: .osh.oslo_messaging.admin
      dest:
        path: .values.endpoints.oslo_messaging.auth.admin
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_service_accounts
        path: .osh.glance.oslo_messaging.glance
      dest:
        path: .values.endpoints.oslo_messaging.auth.glance
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_service_accounts
        path: .osh.glance.oslo_db
      dest:
        path: .values.endpoints.oslo_db.auth.glance
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_service_accounts
        path: .osh.glance.oslo_db.database
      dest:
        path: .values.endpoints.oslo_db.path
        pattern: DB_NAME
    - src:
        schema: pegleg/AccountCatalogue/v1
        name: osh_service_accounts
        path: .osh.glance.ceph_object_store
      dest:
        path: .values.endpoints.ceph_object_store.auth.glance
    - src:
        schema: pegleg/EndpointCatalogue/v1
        name: osh_endpoints
        path: .osh.image.name
      dest:
        path: .values.endpoints.oslo_messaging.path
        pattern: VHOST_NAME

    # Secrets
    - dest:
        path: .values.endpoints.identity.auth.admin.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_keystone_admin_password
        path: .
    - dest:
        path: .values.endpoints.identity.auth.glance.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_glance_password
        path: .
    - dest:
        path: .values.endpoints.oslo_messaging.auth.admin.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_oslo_messaging_admin_password
        path: .
    - dest:
        path: .values.endpoints.oslo_messaging.auth.glance.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_oslo_messaging_admin_password
        path: .
    - dest:
        path: .values.endpoints.oslo_db.auth.glance.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_glance_oslo_db_password
        path: .
    - dest:
        path: .values.endpoints.oslo_db.auth.admin.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_oslo_db_admin_password
        path: .
    - dest:
        path: .values.endpoints.oslo_cache.auth.memcache_secret_key
      src:
        schema: deckhand/Passphrase/v1
        name: osh_oslo_cache_secret_key
        path: .
    - dest:
        path: .values.endpoints.object_store.auth.glance.tmpurlkey
      src:
        schema: deckhand/Passphrase/v1
        name: ceph_swift_keystone_password
        path: .
    - dest:
        path: .values.endpoints.ceph_object_store.auth.glance.tmpurlkey
      src:
        schema: deckhand/Passphrase/v1
        name: ceph_swift_keystone_password
        path: .
    - dest:
        path: .values.endpoints.ceph_object_store.auth.glance.password
      src:
        schema: deckhand/Passphrase/v1
        name: osh_glance_password
        path: .
data:
  chart_name: glance
  release: glance
  namespace: openstack
  wait:
    timeout: 900
    labels:
      release_group: airship-glance
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
    pre:
      delete:
        - type: job
          labels:
            release_group: airship-glance
    post:
      create: []
  values:
    pod:
      replicas:
        api: 2
        registry: 2
    labels:
      api:
        node_selector_key: openstack-control-plane
        node_selector_value: enabled
      job:
        node_selector_key: openstack-control-plane
        node_selector_value: enabled
      registry:
        node_selector_key: openstack-control-plane
        node_selector_value: enabled
    ceph_client:
      configmap: tenant-ceph-etc
      user_secret_name: pvc-tceph-client-key
    conf:
      logging:
        loggers:
          keys:
            - root
            - glance
        handlers:
          keys:
            - stdout
            - stderr
            - 'null'
            - fluent
        formatters:
          keys:
            - context
            - default
            - fluent
        logger_root:
          level: WARNING
          handlers: stdout
        logger_glance:
          level: INFO
          handlers:
            - stdout
            - fluent
          qualname: glance
        logger_amqp:
          level: WARNING
          handlers: stderr
          qualname: amqp
        logger_amqplib:
          level: WARNING
          handlers: stderr
          qualname: amqplib
        logger_eventletwsgi:
          level: WARNING
          handlers: stderr
          qualname: eventlet.wsgi.server
        logger_sqlalchemy:
          level: WARNING
          handlers: stderr
          qualname: sqlalchemy
        logger_boto:
          level: WARNING
          handlers: stderr
          qualname: boto
        handler_null:
          class: logging.NullHandler
          formatter: default
          args: ()
        handler_stdout:
          class: StreamHandler
          args: (sys.stdout,)
          formatter: context
        handler_stderr:
          class: StreamHandler
          args: (sys.stderr,)
          formatter: context
        handler_fluent:
          class: fluent.handler.FluentHandler
          args: ('openstack.glance', 'fluentd-logging.osh-infra', 24224)
          formatter: fluent
        formatter_fluent:
          class: oslo_log.formatters.FluentFormatter
        formatter_context:
          class: oslo_log.formatters.ContextFormatter
        formatter_default:
          format: "%(message)s"
  dependencies:
    - glance-htk
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: glance-htk
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .charts.osh.glance-htk
      dest:
        path: .source
data:
  chart_name: glance-htk
  release: glance-htk
  namespace: glance-htk
  values: {}
  dependencies: []
...
---
schema: promenade/EncryptionPolicy/v1
metadata:
  schema: metadata/Document/v1
  name: encryption-policy
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    - src:
        schema: deckhand/Passphrase/v1
        name: apiserver-encryption-key-key1
        path: .
      dest:
        path: .etcd[0].providers[0].secretbox.keys[0].secret
data:
  etcd:
    - resources:
        - 'secrets'
      providers:
        - secretbox:
            keys:
             - name: key1
               secret: null
        - identity: {}
...
---
data:
  charts:
    kubernetes:
      apiserver:
        location: https://opendev.org/airship/promenade
        reference: b77002339cbce6e064b8aa1ea766541b477fc881
        subpath: charts/apiserver
        type: git
      apiserver-htk:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: helm-toolkit
        type: git
      calico:
        calico:
          location: https://opendev.org/openstack/openstack-helm-infra
          reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
          subpath: calico
          type: git
        calico-htk:
          location: https://opendev.org/openstack/openstack-helm-infra
          reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
          subpath: helm-toolkit
          type: git
        etcd:
          location: https://opendev.org/airship/promenade
          reference: b77002339cbce6e064b8aa1ea766541b477fc881
          subpath: charts/etcd
          type: git
        etcd-htk:
          location: https://opendev.org/openstack/openstack-helm-infra
          reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
          subpath: helm-toolkit
          type: git
      controller-manager:
        location: https://opendev.org/airship/promenade
        reference: b77002339cbce6e064b8aa1ea766541b477fc881
        subpath: charts/controller_manager
        type: git
      controller-manager-htk:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: helm-toolkit
        type: git
      coredns:
        location: https://opendev.org/airship/promenade
        reference: b77002339cbce6e064b8aa1ea766541b477fc881
        subpath: charts/coredns
        type: git
      coredns-htk:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: helm-toolkit
        type: git
      etcd:
        location: https://opendev.org/airship/promenade
        reference: b77002339cbce6e064b8aa1ea766541b477fc881
        subpath: charts/etcd
        type: git
      etcd-htk:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: helm-toolkit
        type: git
      haproxy:
        location: https://opendev.org/airship/promenade
        reference: b77002339cbce6e064b8aa1ea766541b477fc881
        subpath: charts/haproxy
        type: git
      haproxy-htk:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: helm-toolkit
        type: git
      ingress:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: ingress
        type: git
      ingress-htk:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: helm-toolkit
        type: git
      proxy:
        location: https://opendev.org/airship/promenade
        reference: b77002339cbce6e064b8aa1ea766541b477fc881
        subpath: charts/proxy
        type: git
      proxy-htk:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: helm-toolkit
        type: git
      scheduler:
        location: https://opendev.org/airship/promenade
        reference: b77002339cbce6e064b8aa1ea766541b477fc881
        subpath: charts/scheduler
        type: git
      scheduler-htk:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: helm-toolkit
        type: git
    osh:
      barbican:
        location: https://opendev.org/openstack/openstack-helm
        reference: d2abe39d498f48c4721e26aca19e81189bc8891b
        subpath: barbican
        type: git
      cinder:
        location: https://opendev.org/openstack/openstack-helm
        reference: d2abe39d498f48c4721e26aca19e81189bc8891b
        subpath: cinder
        type: git
      cinder-htk:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: helm-toolkit
        type: git
      glance:
        location: https://opendev.org/openstack/openstack-helm
        reference: d2abe39d498f48c4721e26aca19e81189bc8891b
        subpath: glance
        type: git
      glance-htk:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: helm-toolkit
        type: git
      heat:
        location: https://opendev.org/openstack/openstack-helm
        reference: d2abe39d498f48c4721e26aca19e81189bc8891b
        subpath: heat
        type: git
      heat-htk:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: helm-toolkit
        type: git
      helm_toolkit:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: helm-toolkit
        type: git
      horizon:
        location: https://opendev.org/openstack/openstack-helm
        reference: d2abe39d498f48c4721e26aca19e81189bc8891b
        subpath: horizon
        type: git
      horizon-htk:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: helm-toolkit
        type: git
      ingress:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: ingress
        type: git
      ingress-htk:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: helm-toolkit
        type: git
      keystone:
        location: https://opendev.org/openstack/openstack-helm
        reference: d2abe39d498f48c4721e26aca19e81189bc8891b
        subpath: keystone
        type: git
      keystone-htk:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: helm-toolkit
        type: git
      libvirt:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: libvirt
        type: git
      libvirt-htk:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: helm-toolkit
        type: git
      mariadb:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: mariadb
        type: git
      mariadb-htk:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: helm-toolkit
        type: git
      memcached:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: memcached
        type: git
      memcached-htk:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: helm-toolkit
        type: git
      neutron:
        location: https://opendev.org/openstack/openstack-helm
        reference: 93aca8ee38eb34c8ee3ae70ae5bc2e3c369dcd80
        subpath: neutron
        type: git
      neutron-htk:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: helm-toolkit
        type: git
      nova:
        location: https://opendev.org/openstack/openstack-helm
        reference: d2abe39d498f48c4721e26aca19e81189bc8891b
        subpath: nova
        type: git
      nova-htk:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: helm-toolkit
        type: git
      openvswitch:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: openvswitch
        type: git
      openvswitch-htk:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: helm-toolkit
        type: git
      rabbitmq:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: rabbitmq
        type: git
      rabbitmq-htk:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: helm-toolkit
        type: git
      tempest:
        location: https://opendev.org/openstack/openstack-helm
        reference: d2abe39d498f48c4721e26aca19e81189bc8891b
        subpath: tempest
        type: git
      tempest-htk:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: helm-toolkit
        type: git
    osh_infra:
      elasticsearch:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: elasticsearch
        type: git
      fluentbit:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: fluentbit
        type: git
      fluentd:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: fluentd
        type: git
      grafana:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: grafana
        type: git
      helm_toolkit:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: helm-toolkit
        type: git
      kibana:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: kibana
        type: git
      nagios:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: nagios
        type: git
      nfs_provisioner:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: nfs-provisioner
        type: git
      podsecuritypolicy:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: podsecuritypolicy
        type: git
      prometheus:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: prometheus
        type: git
      prometheus_alertmanager:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: prometheus-alertmanager
        type: git
      prometheus_kube_state_metrics:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: prometheus-kube-state-metrics
        type: git
      prometheus_node_exporter:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: prometheus-node-exporter
        type: git
      prometheus_openstack_exporter:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: prometheus-openstack-exporter
        type: git
      prometheus_process_exporter:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: prometheus-process-exporter
        type: git
    ucp:
      armada:
        location: https://opendev.org/airship/armada
        reference: d5ab6a05c4fbd0884b7cbc4169708bed1d12dfab
        subpath: charts/armada
        type: git
      armada-htk:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: helm-toolkit
        type: git
      barbican:
        location: https://opendev.org/openstack/openstack-helm
        reference: d2abe39d498f48c4721e26aca19e81189bc8891b
        subpath: barbican
        type: git
      barbican-htk:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: helm-toolkit
        type: git
      ceph-client:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: ceph-client
        type: git
      ceph-htk:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: helm-toolkit
        type: git
      ceph-mon:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: ceph-mon
        type: git
      ceph-osd:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: ceph-osd
        type: git
      ceph-provisioners:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: ceph-provisioners
        type: git
      ceph-rgw:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: ceph-rgw
        type: git
      deckhand:
        location: https://opendev.org/airship/deckhand
        reference: 070124b578ee95ed1da5cae0a82b825bceb373e0
        subpath: charts/deckhand
        type: git
      deckhand-htk:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: helm-toolkit
        type: git
      divingbell:
        location: https://opendev.org/airship/divingbell
        reference: 2e5ffaccca1f8824384569f8add5bead28fddcdc
        subpath: divingbell
        type: git
      divingbell-htk:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: helm-toolkit
        type: git
      drydock:
        location: https://opendev.org/airship/drydock
        reference: 2e97bd5b72cd4cf13b3a993d6e083b25ca292587
        subpath: charts/drydock
        type: git
      drydock-htk:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: helm-toolkit
        type: git
      ingress:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: ingress
        type: git
      ingress-htk:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: helm-toolkit
        type: git
      keystone:
        location: https://opendev.org/openstack/openstack-helm
        reference: d2abe39d498f48c4721e26aca19e81189bc8891b
        subpath: keystone
        type: git
      keystone-htk:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: helm-toolkit
        type: git
      maas:
        location: https://opendev.org/airship/maas
        reference: a8887a93b4fea102aeaa8aa17bfaa648126049a9
        subpath: charts/maas
        type: git
      maas-htk:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: helm-toolkit
        type: git
      mariadb:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: mariadb
        type: git
      mariadb-htk:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: helm-toolkit
        type: git
      memcached:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: memcached
        type: git
      memcached-htk:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: helm-toolkit
        type: git
      postgresql:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: postgresql
        type: git
      postgresql-htk:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: helm-toolkit
        type: git
      promenade:
        location: https://opendev.org/airship/promenade
        reference: b77002339cbce6e064b8aa1ea766541b477fc881
        subpath: charts/promenade
        type: git
      promenade-htk:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: helm-toolkit
        type: git
      rabbitmq:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: rabbitmq
        type: git
      rabbitmq-htk:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: helm-toolkit
        type: git
      shipyard:
        location: https://opendev.org/airship/shipyard
        reference: f42e85d7cedb0a22c2f90f53ad4902cc4a62d9a9
        subpath: charts/shipyard
        type: git
      shipyard-htk:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: helm-toolkit
        type: git
      tenant-ceph-client:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: ceph-client
        type: git
      tenant-ceph-htk:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: helm-toolkit
        type: git
      tenant-ceph-mon:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: ceph-mon
        type: git
      tenant-ceph-osd:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: ceph-osd
        type: git
      tenant-ceph-provisioners:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: ceph-provisioners
        type: git
      tenant-ceph-rgw:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: ceph-rgw
        type: git
      tiller:
        location: https://opendev.org/airship/armada
        reference: d5ab6a05c4fbd0884b7cbc4169708bed1d12dfab
        subpath: charts/tiller
        type: git
      tiller-htk:
        location: https://opendev.org/openstack/openstack-helm-infra
        reference: d1188dd7a7dc90bb562752d0d18ed8019416821e
        subpath: helm-toolkit
        type: git
  images:
    calico:
      calico: {}
      etcd: {}
    ceph:
      ceph-client: {}
      ceph-mon: {}
      ceph-osd: {}
      ceph-provisioners: {}
      ceph-rgw: {}
      tenant-ceph-client: {}
      tenant-ceph-mon: {}
      tenant-ceph-osd: {}
      tenant-ceph-provisioners: {}
      tenant-ceph-rgw: {}
    kubernetes:
      apiserver:
        anchor: gcr.io/google-containers/hyperkube-amd64:v1.12.9
        apiserver: gcr.io/google-containers/hyperkube-amd64:v1.12.9
      controller-manager:
        anchor: gcr.io/google-containers/hyperkube-amd64:v1.12.9
        controller_manager: gcr.io/google-containers/hyperkube-amd64:v1.12.9
      coredns:
        coredns: docker.io/coredns/coredns:1.1.3
      etcd:
        etcd: quay.io/coreos/etcd:v3.3.12
        etcdctl: quay.io/coreos/etcd:v3.3.12
      haproxy:
        anchor: gcr.io/google-containers/hyperkube-amd64:v1.12.9
        haproxy: docker.io/haproxy:1.8.19
      hyperkube: gcr.io/google-containers/hyperkube-amd64:v1.12.9
      ingress: {}
      pause: gcr.io/google-containers/pause-amd64:3.1
      proxy:
        proxy: gcr.io/google-containers/hyperkube-amd64:v1.12.9
      scheduler:
        anchor: gcr.io/google-containers/hyperkube-amd64:v1.12.9
        scheduler: gcr.io/google-containers/hyperkube-amd64:v1.12.9
    osh:
      barbican: {}
      cinder: {}
      glance: {}
      heat: {}
      horizon: {}
      ingress: {}
      keystone: {}
      libvirt: {}
      mariadb: {}
      memcached: {}
      neutron: {}
      nova: {}
      openvswitch: {}
      rabbitmq: {}
      tempest: {}
    osh_infra:
      elasticsearch: {}
      fluentbit: {}
      fluentd: {}
      grafana: {}
      kibana: {}
      nagios: {}
      nfs_provisioner:
        nfs_provisioner: quay.io/kubernetes_incubator/nfs-provisioner:v2.1.0-k8s1.11
      prometheus: {}
      prometheus_alertmanager: {}
      prometheus_kube_state_metrics: {}
      prometheus_node_exporter: {}
      prometheus_openstack_exporter: {}
      prometheus_process_exporter: {}
    ucp:
      armada:
        api: quay.io/airshipit/armada:d5ab6a05c4fbd0884b7cbc4169708bed1d12dfab-ubuntu_xenial
        helm: docker.io/lachlanevenson/k8s-helm:v2.14.1
        tiller: gcr.io/kubernetes-helm/tiller:v2.14.1
      barbican: {}
      deckhand:
        db_sync: quay.io/airshipit/deckhand:070124b578ee95ed1da5cae0a82b825bceb373e0-ubuntu_xenial
        deckhand: quay.io/airshipit/deckhand:070124b578ee95ed1da5cae0a82b825bceb373e0-ubuntu_xenial
      divingbell:
        divingbell: docker.io/ubuntu:16.04
      drydock:
        drydock: quay.io/airshipit/drydock:2e97bd5b72cd4cf13b3a993d6e083b25ca292587
        drydock_db_sync: quay.io/airshipit/drydock:2e97bd5b72cd4cf13b3a993d6e083b25ca292587
      ingress: {}
      keystone: {}
      maas:
        bootstrap: quay.io/airshipit/maas-region-controller:a8887a93b4fea102aeaa8aa17bfaa648126049a9
        db_sync: quay.io/airshipit/maas-region-controller:a8887a93b4fea102aeaa8aa17bfaa648126049a9
        export_api_key: quay.io/airshipit/maas-region-controller:a8887a93b4fea102aeaa8aa17bfaa648126049a9
        maas_cache: quay.io/airshipit/sstream-cache:a8887a93b4fea102aeaa8aa17bfaa648126049a9
        maas_rack: quay.io/airshipit/maas-rack-controller:a8887a93b4fea102aeaa8aa17bfaa648126049a9
        maas_region: quay.io/airshipit/maas-region-controller:a8887a93b4fea102aeaa8aa17bfaa648126049a9
      mariadb: {}
      memcached: {}
      pegleg:
        pegleg: quay.io/airshipit/pegleg:5ef28bf804f18f732b5352ad34e10375de2beb5e-ubuntu_xenial
      postgresql: {}
      promenade:
        monitoring_image: busybox:1.28.3
        promenade: quay.io/airshipit/promenade:b77002339cbce6e064b8aa1ea766541b477fc881
      rabbitmq: {}
      shipyard:
        airflow: quay.io/airshipit/airflow:f42e85d7cedb0a22c2f90f53ad4902cc4a62d9a9-ubuntu_xenial
        airflow_db_sync: quay.io/airshipit/airflow:f42e85d7cedb0a22c2f90f53ad4902cc4a62d9a9-ubuntu_xenial
        shipyard: quay.io/airshipit/shipyard:f42e85d7cedb0a22c2f90f53ad4902cc4a62d9a9-ubuntu_xenial
        shipyard_db_sync: quay.io/airshipit/shipyard:f42e85d7cedb0a22c2f90f53ad4902cc4a62d9a9-ubuntu_xenial
  packages:
    gpgkey: |-
      -----BEGIN PGP PUBLIC KEY BLOCK-----
      mQINBFit2ioBEADhWpZ8/wvZ6hUTiXOwQHXMAlaFHcPH9hAtr4F1y2+OYdbtMuth
      lqqwp028AqyY+PRfVMtSYMbjuQuu5byyKR01BbqYhuS3jtqQmljZ/bJvXqnmiVXh
      38UuLa+z077PxyxQhu5BbqntTPQMfiyqEiU+BKbq2WmANUKQf+1AmZY/IruOXbnq
      L4C1+gJ8vfmXQt99npCaxEjaNRVYfOS8QcixNzHUYnb6emjlANyEVlZzeqo7XKl7
      UrwV5inawTSzWNvtjEjj4nJL8NsLwscpLPQUhTQ+7BbQXAwAmeHCUTQIvvWXqw0N
      cmhh4HgeQscQHYgOJjjDVfoY5MucvglbIgCqfzAHW9jxmRL4qbMZj+b1XoePEtht
      ku4bIQN1X5P07fNWzlgaRL5Z4POXDDZTlIQ/El58j9kp4bnWRCJW0lya+f8ocodo
      vZZ+Doi+fy4D5ZGrL4XEcIQP/Lv5uFyf+kQtl/94VFYVJOleAv8W92KdgDkhTcTD
      G7c0tIkVEKNUq48b3aQ64NOZQW7fVjfoKwEZdOqPE72Pa45jrZzvUFxSpdiNk2tZ
      XYukHjlxxEgBdC/J3cMMNRE1F4NCA3ApfV1Y7/hTeOnmDuDYwr9/obA8t016Yljj
      q5rdkywPf4JF8mXUW5eCN1vAFHxeg9ZWemhBtQmGxXnw9M+z6hWwc6ahmwARAQAB
      tCtEb2NrZXIgUmVsZWFzZSAoQ0UgZGViKSA8ZG9ja2VyQGRvY2tlci5jb20+iQI3
      BBMBCgAhBQJYrefAAhsvBQsJCAcDBRUKCQgLBRYCAwEAAh4BAheAAAoJEI2BgDwO
      v82IsskP/iQZo68flDQmNvn8X5XTd6RRaUH33kXYXquT6NkHJciS7E2gTJmqvMqd
      tI4mNYHCSEYxI5qrcYV5YqX9P6+Ko+vozo4nseUQLPH/ATQ4qL0Zok+1jkag3Lgk
      jonyUf9bwtWxFp05HC3GMHPhhcUSexCxQLQvnFWXD2sWLKivHp2fT8QbRGeZ+d3m
      6fqcd5Fu7pxsqm0EUDK5NL+nPIgYhN+auTrhgzhK1CShfGccM/wfRlei9Utz6p9P
      XRKIlWnXtT4qNGZNTN0tR+NLG/6Bqd8OYBaFAUcue/w1VW6JQ2VGYZHnZu9S8LMc
      FYBa5Ig9PxwGQOgq6RDKDbV+PqTQT5EFMeR1mrjckk4DQJjbxeMZbiNMG5kGECA8
      g383P3elhn03WGbEEa4MNc3Z4+7c236QI3xWJfNPdUbXRaAwhy/6rTSFbzwKB0Jm
      ebwzQfwjQY6f55MiI/RqDCyuPj3r3jyVRkK86pQKBAJwFHyqj9KaKXMZjfVnowLh
      9svIGfNbGHpucATqREvUHuQbNnqkCx8VVhtYkhDb9fEP2xBu5VvHbR+3nfVhMut5
      G34Ct5RS7Jt6LIfFdtcn8CaSas/l1HbiGeRgc70X/9aYx/V/CEJv0lIe8gP6uDoW
      FPIZ7d6vH+Vro6xuWEGiuMaiznap2KhZmpkgfupyFmplh0s6knymuQINBFit2ioB
      EADneL9S9m4vhU3blaRjVUUyJ7b/qTjcSylvCH5XUE6R2k+ckEZjfAMZPLpO+/tF
      M2JIJMD4SifKuS3xck9KtZGCufGmcwiLQRzeHF7vJUKrLD5RTkNi23ydvWZgPjtx
      Q+DTT1Zcn7BrQFY6FgnRoUVIxwtdw1bMY/89rsFgS5wwuMESd3Q2RYgb7EOFOpnu
      w6da7WakWf4IhnF5nsNYGDVaIHzpiqCl+uTbf1epCjrOlIzkZ3Z3Yk5CM/TiFzPk
      z2lLz89cpD8U+NtCsfagWWfjd2U3jDapgH+7nQnCEWpROtzaKHG6lA3pXdix5zG8
      eRc6/0IbUSWvfjKxLLPfNeCS2pCL3IeEI5nothEEYdQH6szpLog79xB9dVnJyKJb
      VfxXnseoYqVrRz2VVbUI5Blwm6B40E3eGVfUQWiux54DspyVMMk41Mx7QJ3iynIa
      1N4ZAqVMAEruyXTRTxc9XW0tYhDMA/1GYvz0EmFpm8LzTHA6sFVtPm/ZlNCX6P1X
      zJwrv7DSQKD6GGlBQUX+OeEJ8tTkkf8QTJSPUdh8P8YxDFS5EOGAvhhpMBYD42kQ
      pqXjEC+XcycTvGI7impgv9PDY1RCC1zkBjKPa120rNhv/hkVk/YhuGoajoHyy4h7
      ZQopdcMtpN2dgmhEegny9JCSwxfQmQ0zK0g7m6SHiKMwjwARAQABiQQ+BBgBCAAJ
      BQJYrdoqAhsCAikJEI2BgDwOv82IwV0gBBkBCAAGBQJYrdoqAAoJEH6gqcPyc/zY
      1WAP/2wJ+R0gE6qsce3rjaIz58PJmc8goKrir5hnElWhPgbq7cYIsW5qiFyLhkdp
      YcMmhD9mRiPpQn6Ya2w3e3B8zfIVKipbMBnke/ytZ9M7qHmDCcjoiSmwEXN3wKYI
      mD9VHONsl/CG1rU9Isw1jtB5g1YxuBA7M/m36XN6x2u+NtNMDB9P56yc4gfsZVES
      KA9v+yY2/l45L8d/WUkUi0YXomn6hyBGI7JrBLq0CX37GEYP6O9rrKipfz73XfO7
      JIGzOKZlljb/D9RX/g7nRbCn+3EtH7xnk+TK/50euEKw8SMUg147sJTcpQmv6UzZ
      cM4JgL0HbHVCojV4C/plELwMddALOFeYQzTif6sMRPf+3DSj8frbInjChC3yOLy0
      6br92KFom17EIj2CAcoeq7UPhi2oouYBwPxh5ytdehJkoo+sN7RIWua6P2WSmon5
      U888cSylXC0+ADFdgLX9K2zrDVYUG1vo8CX0vzxFBaHwN6Px26fhIT1/hYUHQR1z
      VfNDcyQmXqkOnZvvoMfz/Q0s9BhFJ/zU6AgQbIZE/hm1spsfgvtsD1frZfygXJ9f
      irP+MSAI80xHSf91qSRZOj4Pl3ZJNbq4yYxv0b1pkMqeGdjdCYhLU+LZ4wbQmpCk
      SVe2prlLureigXtmZfkqevRz7FrIZiu9ky8wnCAPwC7/zmS18rgP/17bOtL4/iIz
      QhxAAoAMWVrGyJivSkjhSGx1uCojsWfsTAm11P7jsruIL61ZzMUVE2aM3Pmj5G+W
      9AcZ58Em+1WsVnAXdUR//bMmhyr8wL/G1YO1V3JEJTRdxsSxdYa4deGBBY/Adpsw
      24jxhOJR+lsJpqIUeb999+R8euDhRHG9eFO7DRu6weatUJ6suupoDTRWtr/4yGqe
      dKxV3qQhNLSnaAzqW/1nA3iUB4k7kCaKZxhdhDbClf9P37qaRW467BLCVO/coL3y
      Vm50dwdrNtKpMBh3ZpbB1uJvgi9mXtyBOMJ3v8RZeDzFiG8HdCtg9RvIt/AIFoHR
      H3S+U79NT6i0KPzLImDfs8T7RlpyuMc4Ufs8ggyg9v3Ae6cN3eQyxcK3w0cbBwsh
      /nQNfsA6uu+9H7NhbehBMhYnpNZyrHzCmzyXkauwRAqoCbGCNykTRwsur9gS41TQ
      M8ssD1jFheOJf3hODnkKU+HKjvMROl1DK7zdmLdNzA1cvtZH/nCC9KPj1z8QC47S
      xx+dTZSx4ONAhwbS/LN3PoKtn8LPjY9NP9uDWI+TWYquS2U+KHDrBDlsgozDbs/O
      jCxcpDzNmXpWQHEtHU7649OXHP7UeNST1mCUCH5qdank0V1iejF6/CfTFU4MfcrG
      YT90qFF93M3v01BbxP+EIY2/9tiIPbrd
      =0YYh
      -----END PGP PUBLIC KEY BLOCK-----
    named:
      docker: docker-ce=18.06.3~ce~3-0~ubuntu
      socat: socat=1.7.3.1-1
    repositories:
      docker:
        components:
        - stable
        distributions:
        - xenial
        repo_type: apt
        url: https://download.docker.com/linux/ubuntu/
      main_archive:
        components:
        - main
        - universe
        - multiverse
        distributions:
        - xenial
        repo_type: apt
        subrepos:
        - security
        - updates
        - backports
        url: http://us.archive.ubuntu.com/ubuntu
    unnamed:
    - ceph-common
metadata:
  labels:
    name: software-versions-global
  layeringDefinition:
    abstract: false
    layer: global
  name: software-versions
  schema: metadata/Document/v1
  storagePolicy: cleartext
schema: pegleg/SoftwareVersions/v1
...
---
schema: promenade/Kubelet/v1
metadata:
  schema: metadata/Document/v1
  name: kubelet
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.kubernetes.pause
      dest:
        path: .images.pause
    - src:
        schema: pegleg/SeccompProfile/v1
        name: seccomp-default
        path: .seccompDirPath
      dest:
        path: .arguments[9]
        pattern: SECCOMP_PROFILE_ROOT
data:
  arguments:
    - --cni-bin-dir=/opt/cni/bin
    - --cni-conf-dir=/etc/cni/net.d
    - --eviction-max-pod-grace-period=-1
    - --network-plugin=cni
    - --node-status-update-frequency=5s
    - --max-pods=200
    - --kube-api-burst=40
    - --kube-api-qps=20
    - --seccomp-profile-root=SECCOMP_PROFILE_ROOT
    - --feature-gates=PodShareProcessNamespace=true
...
---
schema: promenade/Docker/v1
metadata:
  schema: metadata/Document/v1
  name: docker-global
  labels:
    promenade: enabled
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
data:
  config:
    live-restore: true
    storage-driver: overlay2
    registry-mirrors: ["http://10.239.12.37:6000"]
    insecure-registries: ["10.239.12.37:5000"]
...
---
# The global deployment strategy assumes nodes are marked with node_tags
# of masters and workers.
schema: shipyard/DeploymentStrategy/v1
metadata:
  schema: metadata/Document/v1
  name: deployment-strategy
  layeringDefinition:
      abstract: false
      layer: global
  labels:
    name: deployment-strategy-global
  storagePolicy: cleartext
data:
  groups:
    - name: masters
      critical: true
      depends_on: []
      selectors:
        - node_names: []
          node_labels: []
          node_tags:
            - masters
          rack_names: []
      success_criteria:
        percent_successful_nodes: 100
    - name: workers
      critical: true
      depends_on:
        - masters
      selectors:
        - node_names: []
          node_labels: []
          node_tags:
            - workers
          rack_names: []
      success_criteria:
        percent_successful_nodes: 60
...
---
schema: promenade/HostSystem/v1
metadata:
  schema: metadata/Document/v1
  name: host-system
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
  substitutions:
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.kubernetes.hyperkube
      dest:
        path: .files[0].docker_image

    # Initial CoreDNS image (used during node Genesis and node join)
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.kubernetes.coredns.coredns
      dest:
        path: .images.coredns

    # Initial CoreDNS image (used during node Genesis and node join)
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.kubernetes.haproxy.haproxy
      dest:
        path: .images.haproxy

    # Operational tools
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.ucp.armada.helm
      dest:
        path: .images.helm.helm

    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.kubernetes.hyperkube
      dest:
        path: .images.kubernetes.hyperkube

    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.ucp.promenade.monitoring_image
      dest:
        path: .images.monitoring_image

    # System packages
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .packages.named.docker
      dest:
        path: .packages.common.required.docker
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .packages.named.socat
      dest:
        path: .packages.common.required.socat

    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .packages.unnamed
      dest:
        path: .packages.common.additional

    # Docker authorization
    - src:
        schema: deckhand/Passphrase/v1
        path: .
        name: private_docker_key
      dest:
        path: .files[4].content
        pattern: DH_SUB_PRIVATE_DOCKER_KEY

data:
  files:
    - path: /opt/kubernetes/bin/hyperkube
      file_path: /hyperkube
      mode: 0555
    - path: /opt/kubernetes/bin/kubelet
      symlink: /opt/kubernetes/bin/hyperkube
      mode: 0555
    - path: /usr/local/bin/kubectl
      symlink: /opt/kubernetes/bin/hyperkube
      mode: 0555
    - path: /etc/logrotate.d/json-logrotate
      mode: 0444
      content: |-
        /var/lib/docker/containers/*/*-json.log
        {
            compress
            copytruncate
            create 0644 root root
            weekly
            dateext
            dateformat -%Y%m%d-%s
            maxsize 100M
            missingok
            notifempty
            su root root
            rotate 1
        }
    - path: /var/lib/kubelet/.dockercfg
      mode: 0400
      # NOTE: Sample key, this repo does not exist
      content: |-
        {
           "https://private.registry.com": {
             "auth": "DH_SUB_PRIVATE_DOCKER_KEY"
           }
        }
      # Make sure that promjoin script does not run on every boot,
      # otherwise it may downgrade current versions of Docker & Kubelet.
    - path: /var/lib/prom.done
      mode: 0444
      content: ""
    - path: /etc/profile.d/kubeconfig.sh
      mode: 0744
      content: |-
        export KUBECONFIG=/etc/kubernetes/admin/kubeconfig.yaml
  packages:
    common:
      repositories:
        - deb https://download.docker.com/linux/ubuntu/ xenial stable
      keys:
        - |-
          -----BEGIN PGP PUBLIC KEY BLOCK-----

          mQINBFit2ioBEADhWpZ8/wvZ6hUTiXOwQHXMAlaFHcPH9hAtr4F1y2+OYdbtMuth
          lqqwp028AqyY+PRfVMtSYMbjuQuu5byyKR01BbqYhuS3jtqQmljZ/bJvXqnmiVXh
          38UuLa+z077PxyxQhu5BbqntTPQMfiyqEiU+BKbq2WmANUKQf+1AmZY/IruOXbnq
          L4C1+gJ8vfmXQt99npCaxEjaNRVYfOS8QcixNzHUYnb6emjlANyEVlZzeqo7XKl7
          UrwV5inawTSzWNvtjEjj4nJL8NsLwscpLPQUhTQ+7BbQXAwAmeHCUTQIvvWXqw0N
          cmhh4HgeQscQHYgOJjjDVfoY5MucvglbIgCqfzAHW9jxmRL4qbMZj+b1XoePEtht
          ku4bIQN1X5P07fNWzlgaRL5Z4POXDDZTlIQ/El58j9kp4bnWRCJW0lya+f8ocodo
          vZZ+Doi+fy4D5ZGrL4XEcIQP/Lv5uFyf+kQtl/94VFYVJOleAv8W92KdgDkhTcTD
          G7c0tIkVEKNUq48b3aQ64NOZQW7fVjfoKwEZdOqPE72Pa45jrZzvUFxSpdiNk2tZ
          XYukHjlxxEgBdC/J3cMMNRE1F4NCA3ApfV1Y7/hTeOnmDuDYwr9/obA8t016Yljj
          q5rdkywPf4JF8mXUW5eCN1vAFHxeg9ZWemhBtQmGxXnw9M+z6hWwc6ahmwARAQAB
          tCtEb2NrZXIgUmVsZWFzZSAoQ0UgZGViKSA8ZG9ja2VyQGRvY2tlci5jb20+iQI3
          BBMBCgAhBQJYrefAAhsvBQsJCAcDBRUKCQgLBRYCAwEAAh4BAheAAAoJEI2BgDwO
          v82IsskP/iQZo68flDQmNvn8X5XTd6RRaUH33kXYXquT6NkHJciS7E2gTJmqvMqd
          tI4mNYHCSEYxI5qrcYV5YqX9P6+Ko+vozo4nseUQLPH/ATQ4qL0Zok+1jkag3Lgk
          jonyUf9bwtWxFp05HC3GMHPhhcUSexCxQLQvnFWXD2sWLKivHp2fT8QbRGeZ+d3m
          6fqcd5Fu7pxsqm0EUDK5NL+nPIgYhN+auTrhgzhK1CShfGccM/wfRlei9Utz6p9P
          XRKIlWnXtT4qNGZNTN0tR+NLG/6Bqd8OYBaFAUcue/w1VW6JQ2VGYZHnZu9S8LMc
          FYBa5Ig9PxwGQOgq6RDKDbV+PqTQT5EFMeR1mrjckk4DQJjbxeMZbiNMG5kGECA8
          g383P3elhn03WGbEEa4MNc3Z4+7c236QI3xWJfNPdUbXRaAwhy/6rTSFbzwKB0Jm
          ebwzQfwjQY6f55MiI/RqDCyuPj3r3jyVRkK86pQKBAJwFHyqj9KaKXMZjfVnowLh
          9svIGfNbGHpucATqREvUHuQbNnqkCx8VVhtYkhDb9fEP2xBu5VvHbR+3nfVhMut5
          G34Ct5RS7Jt6LIfFdtcn8CaSas/l1HbiGeRgc70X/9aYx/V/CEJv0lIe8gP6uDoW
          FPIZ7d6vH+Vro6xuWEGiuMaiznap2KhZmpkgfupyFmplh0s6knymuQINBFit2ioB
          EADneL9S9m4vhU3blaRjVUUyJ7b/qTjcSylvCH5XUE6R2k+ckEZjfAMZPLpO+/tF
          M2JIJMD4SifKuS3xck9KtZGCufGmcwiLQRzeHF7vJUKrLD5RTkNi23ydvWZgPjtx
          Q+DTT1Zcn7BrQFY6FgnRoUVIxwtdw1bMY/89rsFgS5wwuMESd3Q2RYgb7EOFOpnu
          w6da7WakWf4IhnF5nsNYGDVaIHzpiqCl+uTbf1epCjrOlIzkZ3Z3Yk5CM/TiFzPk
          z2lLz89cpD8U+NtCsfagWWfjd2U3jDapgH+7nQnCEWpROtzaKHG6lA3pXdix5zG8
          eRc6/0IbUSWvfjKxLLPfNeCS2pCL3IeEI5nothEEYdQH6szpLog79xB9dVnJyKJb
          VfxXnseoYqVrRz2VVbUI5Blwm6B40E3eGVfUQWiux54DspyVMMk41Mx7QJ3iynIa
          1N4ZAqVMAEruyXTRTxc9XW0tYhDMA/1GYvz0EmFpm8LzTHA6sFVtPm/ZlNCX6P1X
          zJwrv7DSQKD6GGlBQUX+OeEJ8tTkkf8QTJSPUdh8P8YxDFS5EOGAvhhpMBYD42kQ
          pqXjEC+XcycTvGI7impgv9PDY1RCC1zkBjKPa120rNhv/hkVk/YhuGoajoHyy4h7
          ZQopdcMtpN2dgmhEegny9JCSwxfQmQ0zK0g7m6SHiKMwjwARAQABiQQ+BBgBCAAJ
          BQJYrdoqAhsCAikJEI2BgDwOv82IwV0gBBkBCAAGBQJYrdoqAAoJEH6gqcPyc/zY
          1WAP/2wJ+R0gE6qsce3rjaIz58PJmc8goKrir5hnElWhPgbq7cYIsW5qiFyLhkdp
          YcMmhD9mRiPpQn6Ya2w3e3B8zfIVKipbMBnke/ytZ9M7qHmDCcjoiSmwEXN3wKYI
          mD9VHONsl/CG1rU9Isw1jtB5g1YxuBA7M/m36XN6x2u+NtNMDB9P56yc4gfsZVES
          KA9v+yY2/l45L8d/WUkUi0YXomn6hyBGI7JrBLq0CX37GEYP6O9rrKipfz73XfO7
          JIGzOKZlljb/D9RX/g7nRbCn+3EtH7xnk+TK/50euEKw8SMUg147sJTcpQmv6UzZ
          cM4JgL0HbHVCojV4C/plELwMddALOFeYQzTif6sMRPf+3DSj8frbInjChC3yOLy0
          6br92KFom17EIj2CAcoeq7UPhi2oouYBwPxh5ytdehJkoo+sN7RIWua6P2WSmon5
          U888cSylXC0+ADFdgLX9K2zrDVYUG1vo8CX0vzxFBaHwN6Px26fhIT1/hYUHQR1z
          VfNDcyQmXqkOnZvvoMfz/Q0s9BhFJ/zU6AgQbIZE/hm1spsfgvtsD1frZfygXJ9f
          irP+MSAI80xHSf91qSRZOj4Pl3ZJNbq4yYxv0b1pkMqeGdjdCYhLU+LZ4wbQmpCk
          SVe2prlLureigXtmZfkqevRz7FrIZiu9ky8wnCAPwC7/zmS18rgP/17bOtL4/iIz
          QhxAAoAMWVrGyJivSkjhSGx1uCojsWfsTAm11P7jsruIL61ZzMUVE2aM3Pmj5G+W
          9AcZ58Em+1WsVnAXdUR//bMmhyr8wL/G1YO1V3JEJTRdxsSxdYa4deGBBY/Adpsw
          24jxhOJR+lsJpqIUeb999+R8euDhRHG9eFO7DRu6weatUJ6suupoDTRWtr/4yGqe
          dKxV3qQhNLSnaAzqW/1nA3iUB4k7kCaKZxhdhDbClf9P37qaRW467BLCVO/coL3y
          Vm50dwdrNtKpMBh3ZpbB1uJvgi9mXtyBOMJ3v8RZeDzFiG8HdCtg9RvIt/AIFoHR
          H3S+U79NT6i0KPzLImDfs8T7RlpyuMc4Ufs8ggyg9v3Ae6cN3eQyxcK3w0cbBwsh
          /nQNfsA6uu+9H7NhbehBMhYnpNZyrHzCmzyXkauwRAqoCbGCNykTRwsur9gS41TQ
          M8ssD1jFheOJf3hODnkKU+HKjvMROl1DK7zdmLdNzA1cvtZH/nCC9KPj1z8QC47S
          xx+dTZSx4ONAhwbS/LN3PoKtn8LPjY9NP9uDWI+TWYquS2U+KHDrBDlsgozDbs/O
          jCxcpDzNmXpWQHEtHU7649OXHP7UeNST1mCUCH5qdank0V1iejF6/CfTFU4MfcrG
          YT90qFF93M3v01BbxP+EIY2/9tiIPbrd
          =0YYh
          -----END PGP PUBLIC KEY BLOCK-----
...
---
schema: promenade/Genesis/v1
metadata:
  schema: metadata/Document/v1
  name: genesis-global
  layeringDefinition:
    abstract: true
    layer: global
  labels:
    name: genesis-global
  storagePolicy: cleartext
  substitutions:
    # Software versions for bootstrapping phase
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.ucp.armada.api
      dest:
        path: .images.armada
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.ucp.armada.tiller
      dest:
        path: .images.helm.tiller
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.kubernetes.apiserver.apiserver
      dest:
        path: .images.kubernetes.apiserver
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.kubernetes.controller-manager.controller_manager
      dest:
        path: .images.kubernetes.controller-manager
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.kubernetes.etcd.etcd
      dest:
        path: .images.kubernetes.etcd
    - src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .images.kubernetes.scheduler.scheduler
      dest:
        path: .images.kubernetes.scheduler

    # Site-specific configuration
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .genesis.hostname
      dest:
        path: .hostname
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .genesis.ip
      dest:
        path: .ip

    # Command prefix
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .kubernetes.service_cidr
      dest:
        path: .apiserver.arguments[2]
        pattern: SERVICE_CIDR
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .kubernetes.service_node_port_range
      dest:
        path: .apiserver.arguments[3]
        pattern: SERVICE_NODE_PORT_RANGE

    # Set etcd encryption policy
    - src:
        schema: promenade/EncryptionPolicy/v1
        name: encryption-policy
        path: .etcd
      dest:
        path: .apiserver.encryption

data:
  apiserver:
    arguments:
      - --authorization-mode=Node,RBAC
      - --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,PersistentVolumeLabel,DefaultStorageClass,ResourceQuota,DefaultTolerationSeconds,NodeRestriction,EventRateLimit
      - --service-cluster-ip-range=SERVICE_CIDR
      - --service-node-port-range=SERVICE_NODE_PORT_RANGE
      - --endpoint-reconciler-type=lease
      - --feature-gates=PodShareProcessNamespace=true
      - --v=3
      - --admission-control-config-file=/etc/kubernetes/apiserver/acconfig.yaml
      - --experimental-encryption-provider-config=/etc/kubernetes/apiserver/encryption_provider.yaml
      - --requestheader-allowed-names='aggregator'
  armada:
    target_manifest: cluster-bootstrap
  haproxy:
    run_as_user: 65534
  labels:
    dynamic:
      - beta.kubernetes.io/fluentd-ds-ready=true
      - calico-etcd=enabled
      - ceph-mds=enabled
      - ceph-mon=enabled
      - ceph-osd=enabled
      - ceph-rgw=enabled
      - ceph-mgr=enabled
      - tenant-ceph-control-plane=enabled
      - tenant-ceph-mon=enabled
      - tenant-ceph-rgw=enabled
      - tenant-ceph-mgr=enabled
      - kube-dns=enabled
      - kube-ingress=enabled
      - kubernetes-apiserver=enabled
      - kubernetes-controller-manager=enabled
      - kubernetes-etcd=enabled
      - kubernetes-scheduler=enabled
      - promenade-genesis=enabled
      - ucp-control-plane=enabled
      - maas-rack=enabled
      - maas-region=enabled
      - node-exporter=enabled
  files:
    - path: /var/lib/anchor/calico-etcd-bootstrap
      content: "# placeholder for triggering calico etcd bootstrapping\n# this file will be deleted"
      mode: 0644
    - path: /etc/genesis/apiserver/acconfig.yaml
      mode: 0444
      content: |
        kind: AdmissionConfiguration
        apiVersion: apiserver.k8s.io/v1alpha1
        plugins:
          - name: EventRateLimit
            path: eventconfig.yaml
    - path: /etc/genesis/apiserver/eventconfig.yaml
      mode: 0444
      content: |
        kind: Configuration
        apiVersion: eventratelimit.admission.k8s.io/v1alpha1
        limits:
          - type: Server
            qps: 1000
            burst: 10000
...
---
schema: drydock/HostProfile/v1
metadata:
  schema: metadata/Document/v1
  name: cp-global
  storagePolicy: cleartext
  labels:
    hosttype: cp-global
  layeringDefinition:
    abstract: true
    layer: global
  substitutions:
    - dest:
        path: .oob.credential
      src:
        schema: deckhand/Passphrase/v1
        name: ipmi_admin_password
        path: .
data:
  oob:
    type: 'ipmi'
    network: 'oob'
    account: 'root'
  storage:
    physical_devices:
      sda:
        labels:
          bootdrive: 'true'
        partitions:
          - name: 'root'
            size: '30g'
            bootable: true
            filesystem:
              mountpoint: '/'
              fstype: 'ext4'
              mount_options: 'defaults'
          - name: 'boot'
            size: '1g'
            filesystem:
              mountpoint: '/boot'
              fstype: 'ext4'
              mount_options: 'defaults'
          - name: 'var'
            size: '>100g'
            filesystem:
              mountpoint: '/var'
              fstype: 'ext4'
              mount_options: 'defaults'
  platform:
    image: 'xenial'
    kernel: 'hwe-16.04'
    kernel_params:
      kernel_package: 'linux-image-4.15.0-46-generic'

  metadata:
    owner_data:
      control-plane: enabled
      ucp-control-plane: enabled
      openstack-control-plane: enabled
      openstack-heat: enabled
      openstack-keystone: enabled
      openstack-rabbitmq: enabled
      openstack-dns-helper: enabled
      openstack-mariadb: enabled
      openstack-nova-control: enabled
      openstack-etcd: enabled
      openstack-mistral: enabled
      openstack-memcached: enabled
      openstack-glance: enabled
      openstack-horizon: enabled
      openstack-cinder-control: enabled
      openstack-cinder-volume: control
      openstack-neutron: enabled
      openvswitch: enabled
      ucp-barbican: enabled
      ceph-mon: enabled
      ceph-mgr: enabled
      ceph-osd: enabled
      ceph-mds: enabled
      ceph-rgw: enabled
      tenant-ceph-control-plane: enabled
      tenant-ceph-mon: enabled
      tenant-ceph-rgw: enabled
      tenant-ceph-mgr: enabled
      maas-rack: enabled
      maas-region: enabled
      kube-dns: enabled
      kubernetes-apiserver: enabled
      kubernetes-controller-manager: enabled
      kubernetes-etcd: enabled
      kubernetes-scheduler: enabled
      tiller-helm: enabled
      kube-etcd: enabled
      calico-policy: enabled
      calico-node: enabled
      calico-etcd: enabled
      ucp-armada: enabled
      ucp-drydock: enabled
      ucp-deckhand: enabled
      ucp-shipyard: enabled
      IAM: enabled
      ucp-promenade: enabled
      prometheus-server: enabled
      prometheus-client: enabled
      fluentd: enabled
      fluentbit: enabled
      influxdb: enabled
      kibana: enabled
      elasticsearch-client: enabled
      elasticsearch-master: enabled
      elasticsearch-data: enabled
      postgresql: enabled
      kube-ingress: enabled
      beta.kubernetes.io/fluentd-ds-ready: 'true'
      node-exporter: enabled
...
---
schema: drydock/HostProfile/v1
metadata:
  schema: metadata/Document/v1
  name: dp-global
  labels:
    hosttype: dp-global
  layeringDefinition:
    abstract: true
    layer: global
  storagePolicy: cleartext
  substitutions:
    - dest:
        path: .oob.credential
      src:
        schema: deckhand/Passphrase/v1
        name: ipmi_admin_password
        path: .
data:
  oob:
    type: 'ipmi'
    network: 'oob'
    account: 'root'
  storage:
    physical_devices:
      sda:
        labels:
          bootdrive: 'true'
        partitions:
          - name: 'root'
            size: '30g'
            bootable: true
            filesystem:
              mountpoint: '/'
              fstype: 'ext4'
              mount_options: 'defaults'
          - name: 'boot'
            size: '1g'
            filesystem:
              mountpoint: '/boot'
              fstype: 'ext4'
              mount_options: 'defaults'
          - name: 'var'
            size: '>100g'
            filesystem:
              mountpoint: '/var'
              fstype: 'ext4'
              mount_options: 'defaults'
  platform:
    image: 'xenial'
    kernel: 'hwe-16.04'
    kernel_params:
      kernel_package: 'linux-image-4.15.0-46-generic'

  metadata:
    owner_data:
      openstack-nova-compute: enabled
      tenant-ceph-osd: enabled
      openvswitch: enabled
      contrail-vrouter: kernel
      openstack-libvirt: kernel
      beta.kubernetes.io/fluentd-ds-ready: 'true'
      node-exporter: enabled
      fluentbit: enabled
      openstack-control-plane: enabled
      openvswitch: enabled
      openstack-l3-agent: enabled

...
---
schema: 'drydock/HardwareProfile/v1'
metadata:
  schema: 'metadata/Document/v1'
  name: DELL_HP_Generic
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
data:
  vendor: Dell
  generation: '8'
  hw_version: '3'
  bios_version: '2.2.3'
  boot_mode: bios
  bootstrap_protocol: pxe
  pxe_interface: 0
  device_aliases: {}
...
---
# The data content of this file is referred from the Moby project as
# mentioned in the link below:
# https://github.com/moby/moby/blob/master/profiles/seccomp/default.json
schema: 'pegleg/SeccompProfile/v1'
metadata:
  schema: 'metadata/Document/v1'
  name: seccomp-default
  storagePolicy: 'cleartext'
  layeringDefinition:
    abstract: false
    layer: global
data:
  # Path for seccomp profile root directory.
  seccompDirPath: /var/lib/kubelet/seccomp
  # Path to save seccomp profile as file.
  # This should be same as seccompDirPath with file name.
  savePath: /var/lib/kubelet/seccomp/seccomp_default
  # Content of default seccomp profile file.
  content: |
    {
        "defaultAction": "SCMP_ACT_ERRNO",
        "archMap": [
            {
                "architecture": "SCMP_ARCH_X86_64",
                "subArchitectures": [
                    "SCMP_ARCH_X86",
                    "SCMP_ARCH_X32"
                ]
            },
            {
                "architecture": "SCMP_ARCH_AARCH64",
                "subArchitectures": [
                    "SCMP_ARCH_ARM"
                ]
            },
            {
                "architecture": "SCMP_ARCH_MIPS64",
                "subArchitectures": [
                    "SCMP_ARCH_MIPS",
                    "SCMP_ARCH_MIPS64N32"
                ]
            },
            {
                "architecture": "SCMP_ARCH_MIPS64N32",
                "subArchitectures": [
                    "SCMP_ARCH_MIPS",
                    "SCMP_ARCH_MIPS64"
                ]
            },
            {
                "architecture": "SCMP_ARCH_MIPSEL64",
                "subArchitectures": [
                    "SCMP_ARCH_MIPSEL",
                    "SCMP_ARCH_MIPSEL64N32"
                ]
            },
            {
                "architecture": "SCMP_ARCH_MIPSEL64N32",
                "subArchitectures": [
                    "SCMP_ARCH_MIPSEL",
                    "SCMP_ARCH_MIPSEL64"
                ]
            },
            {
                "architecture": "SCMP_ARCH_S390X",
                "subArchitectures": [
                    "SCMP_ARCH_S390"
                ]
            }
        ],
        "syscalls": [
            {
                "names": [
                    "accept",
                    "accept4",
                    "access",
                    "adjtimex",
                    "alarm",
                    "bind",
                    "brk",
                    "capget",
                    "capset",
                    "chdir",
                    "chmod",
                    "chown",
                    "chown32",
                    "clock_getres",
                    "clock_gettime",
                    "clock_nanosleep",
                    "close",
                    "connect",
                    "copy_file_range",
                    "creat",
                    "dup",
                    "dup2",
                    "dup3",
                    "epoll_create",
                    "epoll_create1",
                    "epoll_ctl",
                    "epoll_ctl_old",
                    "epoll_pwait",
                    "epoll_wait",
                    "epoll_wait_old",
                    "eventfd",
                    "eventfd2",
                    "execve",
                    "execveat",
                    "exit",
                    "exit_group",
                    "faccessat",
                    "fadvise64",
                    "fadvise64_64",
                    "fallocate",
                    "fanotify_mark",
                    "fchdir",
                    "fchmod",
                    "fchmodat",
                    "fchown",
                    "fchown32",
                    "fchownat",
                    "fcntl",
                    "fcntl64",
                    "fdatasync",
                    "fgetxattr",
                    "flistxattr",
                    "flock",
                    "fork",
                    "fremovexattr",
                    "fsetxattr",
                    "fstat",
                    "fstat64",
                    "fstatat64",
                    "fstatfs",
                    "fstatfs64",
                    "fsync",
                    "ftruncate",
                    "ftruncate64",
                    "futex",
                    "futimesat",
                    "getcpu",
                    "getcwd",
                    "getdents",
                    "getdents64",
                    "getegid",
                    "getegid32",
                    "geteuid",
                    "geteuid32",
                    "getgid",
                    "getgid32",
                    "getgroups",
                    "getgroups32",
                    "getitimer",
                    "getpeername",
                    "getpgid",
                    "getpgrp",
                    "getpid",
                    "getppid",
                    "getpriority",
                    "getrandom",
                    "getresgid",
                    "getresgid32",
                    "getresuid",
                    "getresuid32",
                    "getrlimit",
                    "get_robust_list",
                    "getrusage",
                    "getsid",
                    "getsockname",
                    "getsockopt",
                    "get_thread_area",
                    "gettid",
                    "gettimeofday",
                    "getuid",
                    "getuid32",
                    "getxattr",
                    "inotify_add_watch",
                    "inotify_init",
                    "inotify_init1",
                    "inotify_rm_watch",
                    "io_cancel",
                    "ioctl",
                    "io_destroy",
                    "io_getevents",
                    "ioprio_get",
                    "ioprio_set",
                    "io_setup",
                    "io_submit",
                    "ipc",
                    "kill",
                    "lchown",
                    "lchown32",
                    "lgetxattr",
                    "link",
                    "linkat",
                    "listen",
                    "listxattr",
                    "llistxattr",
                    "_llseek",
                    "lremovexattr",
                    "lseek",
                    "lsetxattr",
                    "lstat",
                    "lstat64",
                    "madvise",
                    "memfd_create",
                    "mincore",
                    "mkdir",
                    "mkdirat",
                    "mknod",
                    "mknodat",
                    "mlock",
                    "mlock2",
                    "mlockall",
                    "mmap",
                    "mmap2",
                    "mprotect",
                    "mq_getsetattr",
                    "mq_notify",
                    "mq_open",
                    "mq_timedreceive",
                    "mq_timedsend",
                    "mq_unlink",
                    "mremap",
                    "msgctl",
                    "msgget",
                    "msgrcv",
                    "msgsnd",
                    "msync",
                    "munlock",
                    "munlockall",
                    "munmap",
                    "nanosleep",
                    "newfstatat",
                    "_newselect",
                    "open",
                    "openat",
                    "pause",
                    "pipe",
                    "pipe2",
                    "poll",
                    "ppoll",
                    "prctl",
                    "pread64",
                    "preadv",
                    "preadv2",
                    "prlimit64",
                    "pselect6",
                    "pwrite64",
                    "pwritev",
                    "pwritev2",
                    "read",
                    "readahead",
                    "readlink",
                    "readlinkat",
                    "readv",
                    "recv",
                    "recvfrom",
                    "recvmmsg",
                    "recvmsg",
                    "remap_file_pages",
                    "removexattr",
                    "rename",
                    "renameat",
                    "renameat2",
                    "restart_syscall",
                    "rmdir",
                    "rt_sigaction",
                    "rt_sigpending",
                    "rt_sigprocmask",
                    "rt_sigqueueinfo",
                    "rt_sigreturn",
                    "rt_sigsuspend",
                    "rt_sigtimedwait",
                    "rt_tgsigqueueinfo",
                    "sched_getaffinity",
                    "sched_getattr",
                    "sched_getparam",
                    "sched_get_priority_max",
                    "sched_get_priority_min",
                    "sched_getscheduler",
                    "sched_rr_get_interval",
                    "sched_setaffinity",
                    "sched_setattr",
                    "sched_setparam",
                    "sched_setscheduler",
                    "sched_yield",
                    "seccomp",
                    "select",
                    "semctl",
                    "semget",
                    "semop",
                    "semtimedop",
                    "send",
                    "sendfile",
                    "sendfile64",
                    "sendmmsg",
                    "sendmsg",
                    "sendto",
                    "setfsgid",
                    "setfsgid32",
                    "setfsuid",
                    "setfsuid32",
                    "setgid",
                    "setgid32",
                    "setgroups",
                    "setgroups32",
                    "setitimer",
                    "setpgid",
                    "setpriority",
                    "setregid",
                    "setregid32",
                    "setresgid",
                    "setresgid32",
                    "setresuid",
                    "setresuid32",
                    "setreuid",
                    "setreuid32",
                    "setrlimit",
                    "set_robust_list",
                    "setsid",
                    "setsockopt",
                    "set_thread_area",
                    "set_tid_address",
                    "setuid",
                    "setuid32",
                    "setxattr",
                    "shmat",
                    "shmctl",
                    "shmdt",
                    "shmget",
                    "shutdown",
                    "sigaltstack",
                    "signalfd",
                    "signalfd4",
                    "sigreturn",
                    "socket",
                    "socketcall",
                    "socketpair",
                    "splice",
                    "stat",
                    "stat64",
                    "statfs",
                    "statfs64",
                    "statx",
                    "symlink",
                    "symlinkat",
                    "sync",
                    "sync_file_range",
                    "syncfs",
                    "sysinfo",
                    "syslog",
                    "tee",
                    "tgkill",
                    "time",
                    "timer_create",
                    "timer_delete",
                    "timerfd_create",
                    "timerfd_gettime",
                    "timerfd_settime",
                    "timer_getoverrun",
                    "timer_gettime",
                    "timer_settime",
                    "times",
                    "tkill",
                    "truncate",
                    "truncate64",
                    "ugetrlimit",
                    "umask",
                    "uname",
                    "unlink",
                    "unlinkat",
                    "utime",
                    "utimensat",
                    "utimes",
                    "vfork",
                    "vmsplice",
                    "wait4",
                    "waitid",
                    "waitpid",
                    "write",
                    "writev"
                ],
                "action": "SCMP_ACT_ALLOW",
                "args": [],
                "comment": "",
                "includes": {},
                "excludes": {}
            },
            {
                "names": [
                    "personality"
                ],
                "action": "SCMP_ACT_ALLOW",
                "args": [
                    {
                        "index": 0,
                        "value": 0,
                        "valueTwo": 0,
                        "op": "SCMP_CMP_EQ"
                    }
                ],
                "comment": "",
                "includes": {},
                "excludes": {}
            },
            {
                "names": [
                    "personality"
                ],
                "action": "SCMP_ACT_ALLOW",
                "args": [
                    {
                        "index": 0,
                        "value": 8,
                        "valueTwo": 0,
                        "op": "SCMP_CMP_EQ"
                    }
                ],
                "comment": "",
                "includes": {},
                "excludes": {}
            },
            {
                "names": [
                    "personality"
                ],
                "action": "SCMP_ACT_ALLOW",
                "args": [
                    {
                        "index": 0,
                        "value": 131072,
                        "valueTwo": 0,
                        "op": "SCMP_CMP_EQ"
                    }
                ],
                "comment": "",
                "includes": {},
                "excludes": {}
            },
            {
                "names": [
                    "personality"
                ],
                "action": "SCMP_ACT_ALLOW",
                "args": [
                    {
                        "index": 0,
                        "value": 131080,
                        "valueTwo": 0,
                        "op": "SCMP_CMP_EQ"
                    }
                ],
                "comment": "",
                "includes": {},
                "excludes": {}
            },
            {
                "names": [
                    "personality"
                ],
                "action": "SCMP_ACT_ALLOW",
                "args": [
                    {
                        "index": 0,
                        "value": 4294967295,
                        "valueTwo": 0,
                        "op": "SCMP_CMP_EQ"
                    }
                ],
                "comment": "",
                "includes": {},
                "excludes": {}
            },
            {
                "names": [
                    "sync_file_range2"
                ],
                "action": "SCMP_ACT_ALLOW",
                "args": [],
                "comment": "",
                "includes": {
                    "arches": [
                        "ppc64le"
                    ]
                },
                "excludes": {}
            },
            {
                "names": [
                    "arm_fadvise64_64",
                    "arm_sync_file_range",
                    "sync_file_range2",
                    "breakpoint",
                    "cacheflush",
                    "set_tls"
                ],
                "action": "SCMP_ACT_ALLOW",
                "args": [],
                "comment": "",
                "includes": {
                    "arches": [
                        "arm",
                        "arm64"
                    ]
                },
                "excludes": {}
            },
            {
                "names": [
                    "arch_prctl"
                ],
                "action": "SCMP_ACT_ALLOW",
                "args": [],
                "comment": "",
                "includes": {
                    "arches": [
                        "amd64",
                        "x32"
                    ]
                },
                "excludes": {}
            },
            {
                "names": [
                    "modify_ldt"
                ],
                "action": "SCMP_ACT_ALLOW",
                "args": [],
                "comment": "",
                "includes": {
                    "arches": [
                        "amd64",
                        "x32",
                        "x86"
                    ]
                },
                "excludes": {}
            },
            {
                "names": [
                    "s390_pci_mmio_read",
                    "s390_pci_mmio_write",
                    "s390_runtime_instr"
                ],
                "action": "SCMP_ACT_ALLOW",
                "args": [],
                "comment": "",
                "includes": {
                    "arches": [
                        "s390",
                        "s390x"
                    ]
                },
                "excludes": {}
            },
            {
                "names": [
                    "open_by_handle_at"
                ],
                "action": "SCMP_ACT_ALLOW",
                "args": [],
                "comment": "",
                "includes": {
                    "caps": [
                        "CAP_DAC_READ_SEARCH"
                    ]
                },
                "excludes": {}
            },
            {
                "names": [
                    "bpf",
                    "clone",
                    "fanotify_init",
                    "lookup_dcookie",
                    "mount",
                    "name_to_handle_at",
                    "perf_event_open",
                    "quotactl",
                    "setdomainname",
                    "sethostname",
                    "setns",
                    "umount",
                    "umount2",
                    "unshare"
                ],
                "action": "SCMP_ACT_ALLOW",
                "args": [],
                "comment": "",
                "includes": {
                    "caps": [
                        "CAP_SYS_ADMIN"
                    ]
                },
                "excludes": {}
            },
            {
                "names": [
                    "clone"
                ],
                "action": "SCMP_ACT_ALLOW",
                "args": [
                    {
                        "index": 0,
                        "value": 2080505856,
                        "valueTwo": 0,
                        "op": "SCMP_CMP_MASKED_EQ"
                    }
                ],
                "comment": "",
                "includes": {},
                "excludes": {
                    "caps": [
                        "CAP_SYS_ADMIN"
                    ],
                    "arches": [
                        "s390",
                        "s390x"
                    ]
                }
            },
            {
                "names": [
                    "clone"
                ],
                "action": "SCMP_ACT_ALLOW",
                "args": [
                    {
                        "index": 1,
                        "value": 2080505856,
                        "valueTwo": 0,
                        "op": "SCMP_CMP_MASKED_EQ"
                    }
                ],
                "comment": "s390 parameter ordering for clone is different",
                "includes": {
                    "arches": [
                        "s390",
                        "s390x"
                    ]
                },
                "excludes": {
                    "caps": [
                        "CAP_SYS_ADMIN"
                    ]
                }
            },
            {
                "names": [
                    "reboot"
                ],
                "action": "SCMP_ACT_ALLOW",
                "args": [],
                "comment": "",
                "includes": {
                    "caps": [
                        "CAP_SYS_BOOT"
                    ]
                },
                "excludes": {}
            },
            {
                "names": [
                    "chroot"
                ],
                "action": "SCMP_ACT_ALLOW",
                "args": [],
                "comment": "",
                "includes": {
                    "caps": [
                        "CAP_SYS_CHROOT"
                    ]
                },
                "excludes": {}
            },
            {
                "names": [
                    "delete_module",
                    "init_module",
                    "finit_module",
                    "query_module"
                ],
                "action": "SCMP_ACT_ALLOW",
                "args": [],
                "comment": "",
                "includes": {
                    "caps": [
                        "CAP_SYS_MODULE"
                    ]
                },
                "excludes": {}
            },
            {
                "names": [
                    "acct"
                ],
                "action": "SCMP_ACT_ALLOW",
                "args": [],
                "comment": "",
                "includes": {
                    "caps": [
                        "CAP_SYS_PACCT"
                    ]
                },
                "excludes": {}
            },
            {
                "names": [
                    "kcmp",
                    "process_vm_readv",
                    "process_vm_writev",
                    "ptrace"
                ],
                "action": "SCMP_ACT_ALLOW",
                "args": [],
                "comment": "",
                "includes": {
                    "caps": [
                        "CAP_SYS_PTRACE"
                    ]
                },
                "excludes": {}
            },
            {
                "names": [
                    "iopl",
                    "ioperm"
                ],
                "action": "SCMP_ACT_ALLOW",
                "args": [],
                "comment": "",
                "includes": {
                    "caps": [
                        "CAP_SYS_RAWIO"
                    ]
                },
                "excludes": {}
            },
            {
                "names": [
                    "settimeofday",
                    "stime",
                    "clock_settime"
                ],
                "action": "SCMP_ACT_ALLOW",
                "args": [],
                "comment": "",
                "includes": {
                    "caps": [
                        "CAP_SYS_TIME"
                    ]
                },
                "excludes": {}
            },
            {
                "names": [
                    "vhangup"
                ],
                "action": "SCMP_ACT_ALLOW",
                "args": [],
                "comment": "",
                "includes": {
                    "caps": [
                        "CAP_SYS_TTY_CONFIG"
                    ]
                },
                "excludes": {}
            },
            {
                "names": [
                    "get_mempolicy",
                    "mbind",
                    "set_mempolicy"
                ],
                "action": "SCMP_ACT_ALLOW",
                "args": [],
                "comment": "",
                "includes": {
                    "caps": [
                        "CAP_SYS_NICE"
                    ]
                },
                "excludes": {}
            }
        ]
    }...
---
schema: 'pegleg/AppArmorProfile/v1'
metadata:
  schema: 'metadata/Document/v1'
  name: airship-default
  storagePolicy: 'cleartext'
  layeringDefinition:
    abstract: false
    layer: global
data:
  savePath: /etc/apparmor.d/profile_airship_default
  content: |
    #include <tunables/global>

    profile airship-default flags=(attach_disconnected,mediate_deleted) {
      #include <abstractions/base>

      network inet tcp,
      network inet udp,
      network inet icmp,

      deny network raw,

      deny network packet,

      file,
      umount,

      deny /bin/** wl,
      deny /boot/** wl,
      deny /dev/** wl,
      deny /etc/** wl,
      deny /home/** wl,
      deny /lib/** wl,
      deny /lib64/** wl,
      deny /media/** wl,
      deny /mnt/** wl,
      deny /opt/** wl,
      deny /proc/** wl,
      deny /root/** wl,
      deny /sbin/** wl,
      deny /srv/** wl,
      deny /tmp/** wl,
      deny /sys/** wl,
      deny /usr/** wl,

      audit /** w,

      deny /bin/dash mrwklx,
      deny /bin/sh mrwklx,
      deny /usr/bin/top mrwklx,

      capability chown,
      capability dac_override,
      capability setuid,
      capability setgid,
      capability net_bind_service,

      deny @{PROC}/* w,   # deny write for all files directly in /proc (not in a subdir)
      # deny write to files not in /proc/<number>/** or /proc/sys/**
      deny @{PROC}/{[^1-9],[^1-9][^0-9],[^1-9s][^0-9y][^0-9s],[^1-9][^0-9][^0-9][^0-9]*}/** w,
      deny @{PROC}/sys/[^k]** w,  # deny /proc/sys except /proc/sys/k* (effectively /proc/sys/kernel)
      deny @{PROC}/sys/kernel/{?,??,[^s][^h][^m]**} w,  # deny everything except shm* in /proc/sys/kernel/
      deny @{PROC}/sysrq-trigger rwklx,
      deny @{PROC}/mem rwklx,
      deny @{PROC}/kmem rwklx,
      deny @{PROC}/kcore rwklx,

      deny mount,

      deny /sys/[^f]*/** wklx,
      deny /sys/f[^s]*/** wklx,
      deny /sys/fs/[^c]*/** wklx,
      deny /sys/fs/c[^g]*/** wklx,
      deny /sys/fs/cg[^r]*/** wklx,
      deny /sys/firmware/** rwklx,
      deny /sys/kernel/security/** rwklx,
    }
...
---
schema: 'pegleg/AppArmorProfile/v1'
metadata:
  schema: 'metadata/Document/v1'
  name: airship-apparmor-loader
  storagePolicy: 'cleartext'
  layeringDefinition:
    abstract: false
    layer: global
data:
  savePath: /etc/apparmor.d/profile_airship_loader
  content: |
    #include <tunables/global>

    profile airship-apparmor-loader flags=(attach_disconnected,mediate_deleted) {
      #include <abstractions/base>

      network inet tcp,
      network inet udp,
      network inet icmp,

      deny network raw,

      deny network packet,

      file,
      umount,

      deny /bin/** wl,
      deny /boot/** wl,
      deny /dev/** wl,
      deny /etc/** wl,
      deny /home/** wl,
      deny /lib/** wl,
      deny /lib64/** wl,
      deny /media/** wl,
      deny /mnt/** wl,
      deny /opt/** wl,
      deny /proc/** wl,
      deny /root/** wl,
      deny /sbin/** wl,
      deny /srv/** wl,
      deny /tmp/** wl,
      deny /sys/** wl,
      deny /usr/** wl,
      audit /etc/apparmor.d/airship_* rwl,

      audit /** w,

      deny /bin/dash mrwklx,
      deny /bin/sh mrwklx,
      deny /usr/bin/top mrwklx,

      capability chown,
      # Allow Apparmor profiles to be loaded
      capability mac_admin,
      capability dac_override,
      capability setuid,
      capability setgid,

      deny @{PROC}/* w,   # deny write for all files directly in /proc (not in a subdir)
      # deny write to files not in /proc/<number>/** or /proc/sys/**
      deny @{PROC}/{[^1-9],[^1-9][^0-9],[^1-9s][^0-9y][^0-9s],[^1-9][^0-9][^0-9][^0-9]*}/** w,
      deny @{PROC}/sys/[^k]** w,  # deny /proc/sys except /proc/sys/k* (effectively /proc/sys/kernel)
      deny @{PROC}/sys/kernel/{?,??,[^s][^h][^m]**} w,  # deny everything except shm* in /proc/sys/kernel/
      deny @{PROC}/sysrq-trigger rwklx,
      deny @{PROC}/mem rwklx,
      deny @{PROC}/kmem rwklx,
      deny @{PROC}/kcore rwklx,

      deny mount,

      deny /sys/[^f]*/** wklx,
      deny /sys/f[^s]*/** wklx,
      deny /sys/fs/[^c]*/** wklx,
      deny /sys/fs/c[^g]*/** wklx,
      deny /sys/fs/cg[^r]*/** wklx,
      deny /sys/firmware/** rwklx,
      deny /sys/kernel/security/** rwklx,
    }
...
---
schema: deckhand/DataSchema/v1
metadata:
  schema: metadata/Control/v1
  name: promenade/HostSystem/v1
  labels:
    application: promenade
data:
  $schema: http://json-schema.org/schema#
  definitions:
    abs_path:
      type: string
      pattern: '^/.+$'
    systemd_unit:
      type: object
      properties:
        enable:
          type: boolean
        disable:
          type: boolean
        start:
          type: boolean
        stop:
          type: boolean
      additionalProperties: false
    apt_source_line:
      type: string
      # XXX add regex
    file:
      properties:
        path:
          $ref: '#/definitions/abs_path'
        content:
          type: string
        mode:
          type: integer
          minimum: 0
        tar_url:
          $ref: '#/definitions/url'
        tar_path:
          $ref: '#/definitions/rel_path'
        docker_image:
          $ref: '#/definitions/url'
        file_path:
          $ref: '#/definitions/abs_path'
        symlink:
          $ref: '#/definitions/abs_path'
      required:
        - mode
        - path
      oneOf:
        - type: object
          required:
            - content
        - type: object
          required:
            - symlink
        - type: object
          allOf:
            - type: object
              required:
                - tar_url
                - tar_path
        - type: object
          allOf:
            - type: object
              required:
                - docker_image
                - file_path
      additionalProperties: false

    image:
      type: string
      # XXX add regex
    package:
      type: string
      # XXX add regex
    public_key:
      type: string
      # XXX add regex
    rel_path:
      type: string
      # XXX add regex
    url:
      type: string
      # XXX add regex

  type: object

  properties:
    files:
      type: array
      items:
        type: object
        items:
          $ref: '#/definitions/file'
    systemd_units:
      type: object
      additionalProperties:
        $ref: '#/definitions/systemd_unit'
    images:
      type: object
      properties:
        coredns:
          $ref: '#/definitions/image'
        haproxy:
          $ref: '#/definitions/image'
        helm:
          type: object
          properties:
            helm:
              $ref: '#/definitions/image'
          required:
            - helm
          additionalProperties: false
        kubernetes:
          type: object
          properties:
            hyperkube:
              $ref: '#/definitions/image'
        monitoring_image:
          $ref: '#/definitions/image'
      required:
        - haproxy
        - helm
        - kubernetes
        - monitoring_image
      additionalProperties: false

    packages:
      type: object
      common:
        type: object
        properties:
          additional:
            type: array
            items:
              $ref: '#/definitions/package'
          keys:
            type: array
            items:
              $ref: '#/definitions/public_key'

          required:
            type: object
            properties:
              docker:
                $ref: '#/definitions/package'
              socat:
                $ref: '#/definitions/package'
            required:
              - docker
              - socat
            additionalProperties: false

          repositories:
            type: array
            items:
              $ref: '#/definitions/apt_source_line'

        required:
          - required
        additionalProperties: false

      genesis:
        type: object
        properties:
          additional:
            type: array
            items:
              $ref: '#/definitions/package'
          keys:
            type: array
            items:
              $ref: '#/definitions/public_key'

          required:
            type: object
            properties:
              docker:
                $ref: '#/definitions/package'
              socat:
                $ref: '#/definitions/package'
            required:
              - docker
              - socat
            additionalProperties: false

          repositories:
            type: array
            items:
              $ref: '#/definitions/apt_source_line'

        required:
          - required
        additionalProperties: false

      join:
        type: object
        properties:
          additional:
            type: array
            items:
              $ref: '#/definitions/package'
          keys:
            type: array
            items:
              $ref: '#/definitions/public_key'

          required:
            type: object
            properties:
              docker:
                $ref: '#/definitions/package'
              socat:
                $ref: '#/definitions/package'
            required:
              - docker
              - socat
            additionalProperties: false

          repositories:
            type: array
            items:
              $ref: '#/definitions/apt_source_line'

        required:
          - required
        additionalProperties: false

    validation:
      type: object
      properties:
        pod_logs:
          type: object
          properties:
            image:
              type: string
          additionalProperties: false
      additionalProperties: false

  required:
    - images
    - packages
  additionalProperties: false
...
---
schema: deckhand/DataSchema/v1
metadata:
  schema: metadata/Control/v1
  name: promenade/Genesis/v1
  labels:
    application: promenade
data:
  $schema: http://json-schema.org/schema#
  definitions:
    abs_path:
      type: string
      pattern: '^/.+$'
    hostname:
      type: string
      pattern: '^[a-z][a-z0-9-]+$'
    file:
      properties:
        path:
          $ref: '#/definitions/abs_path'
        content:
          type: string
        mode:
          type: integer
          minimum: 0
        tar_url:
          $ref: '#/definitions/url'
        tar_path:
          $ref: '#/definitions/rel_path'

      required:
        - mode
        - path
      oneOf:
        - type: object
          required:
            - content
        - type: object
          allOf:
            - type: object
              required:
                - tar_url
                - tar_path
      additionalProperties: false
    image:
      type: string
      # XXX add regex
    ip_address:
      type: string
      pattern: '^(\d|[1-9]\d|1\d\d|2([0-4]\d|5[0-5]))\.(\d|[1-9]\d|1\d\d|2([0-4]\d|5[0-5]))\.(\d|[1-9]\d|1\d\d|2([0-4]\d|5[0-5]))\.(\d|[1-9]\d|1\d\d|2([0-4]\d|5[0-5]))$'
    kubernetes_label:
      type: string
      # XXX add regex
    rel_path:
      type: string
      # XXX add regex

  type: object
  properties:
    armada:
      type: object
      properties:
        target_manifest:
          type: string
      additionalProperties: false

    apiserver:
      type: object
      properties:
        arguments:
          type: array
          items:
            type: string
        encryption:
          type: array
          items:
            type: object
            properties:
              resources:
                type: array
                items:
                  type: string
              providers:
                type: array
                items:
                  type: object
                  additionalProperties: true
      additionalProperties: false

    files:
      type: array
      items:
        $ref: '#/definitions/file'

    haproxy:
      type: object
      properties:
        run_as_user:
          type: integer
      additionalProperties: false

    hostname:
      $ref: '#/definitions/hostname'

    domain:
      type: string

    ip:
      $ref: '#/definitions/ip_address'

    labels:
      properties:
        static:
          type: array
          items:
            $ref: '#/definitions/kubernetes_label'
        dynamic:
          type: array
          items:
            $ref: '#/definitions/kubernetes_label'
      additionalProperties: false

    images:
      type: object
      properties:
        armada:
          $ref: '#/definitions/image'
        helm:
          type: object
          properties:
            tiller:
              $ref: '#/definitions/image'
          required:
            - tiller
          additionalProperties: false
        kubernetes:
          type: object
          properties:
            apiserver:
              $ref: '#/definitions/image'
            controller-manager:
              $ref: '#/definitions/image'
            etcd:
              $ref: '#/definitions/image'
            scheduler:
              $ref: '#/definitions/image'
          required:
            - apiserver
            - controller-manager
            - etcd
            - scheduler
          additionalProperties: false
      required:
        - armada
        - helm
        - kubernetes
      additionalProperties: false

  required:
    - hostname
    - ip
    - images
    - labels
  additionalProperties: false
...
---
schema: deckhand/DataSchema/v1
metadata:
  schema: metadata/Control/v1
  name: promenade/KubernetesNode/v1
  labels:
    application: promenade
data:
  $schema: http://json-schema.org/schema#
  definitions:
    hostname:
      type: string
      pattern: '^[a-z][a-z0-9-]+$'
    ip_address:
      type: string
      pattern: '^(\d|[1-9]\d|1\d\d|2([0-4]\d|5[0-5]))\.(\d|[1-9]\d|1\d\d|2([0-4]\d|5[0-5]))\.(\d|[1-9]\d|1\d\d|2([0-4]\d|5[0-5]))\.(\d|[1-9]\d|1\d\d|2([0-4]\d|5[0-5]))$'
    kubernetes_label:
      type: string
      # XXX add regex

  type: object
  properties:
    hostname:
      $ref: '#/definitions/hostname'

    ip:
      $ref: '#/definitions/ip_address'

    join_ip:
      $ref: '#/definitions/ip_address'

    labels:
      properties:
        static:
          type: array
          items:
            $ref: '#/definitions/kubernetes_label'
        dynamic:
          type: array
          items:
            $ref: '#/definitions/kubernetes_label'
      additionalProperties: false

  required:
    - ip
    - join_ip
  additionalProperties: false
...
---
schema: deckhand/DataSchema/v1
metadata:
  schema: metadata/Control/v1
  name: promenade/PKICatalog/v1
  labels:
    application: promenade
data:
  $schema: http://json-schema.org/schema#
  certificate_authorities:
    type: array
    items:
      type: object
      properties:
        description:
          type: string
        certificates:
          type: array
          items:
            type: object
            properties:
              document_name:
                type: string
              description:
                type: string
              common_name:
                type: string
              hosts:
                type: array
                items: string
              groups:
                type: array
                items: string
  keypairs:
    type: array
    items:
      type: object
      properties:
        name:
          type: string
        description:
          type: string
...
---
schema: deckhand/DataSchema/v1
metadata:
  schema: metadata/Control/v1
  name: promenade/EncryptionPolicy/v1
  labels:
    application: promenade
data:
  $schema: http://json-schema.org/schema#

  definitions:
    script_encryption:
      oneof:
        - { $ref: '#/definitions/encryption_method_gpg' }

    etcd_encryption:
      type: array
      items:
        type: object
        additionalProperties: false
        properties:
          resources:
            type: array
            items:
              type: string
          providers:
            type: array
            items:
              type: object
              additionalProperties: true
    encryption_method_gpg:
      properties:
        gpg:
          type: object
          additionalProperties: false
      required:
        - gpg
      additionalProperties: false

  properties:
    etcd:
      $ref: '#/definitions/etcd_encryption'
    scripts:
      properties:
        genesis:
          $ref: '#/definitions/script_encryption'
        join:
          $ref: '#/definitions/script_encryption'
      additionalProperties: false
...
---
schema: deckhand/DataSchema/v1
metadata:
  schema: metadata/Control/v1
  name: promenade/Docker/v1
  labels:
    application: promenade
data:
  $schema: http://json-schema.org/schema#
  type: object
  properties:
    config:
      type: object
  required:
    - config
  additionalProperties: false
...
---
schema: deckhand/DataSchema/v1
metadata:
  schema: metadata/Control/v1
  name: promenade/Kubelet/v1
  labels:
    application: promenade
data:
  $schema: http://json-schema.org/schema#
  type: object
  definitions:
    image:
      type: string
      # XXX add regex

  properties:
    images:
      type: object
      properties:
        pause:
          $ref: '#/definitions/image'
      required:
        - pause
      additionalProperties: false
    arguments:
      type: array
      items:
        type: string
  required:
    - images
  additionalProperties: false
...
---
schema: deckhand/DataSchema/v1
metadata:
  schema: metadata/Control/v1
  name: promenade/KubernetesNetwork/v1
  labels:
    application: promenade
data:
  $schema: http://json-schema.org/schema#
  definitions:
    cidr:
      type: string
      pattern: '^(\d|[1-9]\d|1\d\d|2([0-4]\d|5[0-5]))\.(\d|[1-9]\d|1\d\d|2([0-4]\d|5[0-5]))\.(\d|[1-9]\d|1\d\d|2([0-4]\d|5[0-5]))\.(\d|[1-9]\d|1\d\d|2([0-4]\d|5[0-5]))\/([0-9]|[1-2][0-9]|3[0-2])$'
    domain_name:
      type: string
      format: hostname
    domain_suffix:
      type: string
      pattern: '^\.[a-z0-9][a-z0-9-\.]*$'
    hostname:
      type: string
      format: hostname
    hostname_or_ip_address:
      anyOf:
        - $ref: '#/definitions/hostname'
        - $ref: '#/definitions/ip_address'
        - $ref: '#/definitions/domain_suffix'
    ip_address:
      type: string
      format: ipv4
    url:
      type: string
      format: uri

  type: object
  properties:
    dns:
      type: object
      properties:
        bootstrap_validation_checks:
          type: array
          items:
            $ref: '#/definitions/domain_name'
        cluster_domain:
          $ref: '#/definitions/domain_name'
        service_ip:
          $ref: '#/definitions/ip_address'
        upstream_servers:
          type: array
          items:
            $ref: '#/definitions/ip_address'
      required:
        - cluster_domain
        - service_ip
      additionalProperties: false

    etcd:
      type: object
      properties:
        container_port:
          type: integer
        haproxy_port:
          type: integer
        # NOTE(mark-burnett): No longer used.
        service_ip:
          $ref: '#/definitions/ip_address'
      required:
        - container_port
        - haproxy_port
      additionalProperties: false

    kubernetes:
      type: object
      properties:
        pod_cidr:
          $ref: '#/definitions/cidr'
        service_ip:
          $ref: '#/definitions/ip_address'
        service_cidr:
          $ref: '#/definitions/cidr'
        apiserver_port:
          type: integer
        haproxy_port:
          type: integer
      required:
        - pod_cidr
        - service_cidr
        - service_ip
        - apiserver_port
        - haproxy_port
      additionalProperties: false
    hosts_entries:
      type: array
      items:
        type: object
        properties:
          ip:
            $ref: '#/definitions/ip_address'
          names:
            type: array
            items:
              $ref: '#/definitions/hostname'

    proxy:
      type: object
      properties:
        additional_no_proxy:
          type: array
          items:
            $ref: '#/definitions/hostname_or_ip_address'
        url:
          $ref: '#/definitions/url'
      required:
        - url
      additionalFields: false

  required:
    - dns
    - kubernetes
  additionalProperties: false
...
---
schema: 'deckhand/DataSchema/v1'
metadata:
  schema: metadata/Control/v1
  name: armada/Manifest/v1
  labels:
    application: armada
data:
  $schema: 'http://json-schema.org/schema#'
  type: 'object'
  additionalProperties: true
...
---
schema: 'deckhand/DataSchema/v1'
metadata:
  schema: metadata/Control/v1
  name: armada/Chart/v1
  labels:
    application: armada
data:
  $schema: 'http://json-schema.org/schema#'
  type: 'object'
  additionalProperties: true
...
---
schema: 'deckhand/DataSchema/v1'
metadata:
  schema: metadata/Control/v1
  name: armada/ChartGroup/v1
  labels:
    application: armada
data:
  $schema: 'http://json-schema.org/schema#'
  type: 'object'
  additionalProperties: true
...
---
schema: 'deckhand/DataSchema/v1'
metadata:
  schema: metadata/Control/v1
  name: drydock/HardwareProfile/v1
  labels:
    application: drydock
data:
  $schema: 'http://json-schema.org/schema#'
  type: 'object'
  properties:
    vendor:
      type: 'string'
    generation:
      type: 'string'
    hw_version:
      type: 'string'
    bios_version:
      type: 'string'
    boot_mode:
      type: 'string'
      enum:
        - 'bios'
        - 'uefi'
    bootstrap_protocol:
      type: 'string'
      enum:
        - 'pxe'
        - 'usb'
        - 'hdd'
    pxe_interface:
      type: 'number'
    device_aliases:
      type: 'object'
      additionalProperties: true
    cpu_sets:
      type: 'object'
      additionalProperties:
        type: 'string'
    hugepages:
      type: 'object'
      additionalProperties:
        type: 'object'
        propertes:
          size:
            type: 'string'
          count:
            type: 'number'
  additionalProperties: false
...
---
schema: 'deckhand/DataSchema/v1'
metadata:
  schema: metadata/Control/v1
  name: drydock/Rack/v1
  labels:
    application: drydock
data:
  $schema: 'http://json-schema.org/schema#'
  type: 'object'
  properties:
    tor_switches:
      type: 'object'
      properties:
        mgmt_ip:
          type: 'string'
          format: 'ipv4'
        sdn_api_uri:
          type: 'string'
          format: 'uri'
    location:
      type: 'object'
      properties:
        clli:
          type: 'string'
        grid:
          type: 'string'
    local_networks:
      type: 'array'
      items:
        type: 'string'
    labels:
      type: 'object'
      additionalProperties: true
  additionalProperties: false
...
---
schema: 'deckhand/DataSchema/v1'
metadata:
  schema: metadata/Control/v1
  name: drydock/BaremetalNode/v1
  labels:
    application: drydock
data:
  $schema: 'http://json-schema.org/schema#'
  type: 'object'
  properties:
    addressing:
      type: 'array'
      items:
        type: 'object'
        properties:
          address:
            type: 'string'
          network:
            type: 'string'
    oob:
      type: 'object'
      properties:
        type:
          type: 'string'
        network:
          type: 'string'
        account:
          type: 'string'
        credetial:
          type: 'string'
      additionalProperties: true
    storage:
      type: 'object'
      properties:
        physical_devices:
          type: 'object'
          additionalProperties:
            type: 'object'
            properties:
              labels:
                type: 'object'
                additionalProperties:
                  type: 'string'
              volume_group:
                type: 'string'
              partitions:
                type: 'array'
                items:
                  type: 'object'
                  properties:
                    name:
                      type: 'string'
                    size:
                      type: 'string'
                    part_uuid:
                      type: 'string'
                    volume_group:
                      type: 'string'
                    labels:
                      type: 'object'
                      additionalProperties:
                        type: 'string'
                    bootable:
                      type: 'boolean'
                    filesystem:
                      type: 'object'
                      properties:
                        mountpoint:
                          type: 'string'
                        fstype:
                          type: 'string'
                        mount_options:
                          type: 'string'
                        fs_uuid:
                          type: 'string'
                        fs_label:
                          type: 'string'
                      additionalProperties: false
                  additionalProperties: false
        volume_groups:
          type: 'object'
          additionalProperties:
            type: 'object'
            properties:
              vg_uuid:
                type: 'string'
              logical_volumes:
                type: 'array'
                items:
                  type: 'object'
                  properties:
                    name:
                      type: 'string'
                    lv_uuid:
                      type: 'string'
                    size:
                      type: 'string'
                    filesystem:
                      type: 'object'
                      properties:
                        mountpoint:
                          type: 'string'
                        fstype:
                          type: 'string'
                        mount_options:
                          type: 'string'
                        fs_uuid:
                          type: 'string'
                        fs_label:
                          type: 'string'
    platform:
      type: 'object'
      properties:
        image:
          type: 'string'
        kernel:
          type: 'string'
        kernel_params:
          type: 'object'
          additionalProperties: true
      additionalProperties: false
    metadata:
      type: 'object'
      properties:
        tags:
          type: 'array'
          items:
            type: 'string'
        owner_data:
          type: 'object'
          additionalProperties:
            type: 'string'
        rack:
          type: 'string'
        boot_mac:
          type: 'string'
      additionalProperties: false
    host_profile:
      type: 'string'
    hardware_profile:
      type: 'string'
    primary_network:
      type: 'string'
    interfaces:
      type: 'object'
      additionalProperties:
        type: 'object'
        properties:
          device_link:
           type: 'string'
          slaves:
            type: 'array'
            items:
              type: 'string'
          networks:
            type: 'array'
            items:
              type: 'string'
  additionalProperties: false
...
---
schema: 'deckhand/DataSchema/v1'
metadata:
  schema: metadata/Control/v1
  name: drydock/Region/v1
  labels:
    application: drydock
data:
  $schema: 'http://json-schema.org/schema#'
  type: 'object'
  properties:
    tag_definitions:
      type: 'array'
      items:
        type: 'object'
        properties:
          tag:
            type: 'string'
          definition_type:
            type: 'string'
            enum:
              - 'lshw_xpath'
          definition:
            type: 'string'
        additionalProperties: false
    authorized_keys:
      type: 'array'
      items:
        type: 'string'
    repositories:
      # top level is class (e.g. apt, rpm)
      type: 'object'
      properties:
        remove_unlisted:
          type: 'boolean'
      additionalPropties:
        type: 'object'
        properties:
          repo_type:
            type: 'string'
            pattern: 'apt|rpm'
          url:
            type: 'string'
          distributions:
            type: 'array'
            items:
              type: 'string'
          subrepos:
            type: 'array'
            items:
              type: 'string'
          components:
            type: 'array'
            items:
              type: 'string'
          gpgkey:
            type: 'string'
          arches:
            type: 'array'
            items:
              type: 'string'
          options:
            type: 'object'
            additionalProperties:
              type: 'string'
        additionalProperties: false
        required:
          - 'repo_type'
          - 'url'
          - 'arches'
  additionalProperties: false
...
---
schema: 'deckhand/DataSchema/v1'
metadata:
  schema: metadata/Control/v1
  name: drydock/BootAction/v1
  labels:
    application: drydock
data:
  $schema: 'http://json-schema.org/schema#'
  type: 'object'
  additionalProperties: false
  properties:
    signaling:
      type: 'boolean'
    assets:
      type: 'array'
      items:
        type: 'object'
        additionalProperties: false
        properties:
          path:
            type: 'string'
            pattern: '^/.+'
          location:
            type: 'string'
          type:
            type: 'string'
            enum:
              - 'unit'
              - 'file'
              - 'pkg_list'
          data:
            type: 'string'
          location_pipeline:
            type: 'array'
            items:
              type: 'string'
              enum:
                - 'template'
          data_pipeline:
            type: 'array'
            items:
              type: 'string'
              enum:
                - 'base64_encode'
                - 'template'
                - 'base64_decode'
                - 'utf8_encode'
                - 'utf8_decode'
          permissions:
            type: 'string'
            pattern: '\d{3}'
        required:
          - 'type'
    node_filter:
      type: 'object'
      additionalProperties: false
      properties:
        filter_set_type:
          type: 'string'
          enum:
            - 'intersection'
            - 'union'
        filter_set:
          type: 'array'
          items:
            type: 'object'
            additionalProperties: false
            properties:
              filter_type:
                type: 'string'
                enum:
                  - 'intersection'
                  - 'union'
              node_names:
                type: 'array'
                items:
                  type: 'string'
              node_tags:
                type: 'array'
                items:
                  type: 'string'
              node_labels:
                type: 'object'
                additionalProperties: true
              rack_names:
                type: 'array'
                items:
                  type: 'string'
              rack_labels:
                type: 'object'
                additionalProperties: true
...
---
schema: 'deckhand/DataSchema/v1'
metadata:
  schema: metadata/Control/v1
  name: drydock/HostProfile/v1
  labels:
    application: drydock
data:
  $schema: 'http://json-schema.org/schema#'
  type: 'object'
  properties:
    oob:
      type: 'object'
      properties:
        type:
          type: 'string'
        network:
          type: 'string'
        account:
          type: 'string'
        credetial:
          type: 'string'
      additionalProperties: true
    storage:
      type: 'object'
      properties:
        physical_devices:
          type: 'object'
          additionalProperties:
            type: 'object'
            properties:
              labels:
                type: 'object'
                additionalProperties:
                  type: 'string'
              volume_group:
                type: 'string'
              partitions:
                type: 'array'
                items:
                  type: 'object'
                  properties:
                    name:
                      type: 'string'
                    size:
                      type: 'string'
                    part_uuid:
                      type: 'string'
                    volume_group:
                      type: 'string'
                    labels:
                      type: 'object'
                      additionalProperties:
                        type: 'string'
                    bootable:
                      type: 'boolean'
                    filesystem:
                      type: 'object'
                      properties:
                        mountpoint:
                          type: 'string'
                        fstype:
                          type: 'string'
                        mount_options:
                          type: 'string'
                        fs_uuid:
                          type: 'string'
                        fs_label:
                          type: 'string'
                      additionalProperties: false
                  additionalProperties: false
        volume_groups:
          type: 'object'
          additionalProperties:
            type: 'object'
            properties:
              vg_uuid:
                type: 'string'
              logical_volumes:
                type: 'array'
                items:
                  type: 'object'
                  properties:
                    name:
                      type: 'string'
                    lv_uuid:
                      type: 'string'
                    size:
                      type: 'string'
                    filesystem:
                      type: 'object'
                      properties:
                        mountpoint:
                          type: 'string'
                        fstype:
                          type: 'string'
                        mount_options:
                          type: 'string'
                        fs_uuid:
                          type: 'string'
                        fs_label:
                          type: 'string'
    platform:
      type: 'object'
      properties:
        image:
          type: 'string'
        kernel:
          type: 'string'
        kernel_params:
          type: 'object'
          additionalProperties: true
      additionalProperties: false
    metadata:
      type: 'object'
      properties:
        tags:
          type: 'array'
          items:
            type: 'string'
        owner_data:
          type: 'object'
          additionalProperties:
            type: 'string'
        rack:
          type: 'string'
        boot_mac:
          type: 'string'
      additionalProperties: false
    host_profile:
      type: 'string'
    hardware_profile:
      type: 'string'
    primary_network:
      type: 'string'
    interfaces:
      type: 'object'
      additionalProperties:
        type: 'object'
        properties:
          device_link:
           type: 'string'
          slaves:
            type: 'array'
            items:
              type: 'string'
          networks:
            type: 'array'
            items:
              type: 'string'
          sriov:
            type: 'object'
            properties:
              vf_count:
                type: 'number'
              trustmode:
                type: 'boolean'
  additionalProperties: false
...
---
schema: 'deckhand/DataSchema/v1'
metadata:
  schema: metadata/Control/v1
  name: drydock/NetworkLink/v1
  labels:
    application: drydock
data:
  $schema: 'http://json-schema.org/schema#'
  type: 'object'
  properties:
    bonding:
      type: 'object'
      properties:
        mode:
          type: 'string'
        hash:
          type: 'string'
        peer_rate:
          type: 'string'
        mon_rate:
          type: 'number'
        up_delay:
          type: 'number'
        down_delay:
          type: 'number'
      additionalProperties: false
    mtu:
      type: 'number'
    linkspeed:
      type: 'string'
    trunking:
      type: 'object'
      properties:
        mode:
          type: 'string'
        default_network:
          type: 'string'
      additionalProperties: false
    allowed_networks:
      type: 'array'
      items:
        type: 'string'
    labels:
      type: 'object'
      additionalProperties: true
  additionalProperties: false
...
---
schema: 'deckhand/DataSchema/v1'
metadata:
  schema: metadata/Control/v1
  name: drydock/Network/v1
  labels:
    application: drydock
data:
  $schema: 'http://json-schema.org/schema#'
  type: 'object'
  properties:
    cidr:
      type: 'string'
    ranges:
      type: 'array'
      items:
        type: 'object'
        properties:
          type:
            type: 'string'
          start:
            type: 'string'
            format: 'ipv4'
          end:
            type: 'string'
            format: 'ipv4'
        additionalProperties: false
    dns:
      type: 'object'
      properties:
        domain:
          type: 'string'
        servers:
          type: 'string'
      additionalProperties: false
    dhcp_relay:
      type: 'object'
      properties:
        self_ip:
          type: 'string'
          format: 'ipv4'
        upstream_target:
          type: 'string'
          format: 'ipv4'
      additionalProperties: false
    mtu:
      type: 'number'
    vlan:
      type: 'string'
    routedomain:
      type: 'string'
    routes:
      type: 'array'
      items:
        type: 'object'
        properties:
          subnet:
            type: 'string'
          gateway:
            type: 'string'
            format: 'ipv4'
          metric:
            type: 'number'
          routedomain:
            type: 'string'
        additionalProperties: false
    labels:
      type: 'object'
      additionalProperties: true
  additionalProperties: false
...
---
schema: 'deckhand/DataSchema/v1'
metadata:
  schema: metadata/Control/v1
  name: pegleg/AccountCatalogue/v1
data:
  $schema: 'http://json-schema.org/schema#'
  type: object
  properties:
    ucp:
      type: object
      properties:
        postgres:
          type: object
          properties:
            admin:
              type: object
              properties:
                username:
                  type: string
        oslo_db:
          type: object
          properties:
            admin:
              type: object
              properties:
                username:
                  type: string
        oslo_messaging:
          type: object
          properties:
            admin:
              type: object
              properties:
                username:
                  type: string
        keystone:
          type: object
          properties:
            admin:
              type: object
              properties:
                region_name:
                  type: string
                username:
                  type: string
                project_name:
                  type: string
                user_domain_name:
                  type: string
                project_domain_name:
                  type: string
            oslo_messaging:
              type: object
              properties:
                username:
                  type: string
            oslo_db:
              type: object
              properties:
                username:
                  type: string
                database:
                  type: string
        promenade:
          type: object
          properties:
            keystone:
              type: object
              properties:
                region_name:
                  type: string
                role:
                  type: string
                project_name:
                  type: string
                project_domain_name:
                  type: string
                user_domain_name:
                  type: string
                username:
                  type: string
        drydock:
          type: object
          properties:
            keystone:
              type: object
              properties:
                region_name:
                  type: string
                role:
                  type: string
                project_name:
                  type: string
                project_domain_name:
                  type: string
                user_domain_name:
                  type: string
                username:
                  type: string
            postgres:
              type: object
              properties:
                username:
                  type: string
                database:
                  type: string
        shipyard:
          type: object
          properties:
            keystone:
              type: object
              properties:
                region_name:
                  type: string
                role:
                  type: string
                project_name:
                  type: string
                project_domain_name:
                  type: string
                user_domain_name:
                  type: string
                username:
                  type: string
            postgres:
              type: object
              properties:
                username:
                  type: string
                database:
                  type: string
        airflow:
          type: object
          properties:
            postgres:
              type: object
              properties:
                username:
                  type: string
                database:
                  type: string
            oslo_messaging:
              type: object
              properties:
                username:
                  type: string
        maas:
          type: object
          properties:
            admin:
              type: object
              properties:
                username:
                  type: string
                email:
                  type: string
            postgres:
              type: object
              properties:
                username:
                  type: string
                database:
                  type: string
        barbican:
          type: object
          properties:
            keystone:
              type: object
              properties:
                region_name:
                  type: string
                role:
                  type: string
                project_name:
                  type: string
                project_domain_name:
                  type: string
                user_domain_name:
                  type: string
                username:
                  type: string
            oslo_db:
              type: object
              properties:
                username:
                  type: string
                database:
                  type: string
            oslo_messaging:
              type: object
              properties:
                username:
                  type: string
        armada:
          type: object
          properties:
            keystone:
              type: object
              properties:
                project_domain_name:
                  type: string
                project_name:
                  type: string
                region_name:
                  type: string
                role:
                  type: string
                user_domain_name:
                  type: string
                username:
                  type: string
        deckhand:
          type: object
          properties:
            keystone:
              type: object
              properties:
                region_name:
                  type: string
                role:
                  type: string
                project_name:
                  type: string
                project_domain_name:
                  type: string
                user_domain_name:
                  type: string
                username:
                  type: string
            postgres:
              type: object
              properties:
                username:
                  type: string
                database:
                  type: string
    ceph:
      type: object
      properties:
        swift:
          type: object
          properties:
            keystone:
              type: object
              properties:
                role:
                  type: string
                region_name:
                  type: string
                username:
                  type: string
                project_name:
                  type: string
                user_domain_name:
                  type: string
                project_domain_name:
                  type: string
    osh:
      type: object
      properties:
        keystone:
          type: object
          properties:
            admin:
              type: object
              properties:
                region_name:
                  type: string
                username:
                  type: string
                project_name:
                  type: string
                user_domain_name:
                  type: string
                project_domain_name:
                  type: string
            oslo_messaging:
              type: object
              properties:
                admin:
                  type: object
                  properties:
                    username:
                      type: string
                keystone:
                  type: object
                  properties:
                    username:
                      type: string
            oslo_db:
              type: object
              properties:
                username:
                  type: string
                database:
                  type: string
        cinder:
          type: object
          properties:
            cinder:
              type: object
              properties:
                role:
                  type: string
                region_name:
                  type: string
                username:
                  type: string
                project_name:
                  type: string
                user_domain_name:
                  type: string
                project_domain_name:
                  type: string
            oslo_messaging:
              type: object
              properties:
                admin:
                  type: object
                  properties:
                    username:
                      type: string
                cinder:
                  type: object
                  properties:
                    username:
                      type: string
            oslo_db:
              type: object
              properties:
                username:
                  type: string
                database:
                  type: string
        glance:
          type: object
          properties:
            glance:
              type: object
              properties:
                role:
                  type: string
                region_name:
                  type: string
                username:
                  type: string
                project_name:
                  type: string
                user_domain_name:
                  type: string
                project_domain_name:
                  type: string
            oslo_messaging:
              type: object
              properties:
                admin:
                  type: object
                  properties:
                    username:
                      type: string
                glance:
                  type: object
                  properties:
                    username:
                      type: string
            oslo_db:
              type: object
              properties:
                username:
                  type: string
                database:
                  type: string
            ceph_object_store:
              type: object
              properties:
                username:
                  type: string
        heat:
          type: object
          properties:
            heat:
              type: object
              properties:
                role:
                  type: string
                region_name:
                  type: string
                username:
                  type: string
                project_name:
                  type: string
                user_domain_name:
                  type: string
                project_domain_name:
                  type: string
            heat_trustee:
              type: object
              properties:
                role:
                  type: string
                region_name:
                  type: string
                username:
                  type: string
                project_name:
                  type: string
                user_domain_name:
                  type: string
                project_domain_name:
                  type: string
            heat_stack_user:
              type: object
              properties:
                role:
                  type: string
                region_name:
                  type: string
                username:
                  type: string
                project_name:
                  type: string
                user_domain_name:
                  type: string
                project_domain_name:
                  type: string
            oslo_db:
              type: object
              properties:
                username:
                  type: string
                database:
                  type: string
            oslo_messaging:
              type: object
              properties:
                admin:
                  type: object
                  properties:
                    username:
                      type: string
                heat:
                  type: object
                  properties:
                    username:
                      type: string
        swift:
          type: object
          properties:
            swift:
              type: object
              properties:
                role:
                  type: string
                region_name:
                  type: string
                username:
                  type: string
                project_name:
                  type: string
                user_domain_name:
                  type: string
                project_domain_name:
                  type: string
        oslo_db:
          type: object
          properties:
            admin:
              type: object
              properties:
                username:
                  type: string
        neutron:
          type: object
          properties:
            neutron:
              type: object
              properties:
                role:
                  type: string
                region_name:
                  type: string
                username:
                  type: string
                project_name:
                  type: string
                user_domain_name:
                  type: string
                project_domain_name:
                  type: string
            oslo_messaging:
              type: object
              properties:
                admin:
                  type: object
                  properties:
                    username:
                      type: string
                neutron:
                  type: object
                  properties:
                    username:
                      type: string
            oslo_db:
              type: object
              properties:
                username:
                  type: string
                database:
                  type: string
        nova:
          type: object
          properties:
            nova:
              type: object
              properties:
                role:
                  type: string
                region_name:
                  type: string
                username:
                  type: string
                project_name:
                  type: string
                user_domain_name:
                  type: string
                project_domain_name:
                  type: string
            placement:
              type: object
              properties:
                role:
                  type: string
                region_name:
                  type: string
                username:
                  type: string
                project_name:
                  type: string
                user_domain_name:
                  type: string
                project_domain_name:
                  type: string
            oslo_messaging:
              type: object
              properties:
                admin:
                  type: object
                  properties:
                    username:
                      type: string
                nova:
                  type: object
                  properties:
                    username:
                      type: string
            oslo_db:
              type: object
              properties:
                username:
                  type: string
                database:
                  type: string
            oslo_db_api:
              type: object
              properties:
                username:
                  type: string
                database:
                  type: string
            oslo_db_cell0:
              type: object
              properties:
                username:
                  type: string
                database:
                  type: string
        horizon:
          type: object
          properties:
            oslo_db:
              type: object
              properties:
                username:
                  type: string
                database:
                  type: string
    osh_infra:
      type: object
      properties:
        grafana:
          type: object
          properties:
            admin:
              type: object
              properties:
                username:
                  type: string
            oslo_db:
              type: object
              properties:
                username:
                  type: string
                database:
                  type: string
            oslo_db_session:
              type: object
              properties:
                username:
                  type: string
                database:
                  type: string
        elasticsearch:
          type: object
          properties:
            admin:
              type: object
              properties:
                username:
                  type: string
        oslo_db:
          type: object
          properties:
            admin:
              type: object
              properties:
                username:
                  type: string
        prometheus_openstack_exporter:
          type: object
          properties:
            user:
              type: object
              properties:
                username:
                  type: string
        nagios:
          type: object
          properties:
            admin:
              type: object
              properties:
                username:
                  type: string
...
---
schema: 'deckhand/DataSchema/v1'
metadata:
  schema: metadata/Control/v1
  name: pegleg/SoftwareVersions/v1
data:
  $schema: 'http://json-schema.org/schema#'
  type: object
  properties:
    charts:
      type: object
      properties:
        kubernetes:
          type: object
          properties:
            calico:
              type: object
              properties:
                etcd:
                  type: object
                  properties:
                    type:
                      type: string
                    location:
                      type: string
                    subpath:
                      type: string
                    reference:
                      type: string
                etcd-htk:
                  type: object
                  properties:
                    type:
                      type: string
                    location:
                      type: string
                    subpath:
                      type: string
                    reference:
                      type: string
                calico:
                  type: object
                  properties:
                    type:
                      type: string
                    location:
                      type: string
                    subpath:
                      type: string
                    reference:
                      type: string
            apiserver:
              type: object
              properties:
                type:
                  type: string
                location:
                  type: string
                subpath:
                  type: string
                reference:
                  type: string
            apiserver-htk:
              type: object
              properties:
                type:
                  type: string
                location:
                  type: string
                subpath:
                  type: string
                reference:
                  type: string
            controller-manager:
              type: object
              properties:
                type:
                  type: string
                location:
                  type: string
                subpath:
                  type: string
                reference:
                  type: string
            controller-manager-htk:
              type: object
              properties:
                type:
                  type: string
                location:
                  type: string
                subpath:
                  type: string
                reference:
                  type: string
            coredns:
              type: object
              properties:
                type:
                  type: string
                location:
                  type: string
                subpath:
                  type: string
                reference:
                  type: string
            coredns-htk:
              type: object
              properties:
                type:
                  type: string
                location:
                  type: string
                subpath:
                  type: string
                reference:
                  type: string
            haroxy:
              type: object
              properties:
                type:
                  type: string
                location:
                  type: string
                subpath:
                  type: string
            haroxy-htk:
              type: object
              properties:
                type:
                  type: string
                location:
                  type: string
                subpath:
                  type: string
            etcd:
              type: object
              properties:
                type:
                  type: string
                location:
                  type: string
                subpath:
                  type: string
                reference:
                  type: string
            etcd-htk:
              type: object
              properties:
                type:
                  type: string
                location:
                  type: string
                subpath:
                  type: string
                reference:
                  type: string
            ingress:
              type: object
              properties:
                type:
                  type: string
                location:
                  type: string
                subpath:
                  type: string
                reference:
                  type: string
            ingress-htk:
              type: object
              properties:
                type:
                  type: string
                location:
                  type: string
                subpath:
                  type: string
                reference:
                  type: string
            proxy:
              type: object
              properties:
                type:
                  type: string
                location:
                  type: string
                subpath:
                  type: string
                reference:
                  type: string
            proxy-htk:
              type: object
              properties:
                type:
                  type: string
                location:
                  type: string
                subpath:
                  type: string
                reference:
                  type: string
            scheduler:
              type: object
              properties:
                type:
                  type: string
                location:
                  type: string
                subpath:
                  type: string
                reference:
                  type: string
            scheduler-htk:
              type: object
              properties:
                type:
                  type: string
                location:
                  type: string
                subpath:
                  type: string
                reference:
                  type: string
        osh_infra:
          type: object
          properties:
            elasticsearch:
              type: object
              properties:
                type:
                  type: string
                location:
                  type: string
                subpath:
                  type: string
                reference:
                  type: string
            fluentbit:
              type: object
              properties:
                type:
                  type: string
                location:
                  type: string
                subpath:
                  type: string
                reference:
                  type: string
            fluentd:
              type: object
              properties:
                type:
                  type: string
                location:
                  type: string
                subpath:
                  type: string
                reference:
                  type: string
            kibana:
              type: object
              properties:
                type:
                  type: string
                location:
                  type: string
                subpath:
                  type: string
                reference:
                  type: string
            prometheus:
              type: object
              properties:
                type:
                  type: string
                location:
                  type: string
                subpath:
                  type: string
                reference:
                  type: string
            prometheus_node_exporter:
              type: object
              properties:
                type:
                  type: string
                location:
                  type: string
                subpath:
                  type: string
                reference:
                  type: string
            prometheus_kube_state_metrics:
              type: object
              properties:
                type:
                  type: string
                location:
                  type: string
                subpath:
                  type: string
                reference:
                  type: string
            prometheus_alertmanager:
              type: object
              properties:
                type:
                  type: string
                location:
                  type: string
                subpath:
                  type: string
                reference:
                  type: string
            grafana:
              type: object
              properties:
                type:
                  type: string
                location:
                  type: string
                subpath:
                  type: string
                reference:
                  type: string
            prometheus_openstack_exporter:
              type: object
              properties:
                type:
                  type: string
                location:
                  type: string
                subpath:
                  type: string
                reference:
                  type: string
            nagios:
              type: object
              properties:
                type:
                  type: string
                location:
                  type: string
                subpath:
                  type: string
                reference:
                  type: string
        osh:
          type: object
          properties:
            barbican:
              type: object
              properties:
                type:
                  type: string
                location:
                  type: string
                subpath:
                  type: string
                reference:
                  type: string
            cinder:
              type: object
              properties:
                type:
                  type: string
                location:
                  type: string
                subpath:
                  type: string
                reference:
                  type: string
            glance:
              type: object
              properties:
                type:
                  type: string
                location:
                  type: string
                subpath:
                  type: string
                reference:
                  type: string
            heat:
              type: object
              properties:
                type:
                  type: string
                location:
                  type: string
                subpath:
                  type: string
                reference:
                  type: string
            horizon:
              type: object
              properties:
                type:
                  type: string
                location:
                  type: string
                subpath:
                  type: string
                reference:
                  type: string
            ingress:
              type: object
              properties:
                type:
                  type: string
                location:
                  type: string
                subpath:
                  type: string
                reference:
                  type: string
            keystone:
              type: object
              properties:
                type:
                  type: string
                location:
                  type: string
                subpath:
                  type: string
                reference:
                  type: string
            libvirt:
              type: object
              properties:
                type:
                  type: string
                location:
                  type: string
                subpath:
                  type: string
                reference:
                  type: string
            mariadb:
              type: object
              properties:
                type:
                  type: string
                location:
                  type: string
                subpath:
                  type: string
                reference:
                  type: string
            memcached:
              type: object
              properties:
                type:
                  type: string
                location:
                  type: string
                subpath:
                  type: string
                reference:
                  type: string
            neutron:
              type: object
              properties:
                type:
                  type: string
                location:
                  type: string
                subpath:
                  type: string
                reference:
                  type: string
            nova:
              type: object
              properties:
                type:
                  type: string
                location:
                  type: string
                subpath:
                  type: string
                reference:
                  type: string
            openvswitch:
              type: object
              properties:
                type:
                  type: string
                location:
                  type: string
                subpath:
                  type: string
                reference:
                  type: string
            rabbitmq:
              type: object
              properties:
                type:
                  type: string
                location:
                  type: string
                subpath:
                  type: string
                reference:
                  type: string
        ucp:
          type: object
          properties:
            armada:
              type: object
              properties:
                type:
                  type: string
                location:
                  type: string
                subpath:
                  type: string
                reference:
                  type: string
            barbican:
              type: object
              properties:
                type:
                  type: string
                location:
                  type: string
                subpath:
                  type: string
                reference:
                  type: string
            ceph-mon:
              type: object
              properties:
                type:
                  type: string
                location:
                  type: string
                subpath:
                  type: string
                reference:
                  type: string
            ceph-osd:
              type: object
              properties:
                type:
                  type: string
                location:
                  type: string
                subpath:
                  type: string
                reference:
                  type: string
            ceph-client:
              type: object
              properties:
                type:
                  type: string
                location:
                  type: string
                subpath:
                  type: string
                reference:
                  type: string
            ceph-provisioners:
              type: object
              properties:
                type:
                  type: string
                location:
                  type: string
                subpath:
                  type: string
                reference:
                  type: string
            ceph-rgw:
              type: object
              properties:
                type:
                  type: string
                location:
                  type: string
                subpath:
                  type: string
                reference:
                  type: string
            tenant-ceph-mon:
              type: object
              properties:
                fluentbit:
                  type: string
                ceph_bootstrap:
                  type: string
                dep_check:
                  type: string
                ceph_mon:
                  type: string
                ceph_config_helper:
                  type: string
                ceph_mon_check:
                  type: string
                image_repo_sync:
                  type: string
            tenant-ceph-osd:
              type: object
              properties:
                fluentbit:
                  type: string
                ceph_bootstrap:
                  type: string
                dep_check:
                  type: string
                ceph_osd:
                  type: string
                image_repo_sync:
                  type: string
            tenant-ceph-client:
              type: object
              properties:
                ceph_bootstrap:
                  type: string
                dep_check:
                  type: string
                ceph_mds:
                  type: string
                ceph_mgr:
                  type: string
                ceph_config_helper:
                  type: string
                ceph_rbd_pool:
                  type: string
                image_repo_sync:
                  type: string
            tenant-ceph-provisioners:
              type: object
              properties:
                ceph_bootstrap:
                  type: string
                ceph_cephfs_provisioner:
                  type: string
                ceph_config_helper:
                  type: string
                ceph_rbd_provisioner:
                  type: string
                dep_check:
                  type: string
                image_repo_sync:
                  type: string
            tenant-ceph-rgw:
              type: object
              properties:
                ceph_config_helper:
                  type: string
                ceph_rgw:
                  type: string
                dep_check:
                  type: string
                image_repo_sync:
                  type: string
                rgw_s3_admin:
                  type: string
                ks_endpoints:
                  type: string
                ks_service:
                  type: string
                ks_user:
                  type: string
            deckhand:
              type: object
              properties:
                type:
                  type: string
                location:
                  type: string
                subpath:
                  type: string
                reference:
                  type: string
            drydock:
              type: object
              properties:
                type:
                  type: string
                location:
                  type: string
                subpath:
                  type: string
                reference:
                  type: string
            ingress:
              type: object
              properties:
                type:
                  type: string
                location:
                  type: string
                subpath:
                  type: string
                reference:
                  type: string
            postgresql:
              type: object

              properties:
                type:
                  type: string
                location:
                  type: string
                subpath:
                  type: string
                reference:
                  type: string
            promenade:
              type: object
              properties:
                type:
                  type: string
                location:
                  type: string
                subpath:
                  type: string
                reference:
                  type: string
            keystone:
              type: object
              properties:
                type:
                  type: string
                location:
                  type: string
                subpath:
                  type: string
                reference:
                  type: string
            maas:
              type: object
              properties:
                type:
                  type: string
                location:
                  type: string
                subpath:
                  type: string
                reference:
                  type: string
            mariadb:
              type: object
              properties:
                type:
                  type: string
                location:
                  type: string
                subpath:
                  type: string
                reference:
                  type: string
            memcached:
              type: object
              properties:
                type:
                  type: string
                location:
                  type: string
                subpath:
                  type: string
                reference:
                  type: string
            rabbitmq:
              type: object
              properties:
                type:
                  type: string
                location:
                  type: string
                subpath:
                  type: string
                reference:
                  type: string
            rabbitmq-etcd:
              type: object
              properties:
                type:
                  type: string
                location:
                  type: string
                subpath:
                  type: string
                reference:
                  type: string
            shipyard:
              type: object
              properties:
                type:
                  type: string
                location:
                  type: string
                subpath:
                  type: string
                reference:
                  type: string
            tiller:
              type: object
              properties:
                type:
                  type: string
                location:
                  type: string
                subpath:
                  type: string
                reference:
                  type: string
    files:
      type: object
      properties:
        kubelet:
          type: string
    images:
      type: object
      properties:
        ucp:
          type: object
          properties:
            armada:
              type: object
              properties:
                api:
                  type: string
                dep_check:
                  type: string
                ks_endpoints:
                  type: string
                ks_service:
                  type: string
                ks_user:
                  type: string
                helm:
                  type: string
                tiller:
                  type: string
            promenade:
              type: object
              properties:
                dep_check:
                  type: string
                promenade:
                  type: string
                ks_user:
                  type: string
                ks_service:
                  type: string
                ks_endpoints:
                  type: string
            deckhand:
              type: object
              properties:
                deckhand:
                  type: string
                dep_check:
                  type: string
                db_init:
                  type: string
                db_sync:
                  type: string
                ks_endpoints:
                  type: string
                ks_service:
                  type: string
                ks_user:
                  type: string
            barbican:
              type: object
              properties:
                bootstrap:
                  type: string
                dep_check:
                  type: string
                scripted_test:
                  type: string
                db_init:
                  type: string
                barbican_db_sync:
                  type: string
                db_drop:
                  type: string
                ks_endpoints:
                  type: string
                ks_service:
                  type: string
                ks_user:
                  type: string
                barbican_api:
                  type: string
            drydock:
              type: object
              properties:
                drydock:
                  type: string
                dep_check:
                  type: string
                ks_endpoints:
                  type: string
                ks_service:
                  type: string
                ks_user:
                  type: string
                drydock_db_init:
                  type: string
                drydock_db_sync:
                  type: string
            shipyard:
              type: object
              properties:
                airflow:
                  type: string
                shipyard:
                  type: string
                dep_check:
                  type: string
                shipyard_db_init:
                  type: string
                shipyard_db_sync:
                  type: string
                airflow_db_init:
                  type: string
                airflow_db_sync:
                  type: string
                ks_user:
                  type: string
                ks_service:
                  type: string
                ks_endpoints:
                  type: string
            maas:
              type: object
              properties:
                db_init:
                  type: string
                db_sync:
                  type: string
                maas_rack:
                  type: string
                maas_region:
                  type: string
                bootstrap:
                  type: string
                export_api_key:
                  type: string
                maas_cache:
                  type: string
                dep_check:
                  type: string
            keystone:
              type: object
              properties:
                keystone_bootstrap:
                  type: string
                test:
                  type: string
                db_init:
                  type: string
                keystone_db_sync:
                  type: string
                db_drop:
                  type: string
                keystone_fernet_setup:
                  type: string
                keystone_fernet_rotate:
                  type: string
                keystone_credential_setup:
                  type: string
                keystone_credential_rotate:
                  type: string
                keystone_api:
                  type: string
                dep_check:
                  type: string
            tiller:
              type: object
              properties:
                tiller:
                  type: string
            mariadb:
              type: object
              properties:
                mariadb:
                  type: string
                dep_check:
                  type: string
            postgresql:
              type: object
              properties:
                postgresql:
                  type: string
                dep_check:
                  type: string
            memcached:
              type: object
              properties:
                memcached:
                  type: string
                dep_check:
                  type: string
            rabbitmq:
              type: object
              properties:
                rabbitmq:
                  type: string
                dep_check:
                  type: string
        ceph:
          type: object
          properties:
            ceph-mon:
              type: object
              properties:
                fluentbit:
                  type: string
                ceph_bootstrap:
                  type: string
                dep_check:
                  type: string
                ceph_mon:
                  type: string
                ceph_config_helper:
                  type: string
                ceph_mon_check:
                  type: string
                image_repo_sync:
                  type: string
            ceph-osd:
              type: object
              properties:
                fluentbit:
                  type: string
                ceph_bootstrap:
                  type: string
                dep_check:
                  type: string
                ceph_osd:
                  type: string
                image_repo_sync:
                  type: string
            ceph-client:
              type: object
              properties:
                ks_endpoints:
                  type: string
                ks_service:
                  type: string
                ks_user:
                  type: string
                ceph_bootstrap:
                  type: string
                dep_check:
                  type: string
                ceph_mds:
                  type: string
                ceph_mgr:
                  type: string
                ceph_rgw:
                  type: string
                ceph_config_helper:
                  type: string
                ceph_rbd_pool:
                  type: string
                ceph_rbd_provisioner:
                  type: string
                ceph_cephfs_provisioner:
                  type: string
                image_repo_sync:
                  type: string
            ceph-provisioners:
              type: object
              properties:
                ceph_bootstrap:
                  type: string
                ceph_cephfs_provisioner:
                  type: string
                ceph_config_helper:
                  type: string
                ceph_rbd_provisioner:
                  type: string
                dep_check:
                  type: string
                image_repo_sync:
                  type: string
            ceph-rgw:
              type: object
              properties:
                ceph_config_helper:
                  type: string
                ceph_rgw:
                  type: string
                dep_check:
                  type: string
                image_repo_sync:
                  type: string
                rgw_s3_admin:
                  type: string
                ks_endpoints:
                  type: string
                ks_service:
                  type: string
                ks_user:
                  type: string
        kubernetes:
          type: object
          properties:
            apiserver:
              type: object
              properties:
                anchor:
                  type: string
                apiserver:
                  type: string
                dep_check:
                  type: string
            controller-manager:
              type: object
              properties:
                anchor:
                  type: string
                controller_manager:
                  type: string
                dep_check:
                  type: string
            coredns:
              type: object
              properties:
                coredns:
                  type: string
            haproxy:
              type: object
              properties:
                haproxy:
                  type: string
                anchor:
                  type: string
            etcd:
              type: object
              properties:
                etcd:
                  type: string
                etcdctl:
                  type: string
            kubectl:
              type: string
            pause:
              type: string
            scheduler:
              type: object
              properties:
                anchor:
                  type: string
                scheduler:
                  type: string
            proxy:
              type: object
              properties:
                proxy:
                  type: string
        calico:
          type: object
          properties:
            etcd:
              type: object
              properties:
                etcd:
                  type: string
                etcdctl:
                  type: string
            calico:
              type: object
              properties:
                cni:
                  type: string
                ctl:
                  type: string
                node:
                  type: string
                policy_controller:
                  type: string
    packages:
      type: object
      properties:
        repositories:
          type: object
          additionalProperties:
            type: object
            properties:
              name:
                type: string
              url:
                type: string
              distributions:
                type: array
                items:
                  type: string
              components:
                type: array
                items:
                  type: string
              gpgkey:
                type: string
        named:
          type: object
          properties:
            docker:
              type: string
            socat:
              type: string
        unnamed:
          type: array
          items:
            type: string
...
---
schema: 'deckhand/DataSchema/v1'
metadata:
  schema: metadata/Control/v1
  name: pegleg/CommonAddresses/v1
data:
  $schema: 'http://json-schema.org/schema#'
  type: object
  properties:
    calico:
      type: object
      properties:
        ip_autodetection_method:
          type: string
        etcd:
          type: object
          properties:
            service_ip:
              type: string
    dns:
      type: object
      properties:
        cluster_domain:
          type: string
        service_ip:
          type: string
        upstream_servers:
          type: array
          items:
            type: string
        upstream_servers_joined:
          type: string
    genesis:
      type: object
      properties:
        hostname:
          type: string
        ip:
          type: string
    bootstrap:
      type: object
      properties:
        ip:
          type: string
    kubernetes:
      type: object
      properties:
        api_service_ip:
          type: string
        etcd_service_ip:
          type: string
        pod_cidr:
          type: string
        service_cidr:
          type: string
        apiserver_port:
          type: number
        haproxy_port:
          type: number
        service_node_port_range:
          type: string
    etcd:
      type: object
      properties:
        container_port:
          type: number
        haproxy_port:
          type: number
    masters:
      type: array
      items:
        type: object
        properties:
          hostname:
            type: string
    node_ports:
      type: object
      properties:
        drydock_api:
          type: number
        maas_api:
          type: number
        maas_proxy:
          type: number
        shipyard_api:
          type: number
        airflow_web:
          type: number
    ntp:
      type: object
      properties:
        servers_joined:
          type: string
    storage:
      type: object
      properties:
        ceph:
          type: object
          properties:
            public_cidr:
              type: string
            cluster_cidr:
              type: string
    openvswitch:
      type: object
      properties:
        external_iface:
          type: string
    neutron:
      type: object
      properties:
        tunnel_device:
          type: string
        external_iface:
          type: string
...
---
schema: deckhand/DataSchema/v1
metadata:
  schema: metadata/Control/v1
  name: pegleg/SiteDefinition/v1
data:
  $schema: http://json-schema.org/schema#

  type: object
  properties:
    repositories:
      type: object
      additionalProperties:
        type: object
        properties:
          revision:
            type: string
          url:
            type: string
        required:
          - revision
          - url

    site_type:
      type: string
  required:
    - site_type
  additionalProperties: false
...
---
schema: 'deckhand/DataSchema/v1'
metadata:
  schema: metadata/Control/v1
  name: pegleg/AppArmorProfile/v1
  labels:
    application: pegleg
data:
  $schema: 'http://json-schema.org/schema#'
  type: 'object'
  additionalProperties: false
  properties:
    savePath:
      type: 'string'
    content:
      type: 'string'
  required: ['savePath', 'content']
...
---
schema: deckhand/DataSchema/v1
metadata:
  schema: metadata/Control/v1
  name: pegleg/Script/v1
data:
  $schema: http://json-schema.org/schema#
  type: string
...
---
schema: 'deckhand/DataSchema/v1'
metadata:
  schema: metadata/Control/v1
  name: pegleg/EndpointCatalogue/v1
data:
  $schema: 'http://json-schema.org/schema#'
  type: 'object'
  # Namespace the list of endpoints
  additionalProperties:
    type: 'object'
    additionalProperties:
      type: 'object'
      properties:
        namespace:
          oneOf:
            - type: string
            - type: "null"
        name:
          type: string
        statefulset:
          type: object
          properties:
            replicas:
              type: number
            name:
              type: string
        auth:
          type: object
        hosts:
          type: object
          properties:
            data:
              type: string
            default:
              type: string
            discovery:
              type: string
            public:
              type: string
            internal:
              type: string
          additionalProperties:
            type: string
        host_fqdn_override:
          oneOf:
            - type: object
              properties:
                default:
                  oneOf:
                    - type: string
                    - type: "null"
                    - type: object
                      properties:
                        host:
                          type: string
                        tls:
                          type: object
                          properties:
                            crt:
                              type: string
                            ca:
                              type: string
                            key:
                              type: string
                      additionalProperties:
                        type: string
                public:
                  oneOf:
                    - type: string
                    - type: "null"
                    - type: object
                      properties:
                        host:
                          type: string
                        tls:
                          type: object
                          properties:
                            crt:
                              type: string
                            ca:
                              type: string
                            key:
                              type: string
                      additionalProperties:
                        type: string
                admin:
                  oneOf:
                    - type: string
                    - type: "null"
                    - type: object
                      properties:
                        host:
                          type: string
                        tls:
                          type: object
                          properties:
                            crt:
                              type: string
                            ca:
                              type: string
                            key:
                              type: string
                      additionalProperties:
                        type: string
                internal:
                  oneOf:
                    - type: string
                    - type: "null"
                    - type: object
                      properties:
                        host:
                          type: string
                        tls:
                          type: object
                          properties:
                            crt:
                              type: string
                            ca:
                              type: string
                            key:
                              type: string
                      additionalProperties:
                        type: string
              additionalProperties:
                type: string
            - type: "null"
        path:
          oneOf:
            - type: object
              properties:
                default:
                  oneOf:
                    - type: string
                    - type: "null"
                public:
                  type: string
                internal:
                  type: string
              additionalProperties:
                type: string
            - type: string
        scheme:
          oneOf:
            - type: object
              properties:
                default:
                  type: string
                public:
                  type: string
                internal:
                  type: string
              additionalProperties:
                type: string
            - type: string
        port:
          type: object
          additionalProperties:
            type: object
            properties:
              default:
                type: number
              public:
                type: number
              internal:
                type: number
            additionalProperties:
              type: number
...
---
schema: 'deckhand/DataSchema/v1'
metadata:
  schema: metadata/Control/v1
  name: pegleg/SeccompProfile/v1
  labels:
    application: pegleg
data:
  $schema: 'http://json-schema.org/schema#'
  type: 'object'
  additionalProperties: false
  properties:
    seccompDirPath:
      type: 'string'
    savePath:
      type: 'string'
    content:
      type: 'string'
  required: ['seccompDirPath', 'savePath', 'content']
...
---
schema: 'deckhand/DataSchema/v1'
metadata:
  schema: metadata/Control/v1
  name: pegleg/CommonSoftwareConfig/v1
data:
  $schema: 'http://json-schema.org/schema#'
  type: object
  properties:
    osh:
      type: object
      properties:
        region_name:
          type: string
...
---
schema: 'deckhand/DataSchema/v1'
metadata:
  schema: metadata/Control/v1
  name: shipyard/DeploymentStrategy/v1
  labels:
    application: shipyard
data:
  $schema: 'http://json-schema.org/schema#'
  type: 'object'
  required:
    - groups
  properties:
    groups:
      type: 'array'
      minItems: 0
      items:
        type: 'object'
        required:
          - name
          - critical
          - depends_on
          - selectors
        properties:
          name:
            type: 'string'
            minLength: 1
          critical:
            type: 'boolean'
          depends_on:
            type: 'array'
            minItems: 0
            items:
              type: 'string'
          selectors:
            type: 'array'
            minItems: 0
            items:
              type: 'object'
              minProperties: 1
              properties:
                node_names:
                  type: 'array'
                  items:
                    type: 'string'
                node_labels:
                  type: 'array'
                  items:
                    type: 'string'
                node_tags:
                  type: 'array'
                  items:
                    type: 'string'
                rack_names:
                  type: 'array'
                  items:
                    type: 'string'
              additionalProperties: false
          success_criteria:
            type: 'object'
            minProperties: 1
            properties:
              percent_successful_nodes:
                type: 'integer'
                minimum: 0
                maximum: 100
              minimum_successful_nodes:
                type: 'integer'
                minimum: 0
              maximum_failed_nodes:
                type: 'integer'
                minimum: 0
            additionalProperties: false
...
---
schema: 'deckhand/DataSchema/v1'
metadata:
  schema: metadata/Control/v1
  name: shipyard/DeploymentConfiguration/v1
  labels:
    application: shipyard
data:
  $schema: 'http://json-schema.org/schema#'
  type: 'object'
  properties:
    physical_provisioner:
      type: 'object'
      properties:
        deployment_strategy:
          type: 'string'
        deploy_interval:
          type: 'integer'
        deploy_timeout:
          type: 'integer'
        destroy_interval:
          type: 'integer'
        destroy_timeout:
          type: 'integer'
        join_wait:
          type: 'integer'
        prepare_node_interval:
          type: 'integer'
        prepare_node_timeout:
          type: 'integer'
        prepare_site_interval:
          type: 'integer'
        prepare_site_timeout:
          type: 'integer'
        verify_interval:
          type: 'integer'
        verify_timeout:
          type: 'integer'
      additionalProperties: false
    kubernetes:
      type: 'object'
      properties:
        node_status_interval:
          type: 'integer'
        node_status_timeout:
          type: 'integer'
      additionalProperties: false
    kubernetes_provisioner:
      type: 'object'
      properties:
        drain_timeout:
          type: 'integer'
        drain_grace_period:
          type: 'integer'
        clear_labels_timeout:
          type: 'integer'
        remove_etcd_timeout:
          type: 'integer'
        etcd_ready_timeout:
          type: 'integer'
      additionalProperties: false
    armada:
      type: 'object'
      properties:
        get_releases_timeout:
          type: 'integer'
        get_status_timeout:
          type: 'integer'
        manifest:
          type: 'string'
        post_apply_timeout:
          type: 'integer'
        validate_design_timeout:
          type: 'integer'
      additionalProperties: false
      required:
        - manifest
  additionalProperties: false
  required:
    - armada
...
---
schema: deckhand/PublicKey/v1
metadata:
  schema: metadata/Document/v1
  name: airship_ssh_public_key
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
data: ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCyb6CDrai3VcFW1ew5ikf7IDSpqfFyrJNLI1DPyd28vcy6D1oFXdELYK7DsXzVCgV7YNDiKpneXMBTJ/Mr/aZi9K3eVvtRp1HAK3y6ycx9KRfyfMVAU0aT3xMOpE5xS/xTH8HNRbOSszp0woVYKhncpkumHweji7wbLKm/WxsggIoGDjn29KIoRhpo96tWz/DBsoU1pIHTMoZNyHW2aYWEx6kOzTEmhxL0LkKv7+A/2HJuLnqcXoQH9jl3kRQDyikNlSw2T3gQV3I8m0od/lEf98MZb1Yv9GrlDCmnUPXAJ2HQaWaVaPPpGcBW7veOZlLfeulwD4zlo6P6JW1SZaat airship@seaworthy
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: private_docker_key
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
# sample key for potential private docker registry
# see Docker documentation for info on how to generate the key
# base64 of password123
data: cGFzc3dvcmQxMjM=
...
---
schema: pegleg/Script/v1
metadata:
  schema: metadata/Document/v1
  name: hanging-cgroup-release
  storagePolicy: cleartext
  layeringDefinition:
    abstract: false
    layer: global
data: |-
  #!/bin/bash
  set -ex

  cgroup_count() {
    echo "Current cgroup count: $(find /sys/fs/cgroup/*/system.slice -name tasks | wc -l)"
  }

  DATE=$(date)
  echo "$(cgroup_count)"
  echo   # Stop systemd mount unit that isn't actually mounted
  echo "Stopping Kubernetes systemd mount units that are not mounted to the system."
  systemctl list-units --state=running| \
    sed -rn '/Kubernetes.transient.mount/s,(run-\S+).+(/var/lib/kubelet/pods/.+),\1 \2,p' | \
    xargs -r -l1 sh -c 'test -d $2 || echo $1' -- | \
    xargs -r -tl1 systemctl stop |& wc -l
  echo "$(cgroup_count)"
...
---
schema: pegleg/Script/v1
metadata:
  schema: metadata/Document/v1
  name: configure-ip-rules
  storagePolicy: cleartext
  layeringDefinition:
    abstract: false
    layer: global
data: |-
  #!/bin/bash
  set -ex

  function usage() {
      cat <<EOU
  Options are:

    -c POD_CIDR     The pod CIDR for the Kubernetes cluster, e.g. 10.97.0.0/16
    -i INTERFACE    (optional) The interface for internal pod traffic, e.g.
                    bond0.22.  Used to auto-detect the service gateway.
                    Exclusive with -g.
    -g SERVICE_GW   (optional) The service gateway/VRR IP for routing pod
                    traffic.  Exclusive with -i.
    -o OVERLAP_CIDR (optional) This CIDR will be routed via the VRRP IP on
                    INTERFACE.  It is used to provide a work around when
                    complete Calico routes cannot be received via BGP.
                    e.g. 10.96.0.0/15.  NOTE: This must include the POD_CIDR.
    -s SERVICE_CIDR (optional) A routable CIDR to configure for ingress, maas,
                    e.g. 10.23.22.192/29
  EOU
  }

  SERVICE_CIDR=
  OVERLAP_CIDR=

  while getopts ":c:g:hi:o:s:" o; do
      case "${o}" in
          c)
              POD_CIDR=${OPTARG}
              ;;
          g)
              SERVICE_GW=${OPTARG}
              ;;
          h)
              usage
              exit 0
              ;;
          i)
              INTERFACE=${OPTARG}
              ;;
          o)
              OVERLAP_CIDR=${OPTARG}
              ;;
          s)
              SERVICE_CIDR=${OPTARG}
              ;;
          \?)
              echo "Unknown option: -${OPTARG}" >&2
              exit 1
              ;;
          :)
              echo "Missing argument for option: -${OPTARG}" >&2
              exit 1
              ;;
          *)
              echo "Unimplemented option: -${OPTARG}" >&2
              exit 1
              ;;
      esac
  done
  shift $((OPTIND-1))

  if [ "x$POD_CIDR" == "x" ]; then
      echo "Missing pod CIDR, e.g -c 10.97.0.0/16" >&2
      usage
      exit 1
  fi

  if [ "x$INTERFACE" != "x" ]; then
      while ! ip route list dev "${INTERFACE}" > /dev/null; do
          echo Waiting for device "${INTERFACE}" to be ready. >&2
          sleep 5
      done
  fi

  intra_vrrp_ip=
  if [ "x${SERVICE_GW}" == "x" ]; then
      intra_vrrp_ip=$(ip route list dev "${INTERFACE}" | awk '($2~/via/){print $3}' | head -n 1)
  else
      intra_vrrp_ip=${SERVICE_GW}
  fi

  TABLE="1500"

  if [ "x${intra_vrrp_ip}" == "x" ]; then
      echo "Either INTERFACE or SERVICE_GW is required: e.g. either -i bond0.22 or -g 10.23.22.1"
      usage
      exit 1
  fi

  # Setup a routing table for traffic from service IPs
  ip route flush table "${TABLE}"
  ip route add default via "${intra_vrrp_ip}" table "${TABLE}"

  # Setup arp_announce adjustment on interface facing gateway
  arp_intf=$(ip route get ${intra_vrrp_ip} | grep dev | awk '{print $3}')
  echo 2 > /proc/sys/net/ipv4/conf/${arp_intf}/arp_announce


  if [ "x$OVERLAP_CIDR" != "x" ]; then
      # NOTE: This is a work-around for nodes not receiving complete
      # routes via BGP.
      ip route add "${OVERLAP_CIDR}" via "${intra_vrrp_ip}"
  fi

  if [ "x$SERVICE_CIDR" != "x" ]; then
      # Traffic from the service IPs to pods should use the pod network.
      ip rule add \
          from "${SERVICE_CIDR}" \
          to "${POD_CIDR}" \
          lookup main \
          pref 10000
      # Other traffic from service IPs should only use the VRRP IP
      ip rule add \
          from "${SERVICE_CIDR}" \
          lookup "${TABLE}" \
          pref 10100
  fi
...
---
schema: pegleg/Script/v1
metadata:
  schema: metadata/Document/v1
  name: rbd-roomba-scanner
  storagePolicy: cleartext
  layeringDefinition:
    abstract: false
    layer: global
data: |-
  #!/bin/bash
  set -ex

  # don't put it in /tmp where it can be p0wned (???)
  lsblk | awk '/^rbd/ {if($7==""){print $0}}' | awk '{ printf "/dev/%s\n",$1 }' > /var/run/rbd_list

  # wait a while, so we don't catch rbd devices the kubelet is working on mounting
  sleep 60

  # finally, examine rbd devices again and if any were seen previously (60s ago) we will
  # forcefully unmount them if they have no fs mounts
  DATE=$(date)
  for rbd in `lsblk | awk '/^rbd/ {if($7==""){print $0}}' | awk '{ printf "/dev/%s\n",$1 }'`; do
    if grep -q $rbd /var/run/rbd_list; then
      echo "[${DATE}] Unmapping stale RBD $rbd"
      /usr/bin/rbd unmap -o force $rbd
      # NOTE(supamatt): rbd unmap -o force will only succeed if there are NO pending I/O
    else
      echo "[${DATE}] Skipping RBD $rbd as it hasn't been stale for at least 60 seconds"
    fi
  done
  rm -rf /var/run/rbd_list
...
---
schema: armada/Manifest/v1
metadata:
  schema: metadata/Document/v1
  replacement: true
  name: full-site
  labels:
    name: full-site-type
  layeringDefinition:
    abstract: false
    layer: type
    parentSelector:
      name: full-site-global
    actions:
      - method: merge
        path: .
  storagePolicy: cleartext
data:
  release_prefix: airship
  chart_groups:
    - podsecuritypolicy
    - kubernetes-proxy
    - kubernetes-container-networking
    - kubernetes-dns
    - kubernetes-etcd
    - kubernetes-haproxy
    - kubernetes-core
    - ingress-kube-system
    - ucp-ceph
    - ucp-ceph-config
    - ucp-core
    - ucp-keystone
    - ucp-divingbell
    - ucp-armada
    - ucp-deckhand
    - ucp-drydock
    - ucp-promenade
    - ucp-shipyard
    - ucp-prometheus-openstack-exporter
    - osh-infra-ingress-controller
    - osh-infra-ceph-config
    - osh-infra-radosgw
#    - osh-infra-logging
#    - osh-infra-monitoring
    - osh-infra-mariadb
#    - osh-infra-dashboards
    - openstack-ingress-controller
    - openstack-ceph-config
    - openstack-tenant-ceph
    - openstack-mariadb
    - openstack-rabbitmq
    - openstack-memcached
    - openstack-keystone
    - openstack-radosgw
    - openstack-glance
#    - openstack-cinder
    - openstack-compute-kit
#    - openstack-heat
#    - osh-infra-prometheus-openstack-exporter
#    - openstack-horizon
...
---
schema: armada/Manifest/v1
metadata:
  schema: metadata/Document/v1
  replacement: true
  name: cluster-bootstrap
  labels:
    name: cluster-bootstrap-type
  layeringDefinition:
    abstract: false
    layer: type
    parentSelector:
      name: cluster-bootstrap-global
    actions:
      - method: merge
        path: .
  storagePolicy: cleartext
data:
  release_prefix: airship
  chart_groups:
    - podsecuritypolicy
    - kubernetes-proxy
    - kubernetes-container-networking
    - kubernetes-dns
    - kubernetes-etcd
    - kubernetes-haproxy
    - kubernetes-core
    - ingress-kube-system
    - ucp-ceph
    - ucp-ceph-config
    - ucp-core
    - ucp-keystone
    - ucp-divingbell
    - ucp-armada
    - ucp-deckhand
    - ucp-drydock
    - ucp-promenade
    - ucp-shipyard
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: ingress-kube-system
  labels:
    name: ingress-kube-system-type
  layeringDefinition:
    abstract: false
    layer: type
    parentSelector:
      ingress: kube-system
    actions:
      - method: merge
        path: .
  storagePolicy: cleartext
data: {}
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  replacement: true
  name: osh-infra-ingress-controller
  labels:
    name: osh-infra-ingress-controller-type
  layeringDefinition:
    abstract: false
    layer: type
    parentSelector:
      name: osh-infra-ingress-controller-global
    actions:
      - method: merge
        path: .
  storagePolicy: cleartext
data:
  values:
    pod:
      replicas:
        ingress: 1
        error_page: 1
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  replacement: true
  name: prometheus
  labels:
    name: prometheus-type
  layeringDefinition:
    abstract: false
    layer: type
    parentSelector:
      name: prometheus-global
    actions:
      - method: merge
        path: .
  storagePolicy: cleartext
data:
  values:
    pod:
      replicas:
        prometheus: 1
      resources:
        enabled: true
        prometheus:
          limits:
            memory: "4Gi"
            cpu: "2000m"
          requests:
            memory: "2Gi"
            cpu: "1000m"
    storage:
      requests:
        storage: 50Gi
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: elasticsearch
  labels:
    name: elasticsearch-type
  layeringDefinition:
    abstract: false
    layer: type
    parentSelector:
      hosttype: elasticsearch-global
    actions:
      - method: merge
        path: .
  storagePolicy: cleartext
data:
  values:
    pod:
      replicas:
        master: 2
        data: 1
        client: 2
    storage:
      requests:
        storage: 50Gi
    conf:
      elasticsearch:
        env:
          java_opts:
            client: "-Xms768m -Xmx768m"
            data: "-Xms768m -Xmx768m"
            master: "-Xms768m -Xmx768m"
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: fluentbit
  labels:
    name: fluentbit-type
  layeringDefinition:
    abstract: false
    layer: type
    parentSelector:
      hosttype: fluentbit-global
    actions:
      - method: merge
        path: .
  storagePolicy: cleartext
data:
  values:
    pod:
      replicas:
        fluentd: 1
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  replacement: true
  name: grafana
  labels:
    name: grafana-type
  layeringDefinition:
    abstract: false
    layer: type
    parentSelector:
      name: grafana-global
    actions:
      - method: merge
        path: .
  storagePolicy: cleartext
data:
  values:
    pod:
      replicas:
        grafana: 1
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: fluentd
  labels:
    name: fluentd-type
  layeringDefinition:
    abstract: false
    layer: type
    parentSelector:
      hosttype: fluentd-global
    actions:
      - method: merge
        path: .
  storagePolicy: cleartext
data:
  values:
    pod:
      replicas:
        fluentd: 1
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  replacement: true
  name: osh-infra-mariadb
  labels:
    name: osh-infra-mariadb-type
  layeringDefinition:
    abstract: false
    layer: type
    parentSelector:
      name: osh-infra-mariadb-global
    actions:
      - method: merge
        path: .
  storagePolicy: cleartext
data:
  values:
    pod:
      replicas:
        server: 1
        ingress: 1
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  replacement: true
  name: ucp-armada
  labels:
    name: ucp-armada-type
  layeringDefinition:
    abstract: false
    layer: type
    parentSelector:
      name: ucp-armada-global
    actions:
      - method: merge
        path: .
  storagePolicy: cleartext
data:
  values:
    pod:
      replicas:
        api: 1
...
---
# The purpose of this file is to provide site-specific parameters for the ucp-
# promenade chart.
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: ucp-promenade
  labels:
    name: ucp-promenade-type
  layeringDefinition:
    abstract: false
    layer: type
    parentSelector:
      name: ucp-promenade-global
    actions:
      - method: merge
        path: .
  storagePolicy: cleartext
data:
  values:
    pod:
      replicas:
        api: 1
      env:
        promenade_api: []
          # NEWSITE-CHANGEME: If your site uses an http proxy, enter it here.
          # Otherwise comment out these lines.
          # - name: http_proxy
          #   value: http://proxy.example.com:8080
          # NEWSITE-CHANGEME: If your site uses an https proxy, enter it here.
          # Otherwise comment out these lines.
          # - name: https_proxy
          #   value: http://proxy.example.com:8080
          # NEWSITE-CHANGEME: If your site uses an http/https proxy, enter the
          # IPs / domain names which the proxy should not be used for (i.e. the
          # cluster domain and kubernetes service_cidr defined in common-addresses)
          # Otherwise comment out these lines.
          # - name: no_proxy
          #   value: 10.36.0.1,.cluster.local
          # NEWSITE-CHANGEME: If your site uses an http proxy, enter it here.
          # Otherwise comment out these lines.
          # - name: HTTP_PROXY
          #   value: http://proxy.example.com:8080
          # NEWSITE-CHANGEME: If your site uses an https proxy, enter it here.
          # Otherwise comment out these lines.
          # - name: HTTPS_PROXY
          #   value: http://proxy.example.com:8080
          # NEWSITE-CHANGEME: If your site uses an http/https proxy, enter the
          # IPs / domain names which the proxy should not be used for (i.e. the
          # cluster domain and kubernetes service_cidr defined in common-addresses)
          # Otherwise comment out these lines.
          # - name: NO_PROXY
          #   value: 10.36.0.1,.cluster.local
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  replacement: true
  name: ucp-keystone
  labels:
    name: ucp-keystone-type
  layeringDefinition:
    abstract: false
    layer: type
    parentSelector:
      name: ucp-keystone-global
    actions:
      - method: merge
        path: .
  storagePolicy: cleartext
data:
  values:
    pod:
      replicas:
        api: 1
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  replacement: true
  name: ucp-barbican
  labels:
    name: ucp-barbican-type
  layeringDefinition:
    abstract: false
    layer: type
    parentSelector:
      name: ucp-barbican-global
    actions:
      - method: merge
        path: .
  storagePolicy: cleartext
data:
  values:
    pod:
      replicas:
        api: 1
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  replacement: true
  name: ucp-deckhand
  labels:
    name: ucp-deckhand-type
  layeringDefinition:
    abstract: false
    layer: type
    parentSelector:
      name: ucp-deckhand-global
    actions:
      - method: merge
        path: .
  storagePolicy: cleartext
data:
  values:
    pod:
      replicas:
        deckhand: 1
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  replacement: true
  name: ucp-shipyard
  labels:
    name: ucp-shipyard-type
  layeringDefinition:
    abstract: false
    layer: type
    parentSelector:
      name: ucp-shipyard-global
    actions:
      - method: merge
        path: .
  storagePolicy: cleartext
data:
  values:
    pod:
      replicas:
        shipyard:
          api: 1
        airflow:
          worker: 1
          scheduler: 1
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  replacement: true
  name: ucp-drydock
  labels:
    name: ucp-drydock-type
  layeringDefinition:
    abstract: false
    layer: type
    parentSelector:
      name: ucp-drydock-global
    actions:
      - method: merge
        path: .
  storagePolicy: cleartext
  substitutions:

    # MaaS IPs
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .genesis.ip
      dest:
        path: .values.conf.drydock.maasdriver.maas_api_url
        pattern: 'MAAS_IP'
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .node_ports.maas_api
      dest:
        path: .values.conf.drydock.maasdriver.maas_api_url
        pattern: 'MAAS_PORT'

data:
  values:
    pod:
      security_context:
        drydock:
          pod:
            # NOTE: Drydock has a hardcoded path to SSH key that
            # uses root home directory, default `nobody` user
            # does not have have the access to keys in the root
            # directory, consequently Drydock fails to connect to
            # the Libvirt host using SSH.
            # Remove this workaround when Drydock is fixed.
            runAsUser: 0
    manifests:
      secret_ssh_key: true
    network:
      api:
        nodeport:
          enabled: true
    replicas:
      drydock: 1
    conf:
      ssh:
        private_key: |-
          -----BEGIN RSA PRIVATE KEY-----
          MIIEowIBAAKCAQEAt273lGWAdwslQMwFxzfJpmPDBZkF9pnI5lkQ5OPFcK05F2AT
          BM99t7rN1nD2YKsSDZYkjgR31A6mWWW1HNtKEfKQCvxHzFBgFhgRSQyJ6WDvHRhj
          RZHKGNlRFasvtiHdDMdKdlXPr15pS28GZ6HntDRbwkslmE2phRAh3ox3RN7K9ZxN
          4xGWYPD03J3bQgxp1hMVdhAvwggcklKz3QMhk8RwNL5n/tRnZ3MqfGkQwQLB+y1p
          +BYlZst9EZ9tsY07Oxe+AUV2v7GtM83bi/5Xo830iwBASDg4e1htSnUlHh9hATGb
          xr0Tygki6VvV1R+NoJY0k/8P6sDmyjiYdl25RwIDAQABAoIBAAj2UP5dHzOxxbYu
          wbGEFkQc57BkU5hC1z/55gto3YKt0/ZCaWt4v8m0RM5PYppCgXVMeqi9qyUfOh1w
          DLNGO4447bS7sr5WxvsNXfrVs/9FDym6wU7q2pbwNzf5zzD04pn3OrohYy5MTJS0
          7fkuPeXeEQWjKkkQslGgAiefcUxaCRg5UnIziNoKlsKCfZ/vhcE6m0o+sye01YiD
          c1MhPJYjQfcS0ZqVZ3fTH5IW4Tq75CpsgomgAwOt02Hw/WZC6xgCGz8ElOB50WKs
          wk4HtT5wVDRFirVsjr85fWz0BrnkgV0Ls4M5HoYyQ8g6kCjMOXQsqc4cd6oeQb7g
          BShnDYECgYEA5Fh6wT3s8h6jd7LX4PwiqkNaDoArfv3rMZQZ9FYBUeEyvYujKqsN
          sGJa+Hl1S9Dv6s/hkla7Ey7SMDeTXz/DZwC9WM+Ro+Eb3Xf0LDrKCy37oNxhkb61
          7PkX4BxnMRARzGbsjicVMRRElcGVUzwsaORcdYrfbJi5HxtghtdadyECgYEAzaYN
          WTIEQfRQ8VKbl+aFWxiHvIn2KuWxXbGNB0aur5uOVO7CmcwZsuyVICppWaz8Ye/r
          WH+7mYTRj2OICBu8m955ebs68prkVV7v1SaCjD+mTY7R7muBpEVPsKRso8qi/JLU
          R27+5DQvhrhMJ9NIWaFaXhMD/kG6FFgyaOSNa2cCgYEAnpWgc1KZm7GRn9DyQst5
          G7x47/ctvh2E2ULdH6cXdZEsFx3CbSCs+iHkwgpAXy41YWOMaoCXngP3cAs4636y
          K3gFCIfnwuPU4WOsjYcqyMbfqeFEVd8YJAL/BONU+2sIoWedxD/6ZMKJu2PdXBg/
          U329hNi0wIv0jVkLGbq4lmECgYBGLEJjzFBtnQu9vR2A0NVyh0VSDZWlf6ltOjfQ
          Yssa+y6vRqW6y019o4MjbbVzNzcLyE17bmK6ePr1PdZeRfCvE1RKOJxdyoLdqr6V
          8kUbzGBYGMMD40Vio5AUy5aSsYO6QfQTyAlMH46UHvFFqbAHfaqTbVOwgAcaCBpz
          doHXQwKBgCXBWUDCC84F8PTV4xtYWqaftPhu9rewacInSfmrrqUW0cgOPxWEM1BO
          dFB/BxyieoghGP281VcrnJGuh3j6aqb/rdcWmZ0qgAzSJMls/0+d2RbatRBEOn2K
          sAXnT+qmj2jPiqsAlqtLdCounRKbazUVkIFu+4sCaORdXFJYoqVK
          -----END RSA PRIVATE KEY-----
      drydock:
        plugins:
          oob_driver:
            - 'drydock_provisioner.drivers.oob.pyghmi_driver.driver.PyghmiDriver'
            - 'drydock_provisioner.drivers.oob.libvirt_driver.driver.LibvirtDriver'
        database:
          pool_size: 200
        maasdriver:
          maas_api_url: http://MAAS_IP:MAAS_PORT/MAAS
...
---
# This file defines site-specific deviations for MaaS.
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: ucp-maas
  labels:
    name: ucp-maas-type
  layeringDefinition:
    abstract: false
    layer: type
    parentSelector:
      name: ucp-maas-global
    actions:
      - method: merge
        path: .values.manifests
      - method: merge
        path: .values.network
      - method: merge
        path: .values.conf
      - method: replace
        path: .values.dependencies.static
      - method: replace
        path: .values.endpoints.maas_ingress.hosts
  storagePolicy: cleartext
  substitutions:

    # Drydock IPs
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .bootstrap.ip
      dest:
        path: .values.conf.drydock.bootaction_url
        pattern: '(DRYDOCK_IP)'
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .node_ports.drydock_api
      dest:
        path: .values.conf.drydock.bootaction_url
        pattern: '(DRYDOCK_PORT)'

    # MaaS IPs
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .bootstrap.ip
      dest:
        path: .values.conf.maas.url.maas_url
        pattern: '(MAAS_IP)'
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .node_ports.maas_api
      dest:
        path: .values.conf.maas.url.maas_url
        pattern: '(MAAS_PORT)'
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .node_ports.maas_api
      dest:
        path: .values.endpoints.maas_region.port.region_api.nodeport
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .node_ports.maas_proxy
      dest:
        path: .values.endpoints.maas_region.port.region_proxy.default
data:
  values:
    manifests:
      maas_ingress: false
      secret_ssh_key: true

    network:
      region_proxy:
        node_port:
          enabled: true
      region_api:
        ingress:
          public: false
        node_port:
          enabled: true

    conf:
      ssh:
        private_key: |-
          -----BEGIN RSA PRIVATE KEY-----
          MIIEowIBAAKCAQEAt273lGWAdwslQMwFxzfJpmPDBZkF9pnI5lkQ5OPFcK05F2AT
          BM99t7rN1nD2YKsSDZYkjgR31A6mWWW1HNtKEfKQCvxHzFBgFhgRSQyJ6WDvHRhj
          RZHKGNlRFasvtiHdDMdKdlXPr15pS28GZ6HntDRbwkslmE2phRAh3ox3RN7K9ZxN
          4xGWYPD03J3bQgxp1hMVdhAvwggcklKz3QMhk8RwNL5n/tRnZ3MqfGkQwQLB+y1p
          +BYlZst9EZ9tsY07Oxe+AUV2v7GtM83bi/5Xo830iwBASDg4e1htSnUlHh9hATGb
          xr0Tygki6VvV1R+NoJY0k/8P6sDmyjiYdl25RwIDAQABAoIBAAj2UP5dHzOxxbYu
          wbGEFkQc57BkU5hC1z/55gto3YKt0/ZCaWt4v8m0RM5PYppCgXVMeqi9qyUfOh1w
          DLNGO4447bS7sr5WxvsNXfrVs/9FDym6wU7q2pbwNzf5zzD04pn3OrohYy5MTJS0
          7fkuPeXeEQWjKkkQslGgAiefcUxaCRg5UnIziNoKlsKCfZ/vhcE6m0o+sye01YiD
          c1MhPJYjQfcS0ZqVZ3fTH5IW4Tq75CpsgomgAwOt02Hw/WZC6xgCGz8ElOB50WKs
          wk4HtT5wVDRFirVsjr85fWz0BrnkgV0Ls4M5HoYyQ8g6kCjMOXQsqc4cd6oeQb7g
          BShnDYECgYEA5Fh6wT3s8h6jd7LX4PwiqkNaDoArfv3rMZQZ9FYBUeEyvYujKqsN
          sGJa+Hl1S9Dv6s/hkla7Ey7SMDeTXz/DZwC9WM+Ro+Eb3Xf0LDrKCy37oNxhkb61
          7PkX4BxnMRARzGbsjicVMRRElcGVUzwsaORcdYrfbJi5HxtghtdadyECgYEAzaYN
          WTIEQfRQ8VKbl+aFWxiHvIn2KuWxXbGNB0aur5uOVO7CmcwZsuyVICppWaz8Ye/r
          WH+7mYTRj2OICBu8m955ebs68prkVV7v1SaCjD+mTY7R7muBpEVPsKRso8qi/JLU
          R27+5DQvhrhMJ9NIWaFaXhMD/kG6FFgyaOSNa2cCgYEAnpWgc1KZm7GRn9DyQst5
          G7x47/ctvh2E2ULdH6cXdZEsFx3CbSCs+iHkwgpAXy41YWOMaoCXngP3cAs4636y
          K3gFCIfnwuPU4WOsjYcqyMbfqeFEVd8YJAL/BONU+2sIoWedxD/6ZMKJu2PdXBg/
          U329hNi0wIv0jVkLGbq4lmECgYBGLEJjzFBtnQu9vR2A0NVyh0VSDZWlf6ltOjfQ
          Yssa+y6vRqW6y019o4MjbbVzNzcLyE17bmK6ePr1PdZeRfCvE1RKOJxdyoLdqr6V
          8kUbzGBYGMMD40Vio5AUy5aSsYO6QfQTyAlMH46UHvFFqbAHfaqTbVOwgAcaCBpz
          doHXQwKBgCXBWUDCC84F8PTV4xtYWqaftPhu9rewacInSfmrrqUW0cgOPxWEM1BO
          dFB/BxyieoghGP281VcrnJGuh3j6aqb/rdcWmZ0qgAzSJMls/0+d2RbatRBEOn2K
          sAXnT+qmj2jPiqsAlqtLdCounRKbazUVkIFu+4sCaORdXFJYoqVK
          -----END RSA PRIVATE KEY-----
      drydock:
        bootaction_url: http://DRYDOCK_IP:DRYDOCK_PORT/api/v1.0
      maas:
        images:
          default_os: 'ubuntu'
          default_image: 'xenial'
          default_kernel: 'ga-16.04'
        credentials:
          secret:
            namespace: ucp
        url:
          maas_url: http://MAAS_IP:MAAS_PORT/MAAS

    dependencies:
      static:
        maas_ingress: {}
        rack_controller:
          services:
            - service: maas_region
              endpoint: internal
          jobs:
            - maas-export-api-key
        region_controller:
          jobs:
            - maas-db-sync
          services:
            - service: maas_db
              endpoint: internal
        db_init:
          services:
            - service: maas_db
              endpoint: internal
        db_sync:
          jobs:
            - maas-db-init
        bootstrap_admin_user:
          jobs:
            - maas-db-sync
          services:
            - service: maas_region
              endpoint: internal
            - service: maas_db
              endpoint: internal
        import_resources:
          jobs:
            - maas-bootstrap-admin-user
          services:
            - service: maas_region
              endpoint: internal
            - service: maas_db
              endpoint: internal
        export_api_key:
          jobs:
            - maas-bootstrap-admin-user
          services:
            - service: maas_region
              endpoint: internal
            - service: maas_db
              endpoint: internal

    endpoints:
      maas_ingress:
        hosts:
          default: maas-ingress
          error_pages: maas-ingress-error
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: ucp-postgresql
  replacement: true
  labels:
    name: ucp-postgresql-type
  layeringDefinition:
    abstract: false
    layer: type
    parentSelector:
      name: ucp-postgresql-global
    actions:
      - method: merge
        path: .
  storagePolicy: cleartext
data:
  values:
    pod:
      replicas:
        server: 1
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  replacement: true
  name: ucp-ingress
  labels:
    name: ucp-ingress-type
  layeringDefinition:
    abstract: false
    layer: type
    parentSelector:
      name: ucp-ingress-global
    actions:
      - method: merge
        path: .
  storagePolicy: cleartext
data:
  values:
    pod:
      replicas:
        ingress: 1
        error_page: 1
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  replacement: true
  name: ucp-rabbitmq
  labels:
    name: ucp-rabbitmq-type
  layeringDefinition:
    abstract: false
    layer: type
    parentSelector:
      name: ucp-rabbitmq-global
    actions:
      - method: merge
        path: .
  storagePolicy: cleartext
data:
  values:
    pod:
      replicas:
        server: 1
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  replacement: true
  name: ucp-mariadb
  labels:
    name: ucp-mariadb-type
  layeringDefinition:
    abstract: false
    layer: type
    parentSelector:
      name: ucp-mariadb-global
    actions:
      - method: merge
        path: .
  storagePolicy: cleartext
data:
  values:
    pod:
      replicas:
        server: 1
        ingress: 1
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: ucp-ceph-client
  labels:
    name: ucp-ceph-client-type
  layeringDefinition:
    abstract: false
    layer: type
    parentSelector:
      name: ucp-ceph-client-global
    actions:
      - method: merge
        path: .
  storagePolicy: cleartext
data:
  values:
    conf:
      pool:
        target:
          osd: 1
        spec:
          # RBD pool
          - name: rbd
            application: rbd
            replication: 1
            percent_total_data: 40
          - name: cephfs_metadata
            application: cephfs
            replication: 1
            percent_total_data: 5
          - name: cephfs_data
            application: cephfs
            replication: 1
            percent_total_data: 10
          # RadosGW pools
          - name: .rgw.root
            application: rgw
            replication: 1
            percent_total_data: 0.1
          - name: default.rgw.control
            application: rgw
            replication: 1
            percent_total_data: 0.1
          - name: default.rgw.data.root
            application: rgw
            replication: 1
            percent_total_data: 0.1
          - name: default.rgw.gc
            application: rgw
            replication: 1
            percent_total_data: 0.1
          - name: default.rgw.log
            application: rgw
            replication: 1
            percent_total_data: 0.1
          - name: default.rgw.intent-log
            application: rgw
            replication: 1
            percent_total_data: 0.1
          - name: default.rgw.meta
            application: rgw
            replication: 1
            percent_total_data: 0.1
          - name: default.rgw.usage
            application: rgw
            replication: 1
            percent_total_data: 0.1
          - name: default.rgw.users.keys
            application: rgw
            replication: 1
            percent_total_data: 0.1
          - name: default.rgw.users.email
            application: rgw
            replication: 1
            percent_total_data: 0.1
          - name: default.rgw.users.swift
            application: rgw
            replication: 1
            percent_total_data: 0.1
          - name: default.rgw.users.uid
            application: rgw
            replication: 1
            percent_total_data: 0.1
          - name: default.rgw.buckets.extra
            application: rgw
            replication: 1
            percent_total_data: 0.1
          - name: default.rgw.buckets.index
            application: rgw
            replication: 1
            percent_total_data: 3
          - name: default.rgw.buckets.data
            application: rgw
            replication: 1
            percent_total_data: 34.8
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  replacement: true
  name: ucp-ceph-provisioners
  labels:
    name: ucp-ceph-provisioners-type
  layeringDefinition:
    abstract: false
    layer: type
    parentSelector:
      name: ucp-ceph-provisioners
    actions:
      - method: merge
        path: .
  storagePolicy: cleartext
data:
  values:
    pod:
      replicas:
        cephfs_provisioner: 1
        rbd_provisioner: 1
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  replacement: true
  name: ucp-ceph-ingress
  labels:
    name: ucp-ceph-ingress-type
  layeringDefinition:
    abstract: false
    layer: type
    parentSelector:
      name: ucp-ceph-ingress-global
    actions:
      - method: merge
        path: .
  storagePolicy: cleartext
data:
  values:
    pod:
      replicas:
        ingress: 1
        error_page: 1
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: ucp-ceph-osd
  labels:
    name: ucp-ceph-osd-type
  layeringDefinition:
    abstract: false
    layer: type
    parentSelector:
      name: ucp-ceph-osd-global
    actions:
      - method: merge
        path: .
  storagePolicy: cleartext
data: {}
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: nova
  labels:
    name: nova-type
  layeringDefinition:
    abstract: false
    layer: type
    parentSelector:
      name: nova-global
    actions:
      - method: merge
        path: .
  storagePolicy: cleartext
data:
  test:
    enabled: false
  values:
    pod:
      replicas:
        api_metadata: 1
        compute_ironic: 1
        placement: 1
        osapi: 1
        conductor: 1
        consoleauth: 1
        scheduler: 1
        novncproxy: 1
        spiceproxy: 1
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: neutron
  labels:
    name: neutron-type
  layeringDefinition:
    abstract: false
    layer: type
    parentSelector:
      name: neutron-global
    actions:
      - method: merge
        path: .
  storagePolicy: cleartext
data:
  values:
    pod:
      replicas:
        server: 1
    conf:
      paste:
        app:neutronversions:
          paste.app_factory: neutron.pecan_wsgi.app:versions_factory
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  replacement: true
  name: heat
  labels:
    name: heat-type
  layeringDefinition:
    abstract: false
    layer: type
    parentSelector:
      name: heat-global
    actions:
      - method: merge
        path: .
  storagePolicy: cleartext
data:
  values:
    pod:
      replicas:
        api: 1
        cfn: 1
        cloudwatch: 1
        engine: 1
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  replacement: true
  name: openstack-ingress-controller
  labels:
    name: openstack-ingress-controller-type
  layeringDefinition:
    abstract: false
    layer: type
    parentSelector:
      name: openstack-ingress-controller-global
    actions:
      - method: merge
        path: .
  storagePolicy: cleartext
data:
  values:
    pod:
      replicas:
        ingress: 1
        error_page: 1
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  replacement: true
  name: openstack-rabbitmq
  labels:
    name: openstack-rabbitmq-type
  layeringDefinition:
    abstract: false
    layer: type
    parentSelector:
      name: openstack-rabbitmq-global
    actions:
      - method: merge
        path: .
  storagePolicy: cleartext
data:
  values:
    pod:
      replicas:
        server: 1
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  replacement: true
  name: keystone
  labels:
    name: keystone-type
  layeringDefinition:
    abstract: false
    layer: type
    parentSelector:
      name: keystone-global
    actions:
      - method: merge
        path: .
  storagePolicy: cleartext
data:
  values:
    pod:
      replicas:
        api: 1
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  replacement: true
  name: cinder
  labels:
    name: cinder-type
  layeringDefinition:
    abstract: false
    layer: type
    parentSelector:
      name: cinder-global
    actions:
      - method: merge
        path: .
  storagePolicy: cleartext
data:
  values:
    pod:
      replicas:
        api: 1
        volume: 1
        scheduler: 1
        backup: 1
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  replacement: true
  name: horizon
  labels:
    name: horizon-type
  layeringDefinition:
    abstract: false
    layer: type
    parentSelector:
      name: horizon-global
    actions:
      - method: merge
        path: .
  storagePolicy: cleartext
data:
  values:
    pod:
      replicas:
        server: 1
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  replacement: true
  name: glance
  labels:
    name: glance-type
  layeringDefinition:
    abstract: false
    layer: type
    parentSelector:
      name: glance-global
    actions:
      - method: merge
        path: .
  storagePolicy: cleartext
data:
  test:
    enabled: false
  values:
    pod:
      replicas:
        api: 1
        registry: 1
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  replacement: true
  name: openstack-mariadb
  labels:
    name: openstack-mariadb-type
  layeringDefinition:
    abstract: false
    layer: type
    parentSelector:
      name: openstack-mariadb-global
    actions:
      - method: merge
        path: .
  storagePolicy: cleartext
data:
  values:
    pod:
      replicas:
        server: 1
        ingress: 1
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: tenant-ceph-client
  labels:
    name: tenant-ceph-client-type
  layeringDefinition:
    abstract: false
    layer: type
    parentSelector:
      name: tenant-ceph-client-global
    actions:
      - method: merge
        path: .
  storagePolicy: cleartext
data:
  values:
    pod:
      replicas:
        mds: 1
        mgr: 1
    conf:
      ceph:
        global:
          osd_pool_default_size: 1
      pool:
        default:
          crush_rule: same_host
        spec:
          # RBD pool
          - name: rbd
            application: rbd
            replication: 1
            percent_total_data: 10
          # Cinder volumes  pool
          - name: cinder.volumes
            application: cinder-volume
            replication: 1
            percent_total_data: 40
          # RadosGW pools
          - name: .rgw.root
            application: rgw
            replication: 1
            percent_total_data: 0.1
          - name: default.rgw.control
            application: rgw
            replication: 1
            percent_total_data: 0.1
          - name: default.rgw.data.root
            application: rgw
            replication: 1
            percent_total_data: 0.1
          - name: default.rgw.gc
            application: rgw
            replication: 1
            percent_total_data: 0.1
          - name: default.rgw.log
            application: rgw
            replication: 1
            percent_total_data: 0.1
          - name: default.rgw.intent-log
            application: rgw
            replication: 1
            percent_total_data: 0.1
          - name: default.rgw.meta
            application: rgw
            replication: 1
            percent_total_data: 0.1
          - name: default.rgw.usage
            application: rgw
            replication: 1
            percent_total_data: 0.1
          - name: default.rgw.users.keys
            application: rgw
            replication: 1
            percent_total_data: 0.1
          - name: default.rgw.users.email
            application: rgw
            replication: 1
            percent_total_data: 0.1
          - name: default.rgw.users.swift
            application: rgw
            replication: 1
            percent_total_data: 0.1
          - name: default.rgw.users.uid
            application: rgw
            replication: 1
            percent_total_data: 0.1
          - name: default.rgw.buckets.extra
            application: rgw
            replication: 1
            percent_total_data: 0.1
          - name: default.rgw.buckets.index
            application: rgw
            replication: 1
            percent_total_data: 3
          - name: default.rgw.buckets.data
            application: rgw
            replication: 1
            percent_total_data: 30
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  replacement: true
  name: tenant-ceph-ingress
  labels:
    name: tenant-ceph-ingress-type
  layeringDefinition:
    abstract: false
    layer: type
    parentSelector:
      name: tenant-ceph-ingress-global
    actions:
      - method: merge
        path: .
  storagePolicy: cleartext
data:
  values:
    pod:
      replicas:
        ingress: 1
        error_page: 1
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: tenant-ceph-osd
  labels:
    name: tenant-ceph-osd-type
  layeringDefinition:
    abstract: false
    layer: type
    parentSelector:
      name: tenant-ceph-osd-global
    actions:
      - method: merge
        path: .
  storagePolicy: cleartext
data: {}
...
---
schema: promenade/KubernetesNetwork/v1
metadata:
  schema: metadata/Document/v1
  name: kubernetes-network
  labels:
    name: kubernetes-network-type
  layeringDefinition:
    abstract: false
    layer: type
  storagePolicy: cleartext
  substitutions:
    # DNS
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .dns.cluster_domain
      dest:
        path: .dns.cluster_domain
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .dns.service_ip
      dest:
        path: .dns.service_ip
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .dns.upstream_servers
      dest:
        path: .dns.upstream_servers

    # Kubernetes IPs
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .kubernetes.api_service_ip
      dest:
        path: .kubernetes.service_ip
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .kubernetes.pod_cidr
      dest:
        path: .kubernetes.pod_cidr
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .kubernetes.service_cidr
      dest:
        path: .kubernetes.service_cidr
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .kubernetes.apiserver_port
      dest:
        path: .kubernetes.apiserver_port
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .kubernetes.haproxy_port
      dest:
        path: .kubernetes.haproxy_port

    # etcd IPs
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .etcd.container_port
      dest:
        path: .etcd.container_port
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .etcd.haproxy_port
      dest:
        path: .etcd.haproxy_port

    # proxy
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .proxy.http
      dest:
        path: .proxy.url
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .proxy.no_proxy
      dest:
        path: .proxy.additional_no_proxy

data:
  dns:
    bootstrap_validation_checks:
      - calico-etcd.kube-system.svc.cluster.local
      - kubernetes-etcd.kube-system.svc.cluster.local
      - kubernetes.default.svc.cluster.local
...
---
# The purpose of this file is to define the site's endpoint catalog. This should
# not need to be modified for a new site.
# #GLOBAL-CANDIDATE#
schema: pegleg/EndpointCatalogue/v1
metadata:
  schema: metadata/Document/v1
  name: ucp_endpoints
  labels:
    name: ucp_endpoints-type
  layeringDefinition:
    abstract: false
    layer: type
  storagePolicy: cleartext
data:
  ucp:
    identity:
      namespace: ucp
      name: keystone
      host_fqdn_override:
        default: null
      path:
        default: /v3
      scheme:
        default: "http"
        internal: "http"
      port:
        api:
          default: 80
          internal: 5000
    armada:
      name: armada
      hosts:
        default: armada-api
        public: armada
      port:
        api:
          default: 8000
      path:
        default: /api/v1.0
      scheme:
        default: "http"
      host_fqdn_override:
        default: null
    deckhand:
      name: deckhand
      hosts:
        default: deckhand-int
        public: deckhand-api
      port:
        api:
          default: 9000
      path:
        default: /api/v1.0
      scheme:
        default: "http"
      host_fqdn_override:
        default: null
    postgresql:
      name: postgresql
      hosts:
        default: postgresql
      path: /DB_NAME
      scheme: postgresql+psycopg2
      port:
        postgresql:
          default: 5432
      host_fqdn_override:
        default: null
    postgresql_airflow_celery:
      name: postgresql_airflow_celery_db
      hosts:
        default: postgresql
      path: /DB_NAME
      scheme: db+postgresql
      port:
        postgresql:
          default: 5432
      host_fqdn_override:
        default: null
    oslo_db:
      hosts:
        default: mariadb
        discovery: mariadb-discovery
      host_fqdn_override:
        default: null
      path: /DB_NAME
      scheme: mysql+pymysql
      port:
        mysql:
          default: 3306
        wsrep:
          default: 4567
    key_manager:
      name: barbican
      hosts:
        default: barbican-api
        public: barbican
      host_fqdn_override:
        default: null
      path:
        default: /v1
      scheme:
        default: "http"
      port:
        api:
          default: 9311
          public: 80
    airflow_oslo_messaging:
      namespace: null
      statefulset:
        name: airship-ucp-rabbitmq-rabbitmq
      hosts:
        default: rabbitmq
      host_fqdn_override:
        default: null
      path: /airflow
      scheme: amqp
      port:
        amqp:
          default: 5672
        http:
          default: 15672
    oslo_messaging:
      namespace: null
      statefulset:
        name: airship-ucp-rabbitmq-rabbitmq
      hosts:
        default: rabbitmq
      host_fqdn_override:
        default: null
      path: /keystone
      scheme: rabbit
      port:
        amqp:
          default: 5672
    oslo_cache:
      hosts:
        default: memcached
      host_fqdn_override:
        default: null
      port:
        memcache:
          default: 11211
    physicalprovisioner:
      name: drydock
      hosts:
        default: drydock-api
        public: drydock-api
      port:
        api:
          default: 9000
          nodeport: 31900
          public: 80
      path:
        default: /api/v1.0
      scheme:
        default: "http"
        public: "http"
      host_fqdn_override:
        default: null
    maas_region:
      name: maas-region
      hosts:
        default: maas-region
        public: maas-region
      path:
        default: /MAAS
      scheme:
        default: "http"
      port:
        region_api:
          default: 80
          nodeport: 31900
          podport: 80
          public: 80
        region_proxy:
          default: 8000
      host_fqdn_override:
        default: null
    maas_ingress:
      hosts:
        default: maas-ingress
        error_pages: maas-ingress-error
      host_fqdn_override:
        public: null
      port:
        http:
          default: 80
        https:
          default: 443
        ingress_default_server:
          default: 8383
        error_pages:
          default: 8080
          podport: 8080
        healthz:
          podport: 10259
        status:
          podport: 18089
    kubernetesprovisioner:
      name: promenade
      hosts:
        default: promenade-api
      port:
        api:
          default: 80
      path:
        default: /api/v1.0
      scheme:
        default: "http"
      host_fqdn_override:
        default: null
    shipyard:
      name: shipyard
      hosts:
        default: shipyard-int
        public: shipyard-api
      port:
        api:
          default: 9000
          public: 80
      path:
        default: /api/v1.0
      scheme:
        default: "http"
        public: "http"
      host_fqdn_override:
        default: null
    prometheus_openstack_exporter:
      namespace: ucp
      hosts:
        default: openstack-metrics
      host_fqdn_override:
        default: null
      path:
        default: null
      scheme:
        default: "http"
      port:
        exporter:
          default: 9103
  ceph:
    object_store:
      name: swift
      namespace: ceph
      hosts:
        default: ceph-rgw
        public: radosgw
      host_fqdn_override:
        default: null
      path:
        default: /swift/v1
      scheme:
        default: "http"
        public: "http"
      port:
        api:
          default: 8088
          public: 80
    ceph_object_store:
      name: radosgw
      namespace: ceph
      hosts:
        default: ceph-rgw
        public: radosgw
      host_fqdn_override:
        default: null
      path:
        default: /auth/v1.0
      scheme:
        default: "http"
        public: "http"
      port:
        api:
          default: 8088
          public: 80
    ceph_mon:
      namespace: ceph
      hosts:
        default: ceph-mon
        discovery: ceph-mon-discovery
      host_fqdn_override:
        default: null
      port:
        mon:
          default: 6789
    ceph_mgr:
      namespace: ceph
      hosts:
        default: ceph-mgr
      host_fqdn_override:
        default: null
      port:
        mgr:
          default: 7000
      scheme:
        default: "http"
    tenant_ceph_mon:
      namespace: tenant-ceph
      hosts:
        default: ceph-mon
        discovery: ceph-mon-discovery
      host_fqdn_override:
        default: null
      port:
        mon:
          default: 6790
    tenant_ceph_mgr:
      namespace: tenant-ceph
      hosts:
        default: ceph-mgr
      host_fqdn_override:
        default: null
      port:
        mgr:
          default: 7001
        metrics:
          default: 9284
      scheme:
        default: http
...
---
schema: pegleg/EndpointCatalogue/v1
metadata:
  schema: metadata/Document/v1
  name: osh_endpoints
  layeringDefinition:
    abstract: false
    layer: type
  storagePolicy: cleartext
data:
  osh:
    object_store:
      name: swift
      namespace: openstack
      hosts:
        default: ceph-rgw
        public: radosgw
      host_fqdn_override:
        default: null
      path:
        default: /swift/v1/KEY_$(tenant_id)s
      scheme:
        default: "http"
        public: "http"
      port:
        api:
          default: 8088
          public: 80
    ceph_object_store:
      name: radosgw
      namespace: openstack
      hosts:
        default: ceph-rgw
        public: radosgw
      host_fqdn_override:
        default: null
      path:
        default: /auth/v1.0
      scheme:
        default: "http"
        public: "http"
      port:
        api:
          default: 8088
          public: 80
    oslo_db:
      hosts:
        default: mariadb
        discovery: mariadb-discovery
      host_fqdn_override:
        default: null
      path: /DB_NAME
      scheme: mysql+pymysql
      port:
        mysql:
          default: 3306
        wsrep:
          default: 4567
    prometheus_mysql_exporter:
      namespace: openstack
      hosts:
        default: mysql-exporter
      host_fqdn_override:
        default: null
      path:
        default: /metrics
      scheme:
        default: 'http'
      port:
        metrics:
          default: 9104
    oslo_messaging:
      statefulset:
        name: airship-openstack-rabbitmq-rabbitmq
      namespace: openstack
      hosts:
        default: openstack-rabbitmq
      host_fqdn_override:
        default: null
      path: /VHOST_NAME
      scheme: rabbit
      port:
        amqp:
          default: 5672
        http:
          default: 15672
    openstack_rabbitmq_exporter:
      namespace: openstack
      hosts:
        default: openstack-rabbitmq-exporter
      host_fqdn_override:
        default: null
      path:
        default: /metrics
      scheme:
        default: "http"
      port:
        metrics:
          default: 9095
    oslo_cache:
      namespace: openstack
      hosts:
        default: memcached
      host_fqdn_override:
        default: null
      port:
        memcache:
          default: 11211
    identity:
      namespace: openstack
      name: keystone
      host_fqdn_override:
        default: null
      path:
        default: /v3
      scheme:
        default: "http"
        internal: "http"
      port:
        api:
          default: 80
          internal: 5000
    image:
      name: glance
      hosts:
        default: glance-api
        public: glance
      host_fqdn_override:
        default: null
      path:
        default: null
      scheme:
        default: "http"
        public: "http"
      port:
        api:
          default: 9292
          public: 80
    image_registry:
      name: glance-registry
      hosts:
        default: glance-registry
        public: glance-reg
      host_fqdn_override:
        default: null
      path:
        default: null
      scheme:
        default: "http"
      port:
        api:
          default: 9191
          public: 80
    volume:
      name: cinder
      hosts:
        default: cinder-api
        public: cinder
      host_fqdn_override:
        default: null
      path:
        default: "/v1/%(tenant_id)s"
      scheme:
        default: "http"
        public: "http"
      port:
        api:
          default: 8776
          public: 80
    volumev2:
      name: cinderv2
      hosts:
        default: cinder-api
        public: cinder
      host_fqdn_override:
        default: null
      path:
        default: "/v2/%(tenant_id)s"
      scheme:
        default: "http"
        public: "http"
      port:
        api:
          default: 8776
          public: 80
    volumev3:
      name: cinderv3
      hosts:
        default: cinder-api
        public: cinder
      host_fqdn_override:
        default: null
      path:
        default: "/v3/%(tenant_id)s"
      scheme:
        default: "http"
        public: "http"
      port:
        api:
          default: 8776
          public: 80
    orchestration:
      name: heat
      hosts:
        default: heat-api
        public: heat
      host_fqdn_override:
        default: null
      path:
        default: "/v1/%(project_id)s"
      scheme:
        default: "http"
        public: "http"
      port:
        api:
          default: 8004
          public: 80
    cloudformation:
      name: heat-cfn
      hosts:
        default: heat-cfn
        public: cloudformation
      host_fqdn_override:
        default: null
      path:
        default: /v1
      scheme:
        default: "http"
        public: "http"
      port:
        api:
          default: 8000
          public: 80
    cloudwatch:
      name: heat-cloudwatch
      hosts:
        default: heat-cloudwatch
        public: cloudwatch
      host_fqdn_override:
        default: null
      path:
        default: null
      type: null
      scheme:
        default: "http"
      port:
        api:
          default: 8003
          public: 80
    network:
      name: neutron
      hosts:
        default: neutron-server
        public: neutron
      host_fqdn_override:
        default: null
      path:
        default: null
      scheme:
        default: "http"
        public: "http"
      port:
        api:
          default: 9696
          public: 80
    compute:
      name: nova
      hosts:
        default: nova-api
        public: nova
      host_fqdn_override:
        default: null
      path:
        default: "/v2/%(tenant_id)s"
      scheme:
        default: "http"
        public: "http"
      port:
        api:
          default: 8774
          public: 80
        novncproxy:
          default: 80
    compute_metadata:
      name: nova
      hosts:
        default: nova-metadata
        public: metadata
      host_fqdn_override:
        default: null
      path:
        default: /
      scheme:
        default: "http"
      port:
        metadata:
          default: 8775
          public: 80
    compute_novnc_proxy:
      name: nova
      hosts:
        default: nova-novncproxy
        public: novncproxy
      host_fqdn_override:
        default: null
      path:
        default: /vnc_auto.html
      scheme:
        default: "http"
        public: "http"
      port:
        novnc_proxy:
          default: 6080
          public: 80
    compute_spice_proxy:
      name: nova
      hosts:
        default: nova-spiceproxy
      host_fqdn_override:
        default: null
      path:
        default: /spice_auto.html
      scheme:
        default: "http"
      port:
        spice_proxy:
          default: 6082
    placement:
      name: placement
      hosts:
        default: placement-api
        public: placement
      host_fqdn_override:
        default: null
      path:
        default: /
      scheme:
        default: "http"
        public: "http"
      port:
        api:
          default: 8778
          public: 80
    dashboard:
      name: horizon
      hosts:
        default: horizon-int
        public: horizon
      host_fqdn_override:
        default: null
      path:
        default: null
      scheme:
        default: "http"
        public: "http"
      port:
        web:
          default: 80
          public: 80
...
---
schema: pegleg/EndpointCatalogue/v1
metadata:
  schema: metadata/Document/v1
  name: osh_infra_endpoints
  layeringDefinition:
    abstract: false
    layer: type
  storagePolicy: cleartext
  substitutions:
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .ldap.auth_path
      dest:
        path: .osh_infra.ldap.path.default
        pattern: AUTH_PATH
data:
  osh_infra:
    ceph_object_store:
      name: radosgw
      namespace: osh-infra
      hosts:
        default: ceph-rgw
        public: radosgw
      host_fqdn_override:
        default: null
      path:
        default: null
      scheme:
        default: "http"
      port:
        api:
          default: 8088
          public: 80
    elasticsearch:
      name: elasticsearch
      namespace: osh-infra
      hosts:
        data: elasticsearch-data
        default: elasticsearch-logging
        discovery: elasticsearch-discovery
        public: elasticsearch
      host_fqdn_override:
        default: null
      path:
        default: null
      scheme:
        default: "http"
    prometheus_elasticsearch_exporter:
      namespace: null
      hosts:
        default: elasticsearch-exporter
      host_fqdn_override:
        default: null
      path:
        default: /metrics
      scheme:
        default: "http"
      port:
        metrics:
          default: 9108
    fluentd:
      namespace: osh-infra
      name: fluentd
      hosts:
        default: fluentd-logging
      host_fqdn_override:
        default: null
      path:
        default: null
      scheme:
        default: "http"
      port:
        service:
          default: 24224
        metrics:
          default: 24220
    prometheus_fluentd_exporter:
      namespace: osh-infra
      hosts:
        default: fluentd-exporter
      host_fqdn_override:
        default: null
      path:
        default: /metrics
      scheme:
        default: "http"
      port:
        metrics:
          default: 9309
    oslo_db:
      namespace: osh-infra
      hosts:
        default: mariadb
      host_fqdn_override:
        default: null
      path: /DB_NAME
      scheme: mysql+pymysql
      port:
        mysql:
          default: 3306
    prometheus_mysql_exporter:
      namespace: osh-infra
      hosts:
        default: mysql-exporter
      host_fqdn_override:
        default: null
      path:
        default: /metrics
      scheme:
        default: 'http'
      port:
        metrics:
          default: 9104
    grafana:
      name: grafana
      namespace: osh-infra
      hosts:
        default: grafana-dashboard
        public: grafana
      host_fqdn_override:
        default: null
      path:
        default: null
      scheme:
        default: "http"
        public: "http"
      port:
        grafana:
          default: 3000
          public: 80
    monitoring:
      name: prometheus
      namespace: osh-infra
      hosts:
        default: prom-metrics
        public: prometheus
      host_fqdn_override:
        default: null
      path:
        default: null
      scheme:
        default: "http"
      port:
        api:
          default: 9090
        http:
          default: 80
    kibana:
      name: kibana
      namespace: osh-infra
      hosts:
        default: kibana-dash
        public: kibana
      host_fqdn_override:
        default: null
      path:
        default: null
      scheme:
        default: "http"
        public: "http"
      port:
        kibana:
          default: 5601
          public: 80
    alerts:
      name: alertmanager
      namespace: osh-infra
      hosts:
        default: alerts-engine
        public: alertmanager
        discovery: alertmanager-discovery
      host_fqdn_override:
        default: null
      path:
        default: null
      scheme:
        default: "http"
      port:
        api:
          default: 9093
          public: 80
        mesh:
          default: 6783
    kube_state_metrics:
      namespace: kube-system
      hosts:
        default: kube-state-metrics
      host_fqdn_override:
        default: null
      path:
        default: null
      scheme:
        default: "http"
      port:
        http:
          default: 8080
    kube_scheduler:
      scheme:
        default: "http"
      path:
        default: /metrics
    kube_controller_manager:
      scheme:
        default: "http"
      path:
        default: /metrics
    node_metrics:
      namespace: kube-system
      hosts:
        default: node-exporter
      host_fqdn_override:
        default: null
      path:
        default: null
      scheme:
        default: "http"
      port:
        metrics:
          default: 9100
        prometheus_port:
          default: 9100
    process_exporter_metrics:
      namespace: kube-system
      hosts:
        default: process-exporter
      host_fqdn_override:
        default: null
      path:
        default: null
      scheme:
        default: "http"
      port:
        metrics:
          default: 9256
    prometheus_openstack_exporter:
      namespace: openstack
      hosts:
        default: openstack-metrics
      host_fqdn_override:
        default: null
      path:
        default: null
      scheme:
        default: "http"
      port:
        exporter:
          default: 9103
    nagios:
      name: nagios
      namespace: osh-infra
      hosts:
        default: nagios-metrics
        public: nagios
      host_fqdn_override:
        default: null
      path:
        default: null
      scheme:
        default: "http"
        public: "http"
      port:
        http:
          default: 80
          public: 80
    ldap:
      hosts:
        default: ldap
      host_fqdn_override:
        default: null
      path:
        default: /AUTH_PATH
      scheme:
        default: "ldap"
      port:
        ldap:
          default: 389
...
---
# The purpose of this file is to define the account catalog for the site. This
# mostly contains service usernames, but also contain some information which
# should be changed like the region (site) name.
schema: pegleg/AccountCatalogue/v1
metadata:
  schema: metadata/Document/v1
  name: ucp_service_accounts
  labels:
    name: ucp_service_accounts-type
  layeringDefinition:
    abstract: false
    layer: type
  storagePolicy: cleartext
data:
    ucp:
        postgres:
            admin:
                username: postgres
            replica:
                username: standby
            exporter:
                username: psql_exporter
        oslo_db:
            admin:
                username: root
        oslo_messaging:
            admin:
                username: rabbitmq
        keystone:
            admin:
                # NEWSITE-CHANGEME: Replace with the site name
                region_name: RegionOne
                username: admin
                project_name: admin
                user_domain_name: default
                project_domain_name: default
            oslo_messaging:
                admin:
                    username: rabbitmq
                keystone:
                    username: keystone
            oslo_db:
                username: keystone
                database: keystone
        promenade:
            keystone:
                # NEWSITE-CHANGEME: Replace with the site name
                region_name: RegionOne
                role: admin
                project_name: service
                project_domain_name: default
                user_domain_name: default
                username: promenade
        drydock:
            keystone:
                # NEWSITE-CHANGEME: Replace with the site name
                region_name: RegionOne
                role: admin
                project_name: service
                project_domain_name: default
                user_domain_name: default
                username: drydock
            postgres:
                username: drydock
                database: drydock
        shipyard:
            keystone:
                # NEWSITE-CHANGEME: Replace with the site name
                region_name: RegionOne
                role: admin
                project_name: service
                project_domain_name: default
                user_domain_name: default
                username: shipyard
            postgres:
                username: shipyard
                database: shipyard
        airflow:
            postgres:
                username: airflow
                database: airflow
            oslo_messaging:
                admin:
                    username: rabbitmq
                user:
                    username: airflow
        maas:
            admin:
                username: admin
                email: none@none
            postgres:
                username: maas
                database: maasdb
        barbican:
            keystone:
                # NEWSITE-CHANGEME: Replace with the site name
                region_name: RegionOne
                role: admin
                project_name: service
                project_domain_name: default
                user_domain_name: default
                username: barbican
            oslo_db:
                username: barbican
                database: barbican
            oslo_messaging:
                admin:
                    username: rabbitmq
                keystone:
                    username: keystone
        armada:
            keystone:
                project_domain_name: default
                user_domain_name: default
                project_name: service
                # NEWSITE-CHANGEME: Replace with the site name
                region_name: RegionOne
                role: admin
                username: armada
        deckhand:
            keystone:
                # NEWSITE-CHANGEME: Replace with the site name
                region_name: RegionOne
                role: admin
                project_name: service
                project_domain_name: default
                user_domain_name: default
                username: deckhand
            postgres:
                username: deckhand
                database: deckhand
        prometheus_openstack_exporter:
            user:
                region_name: RegionOne
                role: admin
                username: prometheus-openstack-exporter
                project_name: service
                user_domain_name: default
                project_domain_name: default
    ceph:
        swift:
            keystone:
                role: admin
                # NEWSITE-CHANGEME: Replace with the site name
                region_name: RegionOne
                username: swift
                project_name: service
                user_domain_name: default
                project_domain_name: default
...
---
schema: pegleg/AccountCatalogue/v1
metadata:
  schema: metadata/Document/v1
  name: osh_service_accounts
  layeringDefinition:
    abstract: false
    layer: type
  storagePolicy: cleartext
  substitutions:
    - src:
        schema: pegleg/CommonSoftwareConfig/v1
        name: common-software-config
        path: .osh.region_name
      dest:
        path: .osh.keystone.admin.region_name
    - src:
        schema: pegleg/CommonSoftwareConfig/v1
        name: common-software-config
        path: .osh.region_name
      dest:
        path: .osh.cinder.cinder.region_name
    - src:
        schema: pegleg/CommonSoftwareConfig/v1
        name: common-software-config
        path: .osh.region_name
      dest:
        path: .osh.glance.glance.region_name
    - src:
        schema: pegleg/CommonSoftwareConfig/v1
        name: common-software-config
        path: .osh.region_name
      dest:
        path: .osh.heat.heat.region_name
    - src:
        schema: pegleg/CommonSoftwareConfig/v1
        name: common-software-config
        path: .osh.region_name
      dest:
        path: .osh.heat.heat_trustee.region_name
    - src:
        schema: pegleg/CommonSoftwareConfig/v1
        name: common-software-config
        path: .osh.region_name
      dest:
        path: .osh.heat.heat_stack_user.region_name
    - src:
        schema: pegleg/CommonSoftwareConfig/v1
        name: common-software-config
        path: .osh.region_name
      dest:
        path: .osh.swift.keystone.region_name
    - src:
        schema: pegleg/CommonSoftwareConfig/v1
        name: common-software-config
        path: .osh.region_name
      dest:
        path: .osh.neutron.neutron.region_name
    - src:
        schema: pegleg/CommonSoftwareConfig/v1
        name: common-software-config
        path: .osh.region_name
      dest:
        path: .osh.nova.nova.region_name
    - src:
        schema: pegleg/CommonSoftwareConfig/v1
        name: common-software-config
        path: .osh.region_name
      dest:
        path: .osh.nova.placement.region_name
    - src:
        schema: pegleg/CommonSoftwareConfig/v1
        name: common-software-config
        path: .osh.region_name
      dest:
        path: .osh.barbican.barbican.region_name
data:
  osh:
    keystone:
      admin:
        username: admin
        project_name: admin
        user_domain_name: default
        project_domain_name: default
      oslo_db:
        username: keystone
        database: keystone
      oslo_messaging:
        keystone:
          username: keystone-rabbitmq-user
      ldap:
        # NEWSITE-CHANGEME: Replace with the site's LDAP account used to
        # authenticate to the active directory backend to validate keystone
        # users.
        username: "test@ldap.example.com"
    cinder:
      cinder:
        role: admin
        username: cinder
        project_name: service
        user_domain_name: default
        project_domain_name: default
      oslo_db:
        username: cinder
        database: cinder
      oslo_messaging:
        cinder:
          username: cinder-rabbitmq-user
    glance:
      glance:
        role: admin
        username: glance
        project_name: service
        user_domain_name: default
        project_domain_name: default
      oslo_db:
        username: glance
        database: glance
      oslo_messaging:
        glance:
          username: glance-rabbitmq-user
      ceph_object_store:
        username: glance
    heat:
      heat:
        role: admin
        username: heat
        project_name: service
        user_domain_name: default
        project_domain_name: default
      heat_trustee:
        role: admin
        username: heat-trust
        project_name: service
        user_domain_name: default
        project_domain_name: default
      heat_stack_user:
        role: admin
        username: heat-domain
        domain_name: heat
      oslo_db:
        username: heat
        database: heat
      oslo_messaging:
        heat:
          username: heat-rabbitmq-user
    swift:
      keystone:
        role: admin
        username: swift
        project_name: service
        user_domain_name: default
        project_domain_name: default
    oslo_db:
      admin:
        username: root
    prometheus_mysql_exporter:
      user:
        username: osh-oslodb-exporter
    neutron:
      neutron:
        role: admin
        username: neutron
        project_name: service
        user_domain_name: default
        project_domain_name: default
      oslo_db:
        username: neutron
        database: neutron
      oslo_messaging:
        neutron:
          username: neutron-rabbitmq-user
    nova:
      nova:
        role: admin
        username: nova
        project_name: service
        user_domain_name: default
        project_domain_name: default
      placement:
        role: admin
        username: placement
        project_name: service
        user_domain_name: default
        project_domain_name: default
      oslo_db:
        username: nova
        database: nova
      oslo_db_api:
        username: nova
        database: nova_api
      oslo_db_cell0:
        username: nova
        database: "nova_cell0"
      oslo_messaging:
        nova:
          username: nova-rabbitmq-user
    horizon:
      oslo_db:
        username: horizon
        database: horizon
    barbican:
      barbican:
        role: admin
        username: barbican
        project_name: service
        user_domain_name: default
        project_domain_name: default
      oslo_db:
        username: barbican
        database: barbican
      oslo_messaging:
        barbican:
          username: barbican-rabbitmq-user
    oslo_messaging:
      admin:
        username: admin
    tempest:
      tempest:
        role: admin
        username: tempest
        project_name: service
        user_domain_name: default
        project_domain_name: default
...
---
schema: pegleg/AccountCatalogue/v1
metadata:
  schema: metadata/Document/v1
  name: osh_infra_service_accounts
  layeringDefinition:
    abstract: false
    layer: type
  storagePolicy: cleartext
  substitutions:
    - src:
        schema: pegleg/CommonSoftwareConfig/v1
        name: common-software-config
        path: .osh.region_name
      dest:
        path: .osh_infra.prometheus_openstack_exporter.user.region_name
data:
  osh_infra:
    ceph_object_store:
      admin:
        username: s3_admin
      elasticsearch:
        username: elasticsearch
    grafana:
      admin:
        username: grafana
      oslo_db:
        username: grafana
        database: grafana
      oslo_db_session:
        username: grafana_session
        database: grafana_session
    elasticsearch:
      admin:
        username: elasticsearch
    oslo_db:
      admin:
        username: root
    prometheus_mysql_exporter:
      user:
        username: osh-infra-oslodb-exporter
    prometheus_openstack_exporter:
      user:
        role: admin
        username: prometheus-openstack-exporter
        project_name: service
        user_domain_name: default
        project_domain_name: default
    nagios:
      admin:
        username: nagios
    prometheus:
      admin:
        username: prometheus
    ldap:
      admin:
        # NEWSITE-CHANGEME: Replace with the site's LDAP account used to
        # authenticate to the active directory backend to validate keystone
        # users.
        bind: "test@ldap.example.com"
...
---
# High-level pegleg site definition file
schema: pegleg/SiteDefinition/v1
metadata:
  schema: metadata/Document/v1
  layeringDefinition:
    abstract: false
    layer: site
  # NEWSITE-CHANGEME: Replace with the site name
  name: airsloop
  storagePolicy: cleartext
data:
  # The type layer this site will delpoy with. Type layer is found in the
  # type folder.
  site_type: sloop
...
---
# Drydock BaremetalNode resources for a specific rack are stored in this file.
#
# NOTE: For new sites, you should complete the networks/physical/networks.yaml
# file before working on this file.
#
# In this file, you should make the number of `drydock/BaremetalNode/v1`
# resources equal the number of bare metal nodes you have, either by deleting
# excess BaremetalNode definitions (if there are too many), or by copying and
# pasting the last BaremetalNode in the file until you have the correct number
# of baremetal nodes (if there are too few).
#
# Then in each file, address all additional NEWSITE-CHANGEME markers to update
# the data in these files with the right values for your new site.
#
# *NOTE: The Genesis node is counted as one of the control plane nodes. Note
# that the Genesis node does not appear on this bare metal list, because the
# procedure to reprovision the Genesis host with MaaS has not yet been
# implemented. Therefore there will be only three bare metal nodes in this file
# with the 'masters' tag, as the genesis roles are assigned in a difference
# place (profiles/genesis.yaml).
# NOTE: The host profiles for the control plane are further divided into two
# variants: primary and secondary. The only significance this has is that the
# "primary" nodes are active Ceph nodes, whereas the "secondary" nodes are Ceph
# standby nodes. For Ceph quorum, this means that the control plane split will
# be 3 primary + 1 standby host profile, and the Genesis node counts toward one
# of the 3 primary profiles. Other control plane services are not affected by
# primary vs secondary designation.
#
# TODO: Include the hostname naming convention
#
schema: 'drydock/BaremetalNode/v1'
metadata:
  schema: 'metadata/Document/v1'
  # NEWSITE-CHANGEME: The next node's hostname
  name: airsloop-compute-1
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data:
  # NEWSITE-CHANGEME: The next node's IPv4 addressing
  addressing:
    - network: oob
      address: 192.168.129.1
    - network: pxe
      address: 192.168.123.22
    - network: oam
      address: 192.168.124.22
    - network: calico
      address: 192.168.125.22
    - network: storage
      address: 192.168.126.22
    - network: overlay
      address: 192.168.127.22
  # NEWSITE-CHANGEME: The next node's host profile
  # This is the third "primary" control plane profile after genesis
  host_profile: compute_r720xd
  metadata:
    boot_mac: '52:54:00:cd:44:76'
    # NEWSITE-CHANGEME: The next node's rack designation
    rack: cab22
    # NEWSITE-CHANGEME: The next node's role desigatnion
    tags:
      - 'workers'
  platform:
    kernel_params:
      hugepagesz: '2M'
      hugepages: 1024
      default_hugepagesz: '2M'
#      iommu: 'pt'
#      intel_iommu: 'on'
#      kvm_intel.nested: 1
...
---
# This file defines a boot action which is responsible for fetching the node's
# promjoin script from the promenade API. This is the script responsible for
# installing kubernetes on the node and joining the kubernetes cluster.
# #GLOBAL-CANDIDATE#
schema: 'drydock/BootAction/v1'
metadata:
  schema: 'metadata/Document/v1'
  name: promjoin
  storagePolicy: 'cleartext'
  layeringDefinition:
    abstract: false
    layer: site
  labels:
    application: 'drydock'
data:
  signaling: false
  # TODO(alanmeadows) move what is global about this document
  assets:
    - path: /opt/promjoin.sh
      type: file
      permissions: '555'
      # The ip= parameter must match the MaaS network name of the network used
      # to contact kubernetes. With a standard, reference Airship deployment where
      # L2 networks are shared between all racks, the network name (i.e. calico)
      # should be correct.
      location: promenade+http://promenade-api.ucp.svc.cluster.local/api/v1.0/join-scripts?design_ref={{ action.design_ref | urlencode }}&hostname={{ node.hostname }}&ip={{ node.network.calico.ip }}{% for k, v in node.labels.items() %}&labels.dynamic={{ k }}={{ v }}{% endfor %}
      location_pipeline:
        - template
      data_pipeline:
        - utf8_decode
...
---
# The purpose of this file is to define network related paramters that are
# referenced elsewhere in the manifests for this site.
#
# TODO: Include bare metal host FQDN naming standards
# TODO: Include ingress FQDN naming standards
schema: pegleg/CommonAddresses/v1
metadata:
  schema: metadata/Document/v1
  name: common-addresses
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data:
  calico:
    # NEWSITE-CHANGEME: The interface that calico will use. Update if your
    # logical bond interface name or calico VLAN have changed from the reference
    # site design.
    # This should be whichever
    # bond and VLAN number specified in networks/physical/networks.yaml for the Calico
    # network. E.g. VLAN 22 for the calico network as a member of bond0, you
    # would set "interface=bond0.22" as shown here.
    ip_autodetection_method: interface=ens6.72
    etcd:
      # etcd service IP address
      service_ip: 10.96.232.136

  dns:
    # Kubernetes cluster domain. Do not change. This is internal to the cluster.
    cluster_domain: cluster.local
    # DNS service ip
    service_ip: 10.96.0.10
    # List of upstream DNS forwards. Verify you can reach them from your
    # environment. If so, you should not need to change them.
    upstream_servers:
      - 8.8.8.8
      - 8.8.4.4
    # Repeat the same values as above, but formatted as a common separated
    # string
    upstream_servers_joined: 8.8.8.8,8.8.4.4
    # NEWSITE-CHANGEME: FQDN for ingress (i.e. "publicly facing" access point)
    # Choose FQDN according to the ingress/public FQDN naming conventions at
    # the top of this document.
    ingress_domain: atlantafoundry.com

  genesis:
    # NEWSITE-CHANGEME: Update with the hostname for the node which will take on
    # the Genesis role. Refer to the hostname naming stardards in
    # networks/physical/networks.yaml
    # NOTE: Ensure that the genesis node is manually configured with this
    # hostname before running `genesis.sh` on the node.
    hostname: airsloop-control-1
    # NEWSITE-CHANGEME: Calico IP of the Genesis node. Use the "start" value for
    # the calico network defined in networks/physical/networks.yaml for this IP.
    ip: 192.168.125.21

  bootstrap:
    # NEWSITE-CHANGEME: Update with the "start" value/IP of the static range
    # defined for the pxe network in networks/physical/networks.yaml
    ip: 192.168.123.21

  kubernetes:
    # K8s API service IP
    api_service_ip: 10.96.0.1
    # etcd service IP
    etcd_service_ip: 10.96.0.2
    # k8s pod CIDR (network which pod traffic will traverse)
    pod_cidr: 10.97.0.0/16
    # k8s service CIDR (network which k8s API traffic will traverse)
    service_cidr: 10.96.0.0/16
    # misc k8s port settings
    apiserver_port: 6443
    haproxy_port: 6553
    service_node_port_range: 30000-32767

  # etcd port settings
  etcd:
    container_port: 2379
    haproxy_port: 2378

  # NEWSITE-CHANGEME: A list of nodes (apart from Genesis) which act as the
  # control plane servers. Ensure that this matches the nodes with the 'masters'
  # tags applied in baremetal/nodes.yaml
  #masters:
  #  - hostname: airsloop-control-2
  #  - hostname: airsloop-control-3

  # NEWSITE-CHANGEME: Environment proxy information.
  # NOTE: Reference Airship sites do not deploy behind a proxy, so this proxy section
  # should be commented out.
  # However if you are in a lab that requires proxy, ensure that these proxy
  # settings are correct and reachable in your environment; otherwise update
  # them with the correct values for your environment.
  proxy:
    http: ""
    https: ""
    no_proxy: []

  node_ports:
    drydock_api: 30000
    maas_api: 30001
    maas_proxy: 31800  # hardcoded in MAAS

  ntp:
    # comma separated NTP server list. Verify that these upstream NTP servers are
    # reachable in your environment; otherwise update them with the correct
    # values for your environment.
    servers_joined: '10.239.12.37'

  # NOTE: This will be updated soon
  ldap:
    # NEWSITE-CHANGEME: FQDN for LDAP. Update to the FQDN that is
    # relevant for your type of deployment (test vs prod values, etc).
    base_url: 'ldap.example.com'
    # NEWSITE-CHANGEME: As above, with the protocol included to create a full URI
    url: 'ldap://ldap.example.com'
    # NEWSITE-CHANGEME: Update to the correct expression relevant for this
    # deployment (test vs prod values, etc)
    auth_path: DC=test,DC=test,DC=com?sAMAccountName?sub?memberof=CN=test,OU=Application,OU=Groups,DC=test,DC=test,DC=com
    # NEWSITE-CHANGEME: Update to the correct AD group that contains the users
    # relevant for this deployment (test users vs prod users/values, etc)
    common_name: test
    # NEWSITE-CHANGEME: Update to the correct subdomain for your type of
    # deployment (test vs prod values, etc)
    subdomain: test
    # NEWSITE-CHANGEME: Update to the correct domain for your type of
    # deployment (test vs prod values, etc)
    domain: example

  storage:
    ceph:
      # NEWSITE-CHANGEME: CIDRs for Ceph. Update to match the network CIDR
      # used for the `storage` network in networks/physical/networks.yaml
      public_cidr: '192.168.126.0/24'
      cluster_cidr: '192.168.126.0/24'

  neutron:
    # NEWSITE-CHANGEME: Overlay network for VM traffic. Ensure the bond name and
    # VLAN number are consistent with what's defined for the bond and the overlay
    # network in networks/physical/networks.yaml
    tunnel_device: 'ens6.74'
    # bond which the overlay is a member of. Ensure the bond name is consistent
    # with the bond assigned to the overlay network in
    # networks/physical/networks.yaml
    external_iface: 'ens6'

  openvswitch:
    # bond which the overlay is a member of. Ensure the bond name is consistent
    # with the bond assigned to the overlay network in
    # networks/physical/networks.yaml
    external_iface: 'ens6'
...
---
# The purpose of this file is to define all of the NetworkLinks (i.e. layer 1
# devices) and Networks (i.e. layer 3 configurations). The following is standard
# for the logical networks in Airship:
#
# +----------+-----------------------------------+----------------+--------------+-----------------+
# | Network  |                                   | Per-rack or    |              |   VLAN tagged   |
# |   Name   |             Purpose               | per-site CIDR? | Has gateway? |  or untagged?   |
# +----------+-----------------------------------+----------------+--------------+-----------------+
# |   oob    | Out of Band devices (iDrac/iLo)   | per-site CIDR  | Has gateway  | Untagged/Native |
# |   pxe    | PXE boot network                  | per-site CIDR  | No gateway   | Untagged/Native |
# |   oam    | management network                | per-site CIDR  | Has gateway  |     tagged      |
# | storage  | storage network                   | per-site CIDR  | No gateway   |     tagged      |
# |  calico  | underlay calico net; k8s traffic  | per-site CIDR  | No gateway   |     tagged      |
# | overlay  | overlay network for openstack SDN | per-site CIDR  | No gateway   |     tagged      |
# +----------+-----------------------------------+----------------+--------------+-----------------+
#
# For standard Airship deployments, you should not need to modify the number of
# NetworkLinks and Networks in this file. Only the IP addresses and CIDRs should
# need editing.
#
# TODO: Given that we expect all network broadcast domains to span all racks in
# Airship, we should choose network names that do not include the rack number.
#
# TODO: FQDN naming standards for hosts
#
schema: 'drydock/NetworkLink/v1'
metadata:
  schema: 'metadata/Document/v1'
  name: oob
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data:
  # MaaS doesnt own this network like it does the others, so the noconfig label
  # is specified.
  labels:
    noconfig: enabled
  bonding:
    mode: disabled
  mtu: 1500
  linkspeed: auto
  trunking:
    mode: disabled
    default_network: oob
  allowed_networks:
    - oob
...
---
schema: 'drydock/Network/v1'
metadata:
  schema: 'metadata/Document/v1'
  name: oob
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data:
  # NEWSITE-CHANGEME: Update with the site's out-of-band CIDR
  cidr: 192.168.129.0/24
  # NEWSITE-CHANGEME: Update with the site's out-of-band IP allocation range
  # FIXME: Is this IP range actually used/allocated for anything? The HW already
  # has its OOB IPs assigned. None of the Ubuntu OS's should need IPs on OOB
  # network either, as they should be routable via the default gw on OAM network
  ranges:
    - type: static
      start: 192.168.129.1
      end: 192.168.129.2
...
---
schema: 'drydock/NetworkLink/v1'
metadata:
  schema: 'metadata/Document/v1'
  name: nat
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data:
  # MaaS doesnt own this network like it does the others, so the noconfig label
  # is specified.
  labels:
    noconfig: enabled
  bonding:
    mode: disabled
  mtu: 1500
  linkspeed: auto
  trunking:
    mode: disabled
    default_network: nat
  allowed_networks:
    - nat
...
---
schema: 'drydock/Network/v1'
metadata:
  schema: 'metadata/Document/v1'
  name: nat
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data:
  # NEWSITE-CHANGEME: Update with the site's out-of-band CIDR
  cidr: 192.168.121.0/24
  # NEWSITE-CHANGEME: Update with the site's out-of-band IP allocation range
  # FIXME: Is this IP range actually used/allocated for anything? The HW already
  # has its OOB IPs assigned. None of the Ubuntu OS's should need IPs on OOB
  # network either, as they should be routable via the default gw on OAM network
  ranges:
    - type: static
      start: 192.168.121.20
      end: 192.168.121.40
...
---
schema: 'drydock/NetworkLink/v1'
metadata:
  schema: 'metadata/Document/v1'
  name: pxe
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data:
  bonding:
    mode: disabled
  mtu: 1500
  linkspeed: auto
  trunking:
    mode: disabled
    default_network: pxe
  allowed_networks:
    - pxe
...
---
schema: 'drydock/Network/v1'
metadata:
  schema: 'metadata/Document/v1'
  name: pxe
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data:
  # NEWSITE-CHANGEME: Update with the site's PXE network CIDR
  # NOTE: The CIDR minimum size = (number of nodes * 2) + 10
  cidr: 192.168.123.0/24
  routes:
    - subnet: 0.0.0.0/0
      # NEWSITE-CHANGEME: Set the OAM network gateway IP address
      gateway: 192.168.123.1
      metric: 100
  # NOTE: The first 10 IPs in the subnet are reserved for network infrastructure.
  # The remainder of the range is divided between two subnets of equal size:
  # one static, and one DHCP.
  # The DHCP addresses are used when nodes perform a PXE boot (DHCP address gets
  # assigned), and when a node is commissioning in MaaS (also uses DHCP to get
  # its IP address). However, when MaaS installs the operating system
  # ("Deploying/Deployed" states), it will write a static IP assignment to
  # /etc/network/interfaces[.d] with IPs from the "static" subnet defined here.
  ranges:
    # NEWSITE-CHANGEME: Update to the first 10 IPs in the CIDR
    - type: reserved
      start: 192.168.123.1
      end: 192.168.123.10
    # NEWSITE-CHANGEME: Update to the first half of the remaining range after
    # excluding the 10 reserved IPs.
    - type: static
      start: 192.168.123.21
      end: 192.168.123.31
    # NEWSITE-CHANGEME: Update to the second half of the remaining range after
    # excluding the 10 reserved IPs.
    - type: dhcp
      start: 192.168.123.40
      end: 192.168.123.80
  dns:
    # NEWSITE-CHANGEME: FQDN for bare metal nodes.
    # Choose FQDN according to the node FQDN naming conventions at the top of
    # this document.
    domain: atlantafoundry.com
    # List of upstream DNS forwards. Verify you can reach them from your
    # environment. If so, you should not need to change them.
    # TODO: This should be populated via substitution from common-addresses
    servers: '8.8.8.8,8.8.4.4,208.67.222.222'
...
---
schema: 'drydock/NetworkLink/v1'
metadata:
  schema: 'metadata/Document/v1'
  name: data
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data:
  bonding:
    mode: disabled
  mtu: 1500
  linkspeed: auto
  trunking:
    mode: 802.1q
  allowed_networks:
    - oam
    - storage
    - overlay
    - calico
...
---
schema: 'drydock/Network/v1'
metadata:
  schema: 'metadata/Document/v1'
  name: oam
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data:
  # NEWSITE-CHANGEME: Set the VLAN ID which the OAM network is on
  vlan: '71'
  mtu: 1500
  # NEWSITE-CHANGEME: Set the CIDR for the OAM network
  # NOTE: The CIDR minimum size = number of nodes + 10
  cidr: 192.168.124.0/24
#  routes:
#    - subnet: 0.0.0.0/0
#      # NEWSITE-CHANGEME: Set the OAM network gateway IP address
#      gateway: 192.168.124.1
#      metric: 100
  ranges:
    # NEWSITE-CHANGEME: Update to the first 10 IPs in the CIDR
    - type: reserved
      start: 192.168.124.1
      end: 192.168.124.10
    # NEWSITE-CHANGEME: Update to the remaining range after excluding the 10
    # 10 reserved IPs.
    - type: static
      start: 192.168.124.21
      end: 192.168.124.31
  dns:
    # NEWSITE-CHANGEME: FQDN for bare metal nodes.
    # Choose FQDN according to the node FQDN naming conventions at the top of
    # this document.
    domain: atlantafoundry.com
    # List of upstream DNS forwards. Verify you can reach them from your
    # environment. If so, you should not need to change them.
    # TODO: This should be populated via substitution from common-addresses
    servers: '8.8.8.8,8.8.4.4'
...
---
schema: 'drydock/Network/v1'
metadata:
  schema: 'metadata/Document/v1'
  name: calico
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data:
  # NEWSITE-CHANGEME: Set the VLAN ID which the calico network is on
  vlan: '72'
  mtu: 1500
  # NEWSITE-CHANGEME: Set the CIDR for the calico network
  # NOTE: The CIDR minimum size = number of nodes + 10
  cidr: 192.168.125.0/24
  ranges:
    # NEWSITE-CHANGEME: Update to the first 10 IPs in the CIDR
    - type: reserved
      start: 192.168.125.1
      end: 192.168.125.10
    # NEWSITE-CHANGEME: Update to the remaining range after excluding the 10
    # 10 reserved IPs.
    - type: static
      start: 192.168.125.21
      end: 192.168.125.31
...
---
schema: 'drydock/Network/v1'
metadata:
  schema: 'metadata/Document/v1'
  name: storage
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data:
  # NEWSITE-CHANGEME: Set the VLAN ID which the storage network is on
  vlan: '73'
  mtu: 1500
  # NEWSITE-CHANGEME: Set the CIDR for the storage network
  # NOTE: The CIDR minimum size = number of nodes + 10
  cidr: 192.168.126.0/24
  ranges:
    # NEWSITE-CHANGEME: Update to the first 10 IPs in the CIDR
    - type: reserved
      start: 192.168.126.1
      end: 192.168.126.10
    # NEWSITE-CHANGEME: Update to the remaining range after excluding the 10
    # 10 reserved IPs.
    - type: static
      start: 192.168.126.21
      end: 192.168.126.31
...
---
schema: 'drydock/Network/v1'
metadata:
  schema: 'metadata/Document/v1'
  name: overlay
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data:
  # NEWSITE-CHANGEME: Set the VLAN ID which the overlay network is on
  vlan: '74'
  mtu: 1500
  # NEWSITE-CHANGEME: Set the CIDR for the overlay network
  # NOTE: The CIDR minimum size = number of nodes + 10
  cidr: 192.168.127.0/24
  ranges:
    # NEWSITE-CHANGEME: Update to the first 10 IPs in the CIDR
    - type: reserved
      start: 192.168.127.1
      end: 192.168.127.10
    # NEWSITE-CHANGEME: Update to the remaining range after excluding the 10
    # 10 reserved IPs.
    - type: static
      start: 192.168.127.21
      end: 192.168.127.31
...
---
# The purpose of this file is to build the list of k8s etcd nodes and the
# k8s etcd certs for those nodes in the environment.
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: kubernetes-etcd
  layeringDefinition:
    abstract: false
    layer: site
    parentSelector:
      name: kubernetes-etcd-global
    actions:
      - method: merge
        path: .
  storagePolicy: cleartext
  substitutions:
    # Generate a list of control plane nodes (i.e. genesis node + master node
    # list) on which k8s etcd will run and will need certs. It is assumed
    # that Airship sites will have 4 control plane nodes, so this should not need to
    # change for a new site.
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .genesis.hostname
      dest:
        path: .values.nodes[0].name
#    - src:
#        schema: pegleg/CommonAddresses/v1
#        name: common-addresses
#        path: .masters[0].hostname
#      dest:
#        path: .values.nodes[1].name
#    - src:
#        schema: pegleg/CommonAddresses/v1
#        name: common-addresses
#        path: .masters[1].hostname
#      dest:
#        path: .values.nodes[2].name

    # Certificate substitutions for the node names assembled on the above list.
    # NEWSITE-CHANGEME: Per above, the number of substitutions should not need
    # to change with a standard Airship deployment. However, the names of each
    # deckhand certficiate should be updated with the correct hostnames for your
    # environment. The ordering is important (Genesis is index 0, then master
    # nodes in the order they are specified in common-addresses).

    # Genesis Exception*
    # *NOTE: This is an exception in that `genesis` is not the hostname of the
    # genesis node, but `genesis` is reference here in the certificate names
    # because of certain Promenade assumptions that may be addressed in the
    # future. Therefore `genesis` is used instead of `airsloop-control-1` here.
    - src:
        schema: deckhand/Certificate/v1
        name: kubernetes-etcd-genesis
        path: .
      dest:
        path: .values.nodes[0].tls.client.cert
    - src:
        schema: deckhand/CertificateKey/v1
        name: kubernetes-etcd-genesis
        path: .
      dest:
        path: .values.nodes[0].tls.client.key
    - src:
        schema: deckhand/Certificate/v1
        name: kubernetes-etcd-genesis-peer
        path: .
      dest:
        path: .values.nodes[0].tls.peer.cert
    - src:
        schema: deckhand/CertificateKey/v1
        name: kubernetes-etcd-genesis-peer
        path: .
      dest:
        path: .values.nodes[0].tls.peer.key

    # master node 1 hostname - airsloop-control-2
#    - src:
#        schema: deckhand/Certificate/v1
#        name: kubernetes-etcd-airsloop-control-2
#        path: .
#      dest:
#        path: .values.nodes[1].tls.client.cert
#    - src:
#        schema: deckhand/CertificateKey/v1
#        name: kubernetes-etcd-airsloop-control-2
#        path: .
#      dest:
#        path: .values.nodes[1].tls.client.key
#    - src:
#        schema: deckhand/Certificate/v1
#        name: kubernetes-etcd-airsloop-control-2-peer
#        path: .
#      dest:
#        path: .values.nodes[1].tls.peer.cert
#    - src:
#        schema: deckhand/CertificateKey/v1
#        name: kubernetes-etcd-airsloop-control-2-peer
#        path: .
#      dest:
#        path: .values.nodes[1].tls.peer.key

    # master node 2 hostname - airsloop-control-3
#    - src:
#        schema: deckhand/Certificate/v1
#        name: kubernetes-etcd-airsloop-control-3
#        path: .
#      dest:
#        path: .values.nodes[2].tls.client.cert
#    - src:
#        schema: deckhand/CertificateKey/v1
#        name: kubernetes-etcd-airsloop-control-3
#        path: .
#      dest:
#        path: .values.nodes[2].tls.client.key
#    - src:
#        schema: deckhand/Certificate/v1
#        name: kubernetes-etcd-airsloop-control-3-peer
#        path: .
#      dest:
#        path: .values.nodes[2].tls.peer.cert
#    - src:
#        schema: deckhand/CertificateKey/v1
#        name: kubernetes-etcd-airsloop-control-3-peer
#        path: $
#      dest:
#        path: .values.nodes[2].tls.peer.key

data: {}
...
---
# The purpose of this file is to build the list of calico etcd nodes and the
# calico etcd certs for those nodes in the environment.
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: kubernetes-calico-etcd
  layeringDefinition:
    abstract: false
    layer: site
    parentSelector:
      name: kubernetes-calico-etcd-global
    actions:
      - method: merge
        path: .
  storagePolicy: cleartext
  substitutions:
    # Generate a list of control plane nodes (i.e. genesis node + master node
    # list) on which calico etcd will run and will need certs. It is assumed
    # that Airship sites will have 4 control plane nodes, so this should not need to
    # change for a new site.
    - src:
        schema: pegleg/CommonAddresses/v1
        name: common-addresses
        path: .genesis.hostname
      dest:
        path: .values.nodes[0].name
#    - src:
#        schema: pegleg/CommonAddresses/v1
#        name: common-addresses
#        path: .masters[0].hostname
#      dest:
#        path: .values.nodes[1].name
#    - src:
#        schema: pegleg/CommonAddresses/v1
#        name: common-addresses
#        path: .masters[1].hostname
#      dest:
#        path: .values.nodes[2].name

    # Certificate substitutions for the node names assembled on the above list.
    # NEWSITE-CHANGEME: Per above, the number of substitutions should not need
    # to change with a standard Airship deployment. However, the names of each
    # deckhand certficiate should be updated with the correct hostnames for your
    # environment. The ordering is important (Genesis is index 0, then master
    # nodes in the order they are specified in common-addresses).

    # Genesis hostname - airsloop-control-1
    - src:
        schema: deckhand/Certificate/v1
        name: calico-etcd-airsloop-control-1
        path: .
      dest:
        path: .values.nodes[0].tls.client.cert
    - src:
        schema: deckhand/CertificateKey/v1
        name: calico-etcd-airsloop-control-1
        path: .
      dest:
        path: .values.nodes[0].tls.client.key
    - src:
        schema: deckhand/Certificate/v1
        name: calico-etcd-airsloop-control-1-peer
        path: .
      dest:
        path: .values.nodes[0].tls.peer.cert
    - src:
        schema: deckhand/CertificateKey/v1
        name: calico-etcd-airsloop-control-1-peer
        path: .
      dest:
        path: .values.nodes[0].tls.peer.key

    # master node 1 hostname - airsloop-control-2
#    - src:
#        schema: deckhand/Certificate/v1
#        name: calico-etcd-airsloop-control-2
#        path: .
#      dest:
#        path: .values.nodes[1].tls.client.cert
#    - src:
#        schema: deckhand/CertificateKey/v1
#        name: calico-etcd-airsloop-control-2
#        path: .
#      dest:
#        path: .values.nodes[1].tls.client.key
#    - src:
#        schema: deckhand/Certificate/v1
#        name: calico-etcd-airsloop-control-2-peer
#        path: .
#      dest:
#        path: .values.nodes[1].tls.peer.cert
#    - src:
#        schema: deckhand/CertificateKey/v1
#        name: calico-etcd-airsloop-control-2-peer
#        path: .
#      dest:
#        path: .values.nodes[1].tls.peer.key
#
#    # master node 2 hostname - airsloop-control-3
#    - src:
#        schema: deckhand/Certificate/v1
#        name: calico-etcd-airsloop-control-3
#        path: .
#      dest:
#        path: .values.nodes[2].tls.client.cert
#    - src:
#        schema: deckhand/CertificateKey/v1
#        name: calico-etcd-airsloop-control-3
#        path: .
#      dest:
#        path: .values.nodes[2].tls.client.key
#    - src:
#        schema: deckhand/Certificate/v1
#        name: calico-etcd-airsloop-control-3-peer
#        path: .
#      dest:
#        path: .values.nodes[2].tls.peer.cert
#    - src:
#        schema: deckhand/CertificateKey/v1
#        name: calico-etcd-airsloop-control-3-peer
#        path: .
#      dest:
#        path: .values.nodes[2].tls.peer.key

data: {}
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: ucp-divingbell
  layeringDefinition:
    abstract: false
    layer: site
    parentSelector:
      name: ucp-divingbell-global
    actions:
      - method: merge
        path: .
  labels:
    name: ucp-divingbell-site
  storagePolicy: cleartext
  substitutions:
    - dest:
        path: .values.conf.uamlite.users[0].user_sshkeys[0]
      src:
        schema: deckhand/PublicKey/v1
        name: airsloop_ssh_public_key
        path: .
    - dest:
        path: .values.conf.uamlite.users[0].user_crypt_passwd
      src:
        schema: deckhand/Passphrase/v1
        name: airsloop_crypt_password
        path: .
data:
  values:
    manifests:
      daemonset_ethtool: false
      daemonset_mounts: false
      daemonset_uamlite: true
      daemonset_sysctl: false
      daemonset_limits: false
      daemonset_apt: true
      daemonset_perm: false
      daemonset_exec: true
      daemonset_apparmor: false
    conf:
      uamlite:
        users:
          - user_name: ubuntu
            user_sudo: true
            user_sshkeys: []
...
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  replacement: true
  name: tenant-ceph-client
  layeringDefinition:
    abstract: false
    layer: site
    parentSelector:
      name: tenant-ceph-client-type
    actions:
      - method: merge
        path: .
  storagePolicy: cleartext
data:
  values:
    conf:
      pool:
        target:
          osd: 1
...
---
# The purpose of this file is to define site-specific common software config
# paramters.
schema: pegleg/CommonSoftwareConfig/v1
metadata:
  schema: metadata/Document/v1
  name: common-software-config
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data:
  osh:
    # NEWSITE-CHANGEME: Replace with the site name
    region_name: airsloop
...
---
schema: pegleg/SoftwareVersions/v1
metadata:
  labels:
    name: software-versions-airsloop
  layeringDefinition:
    abstract: false
    layer: site
    parentSelector:
      name: software-versions-global
    actions:
      - method: merge
        path: .
  name: software-versions
  replacement: true
  schema: metadata/Document/v1
  storagePolicy: cleartext
data:
  images:
    osh:
      glance:
        glance_db_sync: "docker.io/openstackhelm/glance:rocky-ubuntu_bionic"
        db_init: "docker.io/openstackhelm/heat:rocky-ubuntu_bionic"
        db_drop: "docker.io/openstackhelm/heat:rocky-ubuntu_bionic"
        ks_user: "docker.io/openstackhelm/heat:rocky-ubuntu_bionic"
        ks_service: "docker.io/openstackhelm/heat:rocky-ubuntu_bionic"
        ks_endpoints: "docker.io/openstackhelm/heat:rocky-ubuntu_bionic"
        glance_api: "docker.io/openstackhelm/glance:rocky-ubuntu_bionic"
        glance_registry: "docker.io/openstackhelm/glance:rocky-ubuntu_bionic"
        bootstrap: "docker.io/openstackhelm/heat:rocky-ubuntu_bionic"
        #glance_storage_init: "docker.io/openstackhelm/ceph-config-helper:latest-opensuse_15"
      keystone:
        bootstrap: "docker.io/openstackhelm/heat:rocky-ubuntu_bionic"
        db_init: "docker.io/openstackhelm/heat:rocky-ubuntu_bionic"
        keystone_db_sync: "docker.io/openstackhelm/keystone:rocky-ubuntu_bionic"
        db_drop: "docker.io/openstackhelm/heat:rocky-ubuntu_bionic"
        ks_user: "docker.io/openstackhelm/heat:rocky-ubuntu_bionic"
        keystone_fernet_setup: "docker.io/openstackhelm/keystone:rocky-ubuntu_bionic"
        keystone_fernet_rotate: "docker.io/openstackhelm/keystone:rocky-ubuntu_bionic"
        keystone_credential_setup: "docker.io/openstackhelm/keystone:rocky-ubuntu_bionic"
        keystone_credential_rotate: "docker.io/openstackhelm/keystone:rocky-ubuntu_bionic"
        keystone_api: "docker.io/openstackhelm/keystone:rocky-ubuntu_bionic"
        keystone_domain_manage: "docker.io/openstackhelm/keystone:rocky-ubuntu_bionic"
      ingress:
        ingress_module_init: "docker.io/openstackhelm/neutron:rocky-ubuntu_bionic"
        ingress_routed_vip: "docker.io/openstackhelm/neutron:rocky-ubuntu_bionic"
      libvirt:
        libvirt: "docker.io/openstackhelm/libvirt:latest-ubuntu_bionic"
      mariadb:
        prometheus_mysql_exporter_helm_tests: "docker.io/openstackhelm/heat:rocky-ubuntu_bionic"
      neutron:
        bootstrap: "docker.io/openstackhelm/heat:rocky-ubuntu_bionic"
        db_init: "docker.io/openstackhelm/heat:rocky-ubuntu_bionic"
        db_drop: "docker.io/openstackhelm/heat:rocky-ubuntu_bionic"
        ks_user: "docker.io/openstackhelm/heat:rocky-ubuntu_bionic"
        ks_service: "docker.io/openstackhelm/heat:rocky-ubuntu_bionic"
        ks_endpoints: "docker.io/openstackhelm/heat:rocky-ubuntu_bionic"
        neutron_db_sync: "docker.io/openstackhelm/neutron:rocky-ubuntu_bionic"
        neutron_dhcp: "docker.io/openstackhelm/neutron:rocky-ubuntu_bionic"
        neutron_l3: "docker.io/openstackhelm/neutron:rocky-ubuntu_bionic"
        neutron_linuxbridge_agent: "docker.io/openstackhelm/neutron:rocky-ubuntu_bionic"
        neutron_metadata: "docker.io/openstackhelm/neutron:rocky-ubuntu_bionic"
        neutron_openvswitch_agent: "docker.io/openstackhelm/neutron:rocky-ubuntu_bionic"
        neutron_server: "docker.io/openstackhelm/neutron:rocky-ubuntu_bionic"
        neutron_sriov_agent: "docker.io/openstackhelm/neutron:rocky-ubuntu_bionic"
        neutron_sriov_agent_init: "docker.io/openstackhelm/neutron:rocky-ubuntu_bionic"
      nova:
        bootstrap: "docker.io/openstackhelm/heat:rocky-ubuntu_bionic"
        db_drop: "docker.io/openstackhelm/heat:rocky-ubuntu_bionic"
        db_init: "docker.io/openstackhelm/heat:rocky-ubuntu_bionic"
        ks_user: "docker.io/openstackhelm/heat:rocky-ubuntu_bionic"
        ks_service: "docker.io/openstackhelm/heat:rocky-ubuntu_bionic"
        ks_endpoints: "docker.io/openstackhelm/heat:rocky-ubuntu_bionic"
        nova_api: "docker.io/openstackhelm/nova:rocky-ubuntu_bionic"
        nova_cell_setup: "docker.io/openstackhelm/nova:rocky-ubuntu_bionic"
        nova_cell_setup_init: "docker.io/openstackhelm/heat:rocky-ubuntu_bionic"
        nova_compute: "docker.io/openstackhelm/nova:rocky-ubuntu_bionic"
        nova_compute_ssh: "docker.io/openstackhelm/nova:rocky-ubuntu_bionic"
        nova_conductor: "docker.io/openstackhelm/nova:rocky-ubuntu_bionic"
        nova_consoleauth: "docker.io/openstackhelm/nova:rocky-ubuntu_bionic"
        nova_db_sync: "docker.io/openstackhelm/nova:rocky-ubuntu_bionic"
        nova_novncproxy: "docker.io/openstackhelm/nova:rocky-ubuntu_bionic"
        nova_novncproxy_assets: "docker.io/openstackhelm/nova:rocky-ubuntu_bionic"
        nova_placement: "docker.io/openstackhelm/nova:rocky-ubuntu_bionic"
        nova_scheduler: "docker.io/openstackhelm/nova:rocky-ubuntu_bionic"
        nova_spiceproxy: "docker.io/openstackhelm/nova:rocky-ubuntu_bionic"
        nova_spiceproxy_assets: "docker.io/openstackhelm/nova:rocky-ubuntu_bionic"
        nova_service_cleaner: "docker.io/openstackhelm/ceph-config-helper:latest-opensuse_15"
      openvswitch:
        openvswitch_db_server: "docker.io/openstackhelm/openvswitch:latest-ubuntu_bionic-dpdk"
        openvswitch_vswitchd: "docker.io/openstackhelm/openvswitch:latest-ubuntu_bionic-dpdk"
      #rabbitmq:
      #  prometheus_rabbitmq_exporter_helm_tests: "docker.io/openstackhelm/heat:rocky-opensuse_15-20190819"
      #  rabbitmq_init: "docker.io/openstackhelm/heat:rocky-opensuse_15-20190819"
...
---
# The purpose of this file is to provide shipyard related deployment config
# parameters. This should not require modification for a new site. However,
# shipyard deployment strategies can be very useful in getting around certain
# failures, like misbehaving nodes that hold up the deployment. See more at
# https://opendev.org/airship/shipyard/src/branch/master/doc/source/site-definition-documents.rst#using-a-deployment-strategy
schema: shipyard/DeploymentConfiguration/v1
metadata:
  schema: metadata/Document/v1
  name: deployment-configuration
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data:
  physical_provisioner:
    deployment_strategy: deployment-strategy
    deploy_interval: 30
    deploy_timeout: 3600
    destroy_interval: 30
    destroy_timeout: 900
    join_wait: 0
    prepare_node_interval: 30
    prepare_node_timeout: 1800
    prepare_site_interval: 10
    prepare_site_timeout: 300
    verify_interval: 10
    verify_timeout: 60
  kubernetes_provisioner:
    drain_timeout: 3600
    drain_grace_period: 1800
    clear_labels_timeout: 1800
    remove_etcd_timeout: 1800
    etcd_ready_timeout: 600
  armada:
    get_releases_timeout: 300
    get_status_timeout: 300
    manifest: 'full-site'
    post_apply_timeout: 7200
    validate_design_timeout: 600
...
---
# The purpose of this file is to define the PKI certificates for the environment
#
# NOTE: When deploying a new site, this file should not be configured until
# baremetal/nodes.yaml is complete.
#
schema: promenade/PKICatalog/v1
metadata:
  schema: metadata/Document/v1
  name: cluster-certificates
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data:
  certificate_authorities:
    kubernetes:
      description: CA for Kubernetes components
      certificates:
        - document_name: apiserver
          description: Service certificate for Kubernetes apiserver
          common_name: apiserver
          hosts:
            - localhost
            - 127.0.0.1
            # FIXME: Repetition of api_service_ip in common-addresses; use
            # substitution
            - 10.96.0.1
          kubernetes_service_names:
            - kubernetes.default.svc.cluster.local

        # NEWSITE-CHANGEME: The following should be a list of all the nodes in
        # the environment (genesis, control plane, data plane, everything).
        # Add/delete from this list as necessary until all nodes are listed.
        # For each node, the `hosts` list should be comprised of:
        #   1. The node's hostname, as already defined in baremetal/nodes.yaml
        #   2. The node's oam IP address, as already defined in baremetal/nodes.yaml
        #   3. The node's Calico IP address, as already defined in baremetal/nodes.yaml
        # NOTE: This list also needs to include the Genesis node, which is not
        # listed in baremetal/nodes.yaml, but by convention should be allocated
        # the first non-reserved IP in each logical network allocation range
        # defined in networks/physical/networks.yaml
        # NOTE: The genesis node needs to be defined twice (the first two entries
        # on this list) with all of the same paramters except the document_name.
        # In the first case the document_name is `kubelet-genesis`, and in the
        # second case the document_name format is `kubelete-YOUR_GENESIS_HOSTNAME`.
        - document_name: kubelet-genesis
          common_name: system:node:airsloop-control-1
          hosts:
            - airsloop-control-1
            - 192.168.125.21
          groups:
            - system:nodes
        - document_name: kubelet-airsloop-control-1
          common_name: system:node:airsloop-control-1
          hosts:
            - airsloop-control-1
            - 192.168.125.21
          groups:
            - system:nodes
        #- document_name: kubelet-airsloop-control-2
        #  common_name: system:node:airsloop-control-2
        #  hosts:
        #    - airsloop-control-2
        #    - 10.23.22.12
        #  groups:
        #    - system:nodes
        #- document_name: kubelet-airsloop-control-3
        #  common_name: system:node:airsloop-control-3
        #  hosts:
        #    - airsloop-control-3
        #    - 10.23.22.13
        #  groups:
        #    - system:nodes
        - document_name: kubelet-airsloop-compute-1
          common_name: system:node:airsloop-compute-1
          hosts:
            - airsloop-compute-1
            - 192.168.125.22
          groups:
            - system:nodes
        # End node list
        - document_name: scheduler
          description: Service certificate for Kubernetes scheduler
          common_name: system:kube-scheduler
        - document_name: controller-manager
          description: certificate for controller-manager
          common_name: system:kube-controller-manager
        - document_name: admin
          common_name: admin
          groups:
            - system:masters
        - document_name: armada
          common_name: armada
          groups:
            - system:masters
    kubernetes-etcd:
      description: Certificates for Kubernetes's etcd servers
      certificates:
        - document_name: apiserver-etcd
          description: etcd client certificate for use by Kubernetes apiserver
          common_name: apiserver
        # NOTE(mark-burnett): hosts not required for client certificates
        - document_name: kubernetes-etcd-anchor
          description: anchor
          common_name: anchor
        # NEWSITE-CHANGEME: The following should be a list of the control plane
        # nodes in the environment, including genesis.
        # For each node, the `hosts` list should be comprised of:
        #   1. The node's hostname, as already defined in baremetal/nodes.yaml
        #   2. The node's oam IP address, as already defined in baremetal/nodes.yaml
        #   3. The node's Calico IP address, as already defined in baremetal/nodes.yaml
        #   4. 127.0.0.1
        #   5. localhost
        #   6. kubernetes-etcd.kube-system.svc.cluster.local
        # NOTE: This list also needs to include the Genesis node, which is not
        # listed in baremetal/nodes.yaml, but by convention should be allocated
        # the first non-reserved IP in each logical network allocation range
        # defined in networks/physical/networks.yaml, except for the kubernetes
        # service_cidr where it should start with the second IP in the range.
        # NOTE: The genesis node is defined twice with the same `hosts` data:
        # Once with its hostname in the common/document name, and once with
        # `genesis` defined instead of the host. For now, this duplicated
        # genesis definition is required. FIXME: Remove duplicate definition
        # after Promenade addresses this issue.
        - document_name: kubernetes-etcd-genesis
          common_name: kubernetes-etcd-genesis
          hosts:
            - airsloop-control-1
            - 192.168.125.21
            - 127.0.0.1
            - localhost
            - kubernetes-etcd.kube-system.svc.cluster.local
            - 10.96.0.2
        - document_name: kubernetes-etcd-airsloop-control-1
          common_name: kubernetes-etcd-airsloop-control-1
          hosts:
            - airsloop-control-1
            - 192.168.125.21
            - 127.0.0.1
            - localhost
            - kubernetes-etcd.kube-system.svc.cluster.local
            - 10.96.0.2
        #- document_name: kubernetes-etcd-airsloop-control-2
        #  common_name: kubernetes-etcd-airsloop-control-2
        #  hosts:
        #    - airsloop-control-2
        #    - 10.23.22.12
        #    - 127.0.0.1
        #    - localhost
        #    - kubernetes-etcd.kube-system.svc.cluster.local
        #    - 10.96.0.2
        #- document_name: kubernetes-etcd-airsloop-control-3
        #  common_name: kubernetes-etcd-airsloop-control-3
        #  hosts:
        #    - airsloop-control-3
        #    - 10.23.22.13
        #    - 127.0.0.1
        #    - localhost
        #    - kubernetes-etcd.kube-system.svc.cluster.local
        #    - 10.96.0.2
        # End node list
    kubernetes-etcd-peer:
      certificates:
        # NEWSITE-CHANGEME: This list should be identical to the previous list,
        # except that `-peer` has been appended to the document/common names.
        - document_name: kubernetes-etcd-genesis-peer
          common_name: kubernetes-etcd-genesis-peer
          hosts:
            - airsloop-control-1
            - 192.168.125.21
            - 127.0.0.1
            - localhost
            - kubernetes-etcd.kube-system.svc.cluster.local
            - 10.96.0.2
        - document_name: kubernetes-etcd-airsloop-control-1-peer
          common_name: kubernetes-etcd-airsloop-control-1-peer
          hosts:
            - airsloop-control-1
            - 192.168.125.21
            - 127.0.0.1
            - localhost
            - kubernetes-etcd.kube-system.svc.cluster.local
            - 10.96.0.2
        #- document_name: kubernetes-etcd-airsloop-control-2-peer
        #  common_name: kubernetes-etcd-airsloop-control-2-peer
        #  hosts:
        #    - airsloop-control-2
        #    - 10.23.22.12
        #    - 127.0.0.1
        #    - localhost
        #    - kubernetes-etcd.kube-system.svc.cluster.local
        #    - 10.96.0.2
        #- document_name: kubernetes-etcd-airsloop-control-3-peer
        #  common_name: kubernetes-etcd-airsloop-control-3-peer
        #  hosts:
        #    - airsloop-control-3
        #    - 10.23.22.13
        #    - 127.0.0.1
        #    - localhost
        #    - kubernetes-etcd.kube-system.svc.cluster.local
        #    - 10.96.0.2
        # End node list
    calico-etcd:
      description: Certificates for Calico etcd client traffic
      certificates:
        - document_name: calico-etcd-anchor
          description: anchor
          common_name: anchor
        # NEWSITE-CHANGEME: The following should be a list of the control plane
        # nodes in the environment, including genesis.
        # For each node, the `hosts` list should be comprised of:
        #   1. The node's hostname, as already defined in baremetal/nodes.yaml
        #   2. The node's oam IP address, as already defined in baremetal/nodes.yaml
        #   3. The node's Calico IP address, as already defined in baremetal/nodes.yaml
        #   4. 127.0.0.1
        #   5. localhost
        #   6. The calico/etcd/service_ip defined in networks/common-addresses.yaml
        # NOTE: This list also needs to include the Genesis node, which is not
        # listed in baremetal/nodes.yaml, but by convention should be allocated
        # the first non-reserved IP in each logical network allocation range
        # defined in networks/physical/networks.yaml
        - document_name: calico-etcd-airsloop-control-1
          common_name: calico-etcd-airsloop-control-1
          hosts:
            - airsloop-control-1
            - 192.168.125.21
            - 127.0.0.1
            - localhost
            - 10.96.232.136
        #- document_name: calico-etcd-airsloop-control-2
        #  common_name: calico-etcd-airsloop-control-2
        #  hosts:
        #    - airsloop-control-2
        #    - 10.23.22.12
        #    - 127.0.0.1
        #    - localhost
        #    - 10.96.232.136
        #- document_name: calico-etcd-airsloop-control-3
        #  common_name: calico-etcd-airsloop-control-3
        #  hosts:
        #    - airsloop-control-3
        #    - 10.23.22.13
        #    - 127.0.0.1
        #    - localhost
        #    - 10.96.232.136
        - document_name: calico-node
          common_name: calcico-node
        # End node list
    calico-etcd-peer:
      description: Certificates for Calico etcd clients
      certificates:
        # NEWSITE-CHANGEME: This list should be identical to the previous list,
        # except that `-peer` has been appended to the document/common names.
        - document_name: calico-etcd-airsloop-control-1-peer
          common_name: calico-etcd-airsloop-control-1-peer
          hosts:
            - airsloop-control-1
            - 192.168.125.21
            - 127.0.0.1
            - localhost
            - 10.96.232.136
        #- document_name: calico-etcd-airsloop-control-2-peer
        #  common_name: calico-etcd-airsloop-control-2-peer
        #  hosts:
        #    - airsloop-control-2
        #    - 10.23.22.12
        #    - 127.0.0.1
        #    - localhost
        #    - 10.96.232.136
        #- document_name: calico-etcd-airsloop-control-3-peer
        #  common_name: calico-etcd-airsloop-control-3-peer
        #  hosts:
        #    - airsloop-control-3
        #    - 10.23.22.13
        #    - 127.0.0.1
        #    - localhost
        #    - 10.96.232.136
        - document_name: calico-node-peer
          common_name: calcico-node-peer
        # End node list
  keypairs:
    - name: service-account
      description: Service account signing key for use by Kubernetes controller-manager.
...
---
# The purpose of this file is to define the drydock Region, which in turn drives
# the MaaS region.
schema: 'drydock/Region/v1'
metadata:
  schema: 'metadata/Document/v1'
  # NEWSITE-CHANGEME: Replace with the site name
  name: airsloop
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
  substitutions:
    # NEWSITE-CHANGEME: Substitutions from deckhand SSH public keys into the
    # list of authorized keys which MaaS will register for the build-in "ubuntu"
    # account during the PXE process. Create a substitution rule for each SSH
    # key that should have access to the "ubuntu" account (useful for trouble-
    # shooting problems before UAM or UAM-lite is operational). SSH keys are
    # stored as secrets in site/airsloop/secrets.
    - dest:
        # Add/replace the first item in the list
        path: .authorized_keys[0]
      src:
        schema: deckhand/PublicKey/v1
        # This should match the "name" metadata of the SSH key which will be
        # substituted, located in site/airsloop/secrets folder.
        name: airsloop_ssh_public_key
        path: .
    - dest:
        path: .repositories.main_archive
      src:
        schema: pegleg/SoftwareVersions/v1
        name: software-versions
        path: .packages.repositories.main_archive
    # Second key example
    #- dest:
    #    # Increment the list index
    #    path: .authorized_keys[1]
    #  src:
    #    schema: deckhand/PublicKey/v1
    #    # your ssh key
    #    name: MY_USER_ssh_public_key
    #    path: .
data:
  tag_definitions: []
  # This is the list of SSH keys which MaaS will register for the built-in
  # "ubuntu" account during the PXE process. This list is populated by
  # substitution, so the same SSH keys do not need to be repeated in multiple
  # manifests.
  authorized_keys: []
  repositories:
    remove_unlisted: true
...
---
# The purpose of this file is to apply proper labels to Genesis node so the
# proper services are installed and proper configuration applied. This should
# not need to be changed for a new site.
# #GLOBAL-CANDIDATE#
schema: promenade/Genesis/v1
metadata:
  schema: metadata/Document/v1
  name: genesis-site
  layeringDefinition:
    abstract: false
    layer: site
    parentSelector:
      name: genesis-global
    actions:
      - method: merge
        path: .
  storagePolicy: cleartext
data:
  labels:
    dynamic:
      - beta.kubernetes.io/fluentd-ds-ready=true
      - calico-etcd=enabled
      - ceph-mds=enabled
      - ceph-mon=enabled
      - ceph-osd=enabled
      - ceph-rgw=enabled
      - ceph-mgr=enabled
      - ceph-bootstrap=enabled
      - tenant-ceph-control-plane=enabled
      - tenant-ceph-mon=enabled
      - tenant-ceph-rgw=enabled
      - tenant-ceph-mgr=enabled
      - kube-dns=enabled
      - kube-ingress=enabled
      - kubernetes-apiserver=enabled
      - kubernetes-controller-manager=enabled
      - kubernetes-etcd=enabled
      - kubernetes-scheduler=enabled
      - promenade-genesis=enabled
      - ucp-control-plane=enabled
      - maas-rack=enabled
      - maas-region=enabled
      - ceph-osd-bootstrap=enabled
#      - openstack-control-plane=enabled
#      - openvswitch=enabled
#      - openstack-l3-agent=enabled
      - node-exporter=enabled
      - fluentd=enabled
...
---
# The data plane host profile for Airship for DELL R720s, and should
# not need to be altered if you are using matching HW. The host profile is setup
# for cpu isolation (for nova pinning), hugepages, and sr-iov.
schema: drydock/HostProfile/v1
metadata:
  schema: metadata/Document/v1
  name: compute_r720xd
  storagePolicy: cleartext
  layeringDefinition:
    abstract: false
    layer: site
    parentSelector:
      hosttype: dp-global
    actions:
      - method: replace
        path: .interfaces
      - method: replace
        path: .storage
      - method: merge
        path: .
data:
  hardware_profile: dell_r720xd
  oob:
    type: 'libvirt'
    libvirt_uri: 'qemu+ssh://stack@192.168.121.1/system'

  primary_network: nat
  interfaces:
    nat:
      device_link: nat
      slaves:
        - nat_nic01
      networks:
        - nat
    pxe:
      device_link: pxe
      slaves:
        - pxe_nic01
      networks:
        - pxe
    data:
      device_link: data
      slaves:
        - data_nic01
      networks:
        - oam
        - storage
        - overlay
        - calico

  storage:
    physical_devices:
      vda:
        labels:
          bootdrive: 'true'
        partitions:
          - name: 'root'
            size: '30g'
            bootable: true
            filesystem:
              mountpoint: '/'
              fstype: 'ext4'
              mount_options: 'defaults'
          - name: 'boot'
            size: '1g'
            filesystem:
              mountpoint: '/boot'
              fstype: 'ext4'
              mount_options: 'defaults'
          - name: 'var_log'
            size: '100g'
            filesystem:
              mountpoint: '/var/log'
              fstype: 'ext4'
              mount_options: 'defaults'
          - name: 'var'
            size: '>100g'
            filesystem:
              mountpoint: '/var'
              fstype: 'ext4'
              mount_options: 'defaults'
...
---
schema: 'drydock/HardwareProfile/v1'
metadata:
  schema: 'metadata/Document/v1'
  name: dell_r720xd
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data:
  # Vendor of the server chassis
  vendor: DELL
  # Generation of the chassis model
  generation: '8'
  # Version of the chassis model within its generation - not version of the hardware definition
  hw_version: '3'
  # The certified version of the chassis BIOS
  bios_version: '2.2.3'
  # Mode of the default boot of hardware - bios, uefi
  boot_mode: bios
  # Protocol of boot of the hardware - pxe, usb, hdd
  bootstrap_protocol: pxe
  # Which interface to use for network booting within the OOB manager, not OS device
  pxe_interface: 2
  # Map hardware addresses to aliases/roles to allow a mix of hardware configs
  # in a site to result in a consistent configuration
  device_aliases:

    ## network
    nat_nic01:
      address: '0000:00:05.0'
      # type could identify expected hardware - used for hardware manifest validation
      dev_type: 'Red Hat, Inc. Virtio network device'
      bus_type: 'pci'
    # eno1
    pxe_nic01:
      address: '0000:00:07.0'
      # type could identify expected hardware - used for hardware manifest validation
      dev_type: 'Red Hat, Inc. Virtio network device'
      bus_type: 'pci'
    # enp67s0f0
    data_nic01:
      address: '0000:00:06.0'
      dev_type: 'Intel Corporation 82540EM Gigabit Ethernet Controller'
      bus_type: 'pci'
    # enp67s0f1

    ## storage
    # /dev/sda
    vda:
      address: '0000:00:03.0'
      dev_type: 'Virtio block device'
      bus_type: 'pci'
...
---
schema: deckhand/PublicKey/v1
metadata:
  schema: metadata/Document/v1
  name: airsloop_ssh_public_key
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC3bveUZYB3CyVAzAXHN8mmY8MFmQX2mcjmWRDk48VwrTkXYBMEz323us3WcPZgqxINliSOBHfUDqZZZbUc20oR8pAK/EfMUGAWGBFJDInpYO8dGGNFkcoY2VEVqy+2Id0Mx0p2Vc+vXmlLbwZnoee0NFvCSyWYTamFECHejHdE3sr1nE3jEZZg8PTcndtCDGnWExV2EC/CCBySUrPdAyGTxHA0vmf+1Gdncyp8aRDBAsH7LWn4FiVmy30Rn22xjTs7F74BRXa/sa0zzduL/lejzfSLAEBIODh7WG1KdSUeH2EBMZvGvRPKCSLpW9XVH42gljST/w/qwObKOJh2XblH chengli3@chengli3-MOBL
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: osh_infra_grafana_oslo_db_session_password
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: osh_infra_oslo_db_exporter_password
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: osh_nova_metadata_proxy_shared_secret
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: password123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: osh_barbican_oslo_db_password
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: ucp_deckhand_keystone_password
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: osh_heat_oslo_messaging_admin_password
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: osh_keystone_admin_password
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: ucp_postgres_exporter_password
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: password123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: osh_glance_rabbitmq_erlang_cookie
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: ucp_drydock_keystone_password
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: osh_heat_oslo_db_password
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: osh_infra_rgw_s3_admin_access_key
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: admin_access_key
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: osh_neutron_oslo_messaging_password
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: osh_nova_oslo_messaging_password
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: ucp_barbican_oslo_db_password
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: osh_cinder_rabbitmq_erlang_cookie
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: ucp_maas_admin_password
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: osh_barbican_password
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: osh_nova_password
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: osh_rabbitmq_erlang_cookie
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: password123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: ceph_swift_keystone_password
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: ucp_postgres_replication_password
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: osh_placement_password
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: osh_cinder_password
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: ucp_rabbitmq_erlang_cookie
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: osh_keystone_rabbitmq_erlang_cookie
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: osh_infra_prometheus_admin_password
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: ucp_drydock_postgres_password
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: osh_cinder_oslo_messaging_admin_password
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: ucp_postgres_admin_password
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: osh_glance_oslo_messaging_admin_password
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: apiserver-encryption-key-key1
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
# head -c 32 /dev/urandom | base64
data: bL2mHd9Sf5hQvZPuDncZRugYYqYyR3cGcZKVJ9wjswg=
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: osh_cinder_oslo_messaging_password
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: osh_nova_rabbitmq_erlang_cookie
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: osh_nova_oslo_messaging_admin_password
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: osh_neutron_oslo_messaging_admin_password
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: ucp_armada_keystone_password
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: ucp_keystone_oslo_db_password
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: osh_nova_oslo_db_password
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: ceph_fsid
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
# uuidgen
data: d52a9d00-64b9-45f0-b564-08dffe95f847
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: osh_oslo_cache_secret_key
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: osh_heat_stack_user_password
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: osh_glance_oslo_messaging_password
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: ipmi_admin_password
  layeringDefinition:
    abstract: false
    layer: site
  labels:
    name: ipmi-admin-password-site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: osh_oslo_db_admin_password
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: osh_neutron_rabbitmq_erlang_cookie
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: maas-region-key
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
# openssl rand -hex 10
data: e12330cfe038735aee32
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: tenant_ceph_fsid
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
# uuidgen
data: 9e45aa5f-9d75-4fa7-bde5-c99e4a7db7a1
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: osh_barbican_rabbitmq_erlang_cookie
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: osh_barbican_oslo_messaging_password
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: airsloop_crypt_password
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
# Pass: airsloop123
data: $6$AVL7yH1sLYlKqvcK$ngUiLKYZQhhj07Lb3ngWa4qVwDgUP9pCGfGFG7JIpF.6iStnfEMeySf8XusA0/3i9O5gMHE9hbg1/4GrFb5rR0
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: osh_infra_rgw_s3_elasticsearch_secret_key
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: elastic_secret_key
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: ucp_deckhand_postgres_password
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: osh_oslo_messaging_admin_password
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: password123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: osh_barbican_oslo_messaging_admin_password
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: osh_infra_grafana_oslo_db_password
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: osh_heat_oslo_messaging_password
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: ucp_barbican_keystone_password
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: osh_keystone_oslo_messaging_admin_password
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: ucp_airflow_oslo_messaging_password
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: osh_keystone_oslo_messaging_password
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: osh_keystone_ldap_password
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: ucp_shipyard_keystone_password
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: ucp_promenade_keystone_password
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: ucp_oslo_db_admin_password
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: osh_heat_rabbitmq_erlang_cookie
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: ucp_oslo_messaging_password
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: ucp_maas_postgres_password
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: ucp_openstack_exporter_keystone_password
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: osh_neutron_oslo_db_password
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: osh_neutron_password
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: osh_heat_trustee_password
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: osh_infra_rgw_s3_elasticsearch_access_key
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: elastic_access_key
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: osh_infra_grafana_admin_password
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: osh_oslo_db_exporter_password
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: osh_tempest_password
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: password123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: osh_keystone_oslo_db_password
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: osh_infra_rgw_s3_admin_secret_key
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: admin_secret_key
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: osh_infra_nagios_admin_password
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: osh_glance_oslo_db_password
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: osh_infra_oslo_db_admin_password
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: osh_horizon_oslo_db_password
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: osh_heat_password
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: ucp_airflow_postgres_password
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: osh_infra_elasticsearch_admin_password
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: osh_glance_password
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: ucp_shipyard_postgres_password
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: osh_cinder_oslo_db_password
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: ucp_keystone_admin_password
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: deckhand/Passphrase/v1
metadata:
  schema: metadata/Document/v1
  name: osh_infra_openstack_exporter_password
  layeringDefinition:
    abstract: false
    layer: site
  storagePolicy: cleartext
data: airsloop123
...
---
schema: pegleg/DeploymentData/v1
metadata:
  schema: metadata/Document/v1
  name: deployment-version
  layeringDefinition:
    abstract: false
    layer: global
  storagePolicy: cleartext
data:
  documents:
    treasuremap:
      commit: b3a04dfa737291b35cc3a16e36cfe3193dae4879
      dirty: true
      tag: pod14
...
